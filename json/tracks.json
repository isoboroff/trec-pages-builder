{
  "trec1": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new topics. This task is similar to how a researcher might use a library — the collection is known but the questions likely to be asked are not known.",
      "year": 1992
    },
    "routing": {
      "fullname": "Routing",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "In TREC the routing task is represented by using known topics and known relevant documents for those topics, but new data for testing. The participants are given a set of known (or training) topics, along with a set of documents, including known relevant documents for those topics. The topics consist of natural language text describing a user's information need. The topics are used to create a set of queries (the actual input to the retrieval system) which are then used against the training documents.",
      "year": 1992
    }
  },
  "trec2": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new topics. This task is similar to how a researcher might use a library — the collection is known but the questions likely to be asked are not known.",
      "year": 1993
    },
    "routing": {
      "fullname": "Routing",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "In TREC the routing task is represented by using known topics and known relevant documents for those topics, but new data for testing. The participants are given a set of known (or training) topics, along with a set of documents, including known relevant documents for those topics. The topics consist of natural language text describing a user's information need. The topics are used to create a set of queries (the actual input to the retrieval system) which are then used against the training documents.",
      "year": 1993
    }
  },
  "trec3": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new topics. This task is similar to how a researcher might use a library — the collection is known but the questions likely to be asked are not known.",
      "year": 1994
    },
    "routing": {
      "fullname": "Routing",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "In TREC the routing task is represented by using known topics and known relevant documents for those topics, but new data for testing. The participants are given a set of known (or training) topics, along with a set of documents, including known relevant documents for those topics. The topics consist of natural language text describing a user's information need. The topics are used to create a set of queries (the actual input to the retrieval system) which are then used against the training documents.",
      "year": 1994
    }
  },
  "trec4": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The adhoc task is represented by new topics for known documents. 50 new test topics are used to create the adhoc queries for searching against the known documents. Fifty new topics (numbers 201-250) were generated for TREC-4. The known documents used in TREC-4 were on disks 2 and 3.",
      "year": 1995
    },
    "dbmerge": {
      "fullname": "Database Merging",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The database merging task also represents a focussing of the adhoc task. In this case the goal was to investigate techniques for merging results from the various TREC subcollections as opposed to treating the collections as a single entity). There were 10 subcollections defined corresponding to the various dates of the data, i.e., the three different years of the Wall Street Journal, the two different years of the AP newswire, the two sets of Ziff documents (one on each disk), and the three single subcollections (the Federal Register, the San Jose Mercury News, and the U.S. Patents). The 3 participating groups ran the adhoc topics separately on each of the 10 subcollections, merged the results, and submitted these results, along with a baseline run treating the subcollections as a single collection.",
      "year": 1995
    },
    "routing": {
      "fullname": "Routing",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "In TREC the routing task is represented by using known topics and known relevant documents for those topics, but new data for testing. The participants are given a set of known (or training) topics, along with a set of documents, including known relevant documents for those topics. The topics consist of natural language text describing a user's information need. The topics are used to create a set of queries (the actual input to the retrieval system) which are then used against the training documents.",
      "year": 1995
    },
    "spanish": {
      "fullname": "Spanish",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The multilingual track represents an extension of the adhoc task to a second language (Spanish). An informal Spanish test was run in TREC-3, but the data arrived late and few groups were able to take part. In TREC-4 the track was made official and 10 groups took part. There were about 200 megabytes of Spanish data (the El Norte newspaper from Monterey, Mexico), and 25 topics. Groups used the adhoc task guidelines, and submitted the top 1000 documents retrieved for each of the 25 Spanish topics.",
      "year": 1995
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": "",
      "webpage": "",
      "coordinators": "D. Lewis, AT&T Research",
      "description": "The filtering track represents a variation of the routing task, and was designed to investigate concerns about the current definition of this task. It used the same topics, training documents, and test documents as the routing task. The difference was that the results submitted for the filtering runs were unranked sets of documents satisfying three 'utility function' criteria. These criteria were designed to approximate a high precision run, a high recall run, and a balanced run.",
      "year": 1995
    },
    "confusion": {
      "fullname": "Confusion",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The confusion track represents an extension of the current tasks to deal with corrupted data such as would come from OCR or speech input. The track followed the adhoc task, but using only the category B data. This data was randomly corrupted at NIST using character dele-tions, substitutions, and additions to create data with a 10% and 20% error rate (i.e., 10% or 20% of the characters were affected). Note that this process is neutral in that it does not model OCR or speech input. Four groups used the baseline and 10% corruption level; only two groups tried the 20% level.",
      "year": 1995
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The interactive track focusses the adhoc task on the process of doing searches interactively. It was felt by many groups that TREC uses evaluation for a batch retrieval environment rather than the more common interactive environments seen today. However there are few tools for evaluating interactive systems, and none that seem appropriate to TREC. The interactive track has a double goal of developing better methodologies for interactive evaluation and investigating in depth how users search the TREC topics. Eleven groups took part in this track in TREC-4. A subset of the adhoc topics was used, and many different types of experiments were run. The common thread was that all groups used the same topics, performed the same task(s), and recorded the same information about how the searches were done. Task 1 was to retrieve as many relevant documents as possible within a certain timeframe. Task 2 was to construct the best query possible.",
      "year": 1995
    }
  },
  "trec5": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": {
        "A": "Category A",
        "B": "Category B"
      },
      "webpage": "https://trec.nist.gov/data/test_coll.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new topics. This task is similar to how a researcher might use a library — the collection is known but the questions likely to be asked are not known.",
      "year": 1996
    },
    "dbmerge": {
      "fullname": "Database Merging",
      "tasks": "",
      "webpage": "",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "There are many times when users want to search separate text collections as if they were a single collection. For example, computer networks can provide access to a variety of corpora that are owned and maintained by different entities. Instead of issuing search commands to each of the databases in turn and manually collating the individual results, users prefer a mechanism for performing a single, integrated search. In other cases, reliability and efficiency concerns may dictate that databases that are under the same administrative control should be physically separate. Again, users want to issue a single search request that returns an integrated result. The database merging track investigates methods for combining the results of separate searches into a single, cohesive result.",
      "year": 1996
    },
    "routing": {
      "fullname": "Routing",
      "tasks": {
        "A": "Category A",
        "B": "Category B"
      },
      "webpage": "",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The routing task in the TREC workshops investigates the performance of systems that use standing queries to search new streams of documents. These searches are similar to those required by news clipping services and library profiling systems. A true routing environment is simulated in TREC by using topics that have known relevant documents and testing on a completely new document set.",
      "year": 1996
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": "",
      "webpage": "",
      "coordinators": "D. D. Lewis, AT&T Labs-Research",
      "description": "The TREC-5 filtering track, an evaluation of binary text classification systems, was a repeat of the filtering evaluation run in a trial version for TREC-4, with only the data set and participants changing. Seven sites took part, submitting a total of ten runs. We review the nature of the task, the effectiveness measures and evaluation methods used, and briefly discuss the results. Some deficiencies in the evaluation are examined, with an eye toward improving future filtering evaluations.",
      "year": 1996
    },
    "Spanish": {
      "fullname": "Spanish",
      "tasks": "",
      "webpage": "",
      "coordinators": "A. Smeaton, Dublin City University:R. Wilkinson, Royal Melbourne Institute of Technology (RMIT University)",
      "description": "The TREC-5 conference was the third year in which document retrieval in a language other than English was benchmarked. In TREC-3, 4 groups participated in an ad hoc retrieval task on a collection of 208 Mbytes of Mexican newspaper text in the Spanish language. In TREC-4 there were 10 groups who participated, once again in an ad hoc document retrieval task on the same Mexican newspaper texts but with new topics. In TREC-5 there was a change of document corpus and new topics for the Spanish ad hoc retrieval task and a corpus of documents and topics to support ad hoc retrieval in the Chinese language was introduced for the first time. There were 7 groups who submitted runs for the Spanish track and 10 who submitted results for Chinese.",
      "year": 1996
    },
    "Chinese": {
      "fullname": "Chinese",
      "tasks": "",
      "webpage": "",
      "coordinators": "A. Smeaton, Dublin City University:R. Wilkinson, Royal Melbourne Institute of Technology (RMIT University)",
      "description": "The TREC-5 conference was the third year in which document retrieval in a language other than English was benchmarked. In TREC-3, 4 groups participated in an ad hoc retrieval task on a collection of 208 Mbytes of Mexican newspaper text in the Spanish language. In TREC-4 there were 10 groups who participated, once again in an ad hoc document retrieval task on the same Mexican newspaper texts but with new topics. In TREC-5 there was a change of document corpus and new topics for the Spanish ad hoc retrieval task and a corpus of documents and topics to support ad hoc retrieval in the Chinese language was introduced for the first time. There were 7 groups who submitted runs for the Spanish track and 10 who submitted results for Chinese.",
      "year": 1996
    },
    "nlp": {
      "fullname": "NLP",
      "tasks": "",
      "webpage": "",
      "coordinators": "T. Strzalkowski, GE Corporate Research and Development:K. Sparck Jones, University of Cambridge",
      "description": "The NLP track was initiated to explore whether the natural language processing (NLP) techniques available today are mature enough to have an impact on IR, and specifically whether they can offer an advantage over purely quantitative retrieval methods.",
      "year": 1996
    },
    "confusion": {
      "fullname": "Confusion",
      "tasks": "",
      "webpage": "",
      "coordinators": "P. Kantor, Rutgers University:E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": " For TREC-5, retrieval from corrupted data was studied through retrieval of single target  documents from a corpus which was corrupted by producing page images, corrupting the bit  maps, and applying OCR techniques to the results. In general, methods which attempted a probabilistic estimation of the original clean text fare better than methods which simply accept corrupted versions of the query text.",
      "year": 1996
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "",
      "coordinators": "P. Over, National Institute of Standards and Technology (NIST)",
      "description": "The high-level goal of the Interactive Track in TREC-5 was the investigation of searching as an interactive task by examining the process as well as the outcome.",
      "year": 1996
    }
  },
  "trec6": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": {
        "A": "Category A",
        "B": "Category B"
      },
      "webpage": "https://trec.nist.gov/data/test_coll.html",
      "coordinators": "Ellen Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new topics. This task is similar to how a researcher might use a library-the collection is known but the questions likely to be asked are not known.",
      "year": 1997
    },
    "routing": {
      "fullname": "Routing",
      "tasks": {
        "A": "Category A",
        "B": "Category B"
      },
      "webpage": "",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The routing task in the TREC workshops investigates the performance of systems that use standing queries to search new streams of documents. These searches are similar to those required by news clipping services and library profiling systems. A true routing environment is simulated in TREC by using topics that have known relevant documents and testing on a completely new document set.",
      "year": 1997
    },
    "chinese": {
      "fullname": "Chinese",
      "tasks": "",
      "webpage": "",
      "coordinators": "R. Wilkinson, CSIRO",
      "description": "The TREC-6 conference was the fourth year in which document retrieval in a language other than English was carried out. In TREC-3, 4 groups participated in an ad hoc retrieval task on a collection of 208 Mbytes of Mexican newspaper text in the Spanish language. In TREC-4 there were 10 groups who participated, once again in an ad hoc document retrieval task on the same Mexican newspaper texts but with new topics. In TREC-5 there was a change of document corpus and new topics for the Spanish ad hoc retrieval task and a corpus of documents and topics to support ad hoc retrieval in the Chinese language was introduced for the first time. In TREC-6 there was two tracks in which languages other than English were explored. In the Chinese track, a second set of topics were evaluated against the existing corpus. In the cross-lingual track experiments were conducted where queries in one language were used against a document corpus in another language.",
      "year": 1997
    },
    "clir": {
      "fullname": "Cross-Language",
      "tasks": {
        "French-French": "French-French",
        "German-German": "German-German",
        "German-English": "German-English",
        "German-French": "German-French",
        "English-German": "English-German",
        "English-English": "English-English",
        "French-German": "French-German",
        "French-English": "French-English",
        "Dutch-English": "Dutch-English",
        "Spanish-English": "Spanish-English",
        "English-French": "English-French"
      },
      "webpage": "",
      "coordinators": "P. Schäuble, Swiss Federal Institute of Technology (ETH):P. Sheridan, Swiss Federal Institute of Technology (ETH)",
      "description": "Cross-Language Information Retrieval (CLIR) was a new task in the TREC-6 evaluation. In contrast to the multilingual track included in previous TREC evaluations, which was concerned with information retrieval in Spanish or Chi-nese, the cross-language retrieval track focuses on the retrieval situation where the documents are written in a language which is different than the language used to specify the queries. The TREC-6 track used documents in English, French and German and queries in English, French, German, Spanish and Dutch.",
      "year": 1997
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": "",
      "webpage": "",
      "coordinators": "D.A. Hull, Xerox Research Centre Europe",
      "description": "Given a topic description and a large collection of documents, a sample of which have been evaluated as relevant or not relevant for that topic, construct a query profile and a routing function which will score and rank new documents according to their likelihood of relevance.",
      "year": 1997
    },
    "hp": {
      "fullname": "High-Precision",
      "tasks": "",
      "webpage": "",
      "coordinators": "C. Buckley, SabIR Research Inc.",
      "description": "The High-Precision Track is a new track for TREC. It has a very simple short description: for each query, a user should find the best 10 documents possible within 5 minutes clock time. One realistic scenario corresponding to this task might be that your boss asks you for a quick report on some area and you need to find some information on the area fast.",
      "year": 1997
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t6i/t6i.html",
      "coordinators": "P. Over, National Institute of Standards and Technology (NIST)",
      "description": "The high-level goal of the TREC-6 Interactive Track was the investigation of searching as an interactive information retrieval (IR) task by examining the process as well as the outcome. To these ends the track specification provided for two levels of experimentation.",
      "year": 1997
    },
    "nlp": {
      "fullname": "NLP",
      "tasks": "",
      "webpage": "",
      "coordinators": "Ellen Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The NLP track was initiated to explore whether the natural language processing (NLP) techniques available today are mature enough to have an impact on IR, and specifically whether they can offer an advantage over purely quantitative retrieval methods. The track used the 50 ad hoc topics and the Financial Times document set.",
      "year": 1997
    },
    "sdr": {
      "fullname": "Spoken Document Retrieval",
      "tasks": "",
      "webpage": "",
      "coordinators": "J. Garofolo, National Institute of Standards and Technology (NIST):E. Voorhees, National Institute of Standards and Technology (NIST):V. Stanford, National Institute of Standards and Technology (NIST):K. Sparck Jones, Cambridge University",
      "description": "Spoken Document Retrieval (SDR) involves the retrieval of excerpts from recordings of speech using a combination of automatic speech recognition and information retrieval techniques. In performing SDR, a speech recognition engine is applied to an audio input stream and generates a time-marked textual representation (transcription) of the speech. The transcription is then indexed and may be searched using an Information Retrieval engine.",
      "year": 1997
    },
    "vlc": {
      "fullname": "Very Large Corpus",
      "tasks": "",
      "webpage": "",
      "coordinators": "D. Hawking, Australian National University:P. Thistlewaite, Australian National University",
      "description": "The emergence of real world applications for text collections orders of magnitude larger than the TREC collection has motivated the introduction of a Very Large Collection track within the TREC framework.",
      "year": 1997
    }
  },
  "trec7": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/test_coll.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc task investigates the performance of systems that search a static set of documents using new questions (called topics in TREC). This task is similar to how a researcher might use a library—the collection is known but the questions likely to be asked are not known. NIST provides the participants approximately 2 gigabytes worth of documents and a set of 50 natural language topic statements. The participants produce a set of queries from the topic statements and run those queries against the documents. The output from this run is the official test result for the ad hoc task. Participants return the best 1000 documents retrieved for each topic to NIST for evaluation.",
      "year": 1998
    },
    "hp": {
      "fullname": "High-Precision",
      "tasks": "",
      "webpage": "",
      "coordinators": "C. Buckley, SabIR Research, Inc.",
      "description": "TREC 7 is the second year the High-Precision (HP) track has been run. It is an attempt to perform a task that is much more closely related to real-world user interactions than the ad-hoc or routing task. The goal is simple: a user is asked to find 15 relevant documents in 5 minutes. No other restrictions are put on the user (other than no prior knowledge of the query, and no asking other users for help). Official evaluation is simply how many actual relevant documents were found among the 15 documents supplied by the user, modified slightly for those queries with fewer than 15 relevant documents in the collection (Relative Precision at 15 documents).",
      "year": 1998
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": {
        "adaptive": "Adaptive filtering",
        "batch": "Batch filtering",
        "routing": "Routing"
      },
      "webpage": "",
      "coordinators": "D.A. Hull, Xerox Research Centre Europe",
      "description": "Given a topic description, build a filtering profile which will select the most relevant examples from an incoming stream of documents. As the document stream is processed, the system may be provided with a binary judgement of relevance for some of the retrieved documents. This information can be used to adaptively update the filtering profile.",
      "year": 1998
    },
    "sdr": {
      "fullname": "Spoken Document Retrieval",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/sdr.html",
      "coordinators": "J. Garofolo, National Institute of Standards and Technology (NIST):E. Voorhees, National Institute of Standards and Technology (NIST): C. Auzanne, National Institute of Standards and Technology (NIST):V. Stanford, National Institute of Standards and Technology (NIST):B. Lund, National Institute of Standards and Technology (NIST)",
      "description": "Spoken Document Retrieval (SDR) involves the search and retrieval of excerpts from recordings of speech using a combination of automatic speech recognition and information retrieval techniques. In performing SDR, a speech recognition engine is applied to an audio input stream and generates a time-marked textual representation (transcription) of the speech. The transcription is then indexed and may be searched using an information retrieval engine. In traditional information retrieval, a topic (or query) results in a rank-ordered list of documents. In SDR, a topic results in a rank-ordered list of temporal pointers to potentially relevant excerpts. In an operational SDR system, these excerpts could be topical sections of a recording of a conference or radio or television broadcasts.",
      "year": 1998
    },
    "xlingual": {
      "fullname": "Cross-Language",
      "tasks": "",
      "webpage": "",
      "coordinators": "M. Braschler, Eurospider Information Technology:J. Krause, Informationszentrum Sozialwissenschaften:C. Peters, Istituto di Elaborazione della Informazione (IEI-CNR):P. Schäuble, Swiss Federal Institute of Technology (ETH)",
      "description": "This year, the TREC cross-language retrieval track took place for the second time. In TREC-7, we extended the task presented to the participants. The goal was for groups to use queries written in a single language in order to retrieve documents from a multilingual pool of documents written in many different languages. This is also a more comprehensive task than the usual definition of cross-language information retrieval, where systems work with a language pair, retrieving documents in a language L1 using queries in language L2.",
      "year": 1998
    },
    "query": {
      "fullname": "Query",
      "tasks": "",
      "webpage": "",
      "coordinators": "C. Buckley, SabIR Research, Inc.",
      "description": "General IR research is being held up because we don't have enough queries of various types to investigate advanced retrieval techniques that are query dependent. There's no way we can get enough relevance judgements on new queries to form a good query pool. The Query track looks at multiple query variations of past TREC topics to get a large number of query formulations.",
      "year": 1998
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t7i/t7i.html",
      "coordinators": "P. Over, National Institute of Standards and Technology (NIST)",
      "description": "For TREC-7 the high-level goal of the Interactive Track remained the investigation of searching as an interactive task by examining the process as well as the outcome.",
      "year": 1998
    }
  },
  "trec8": {
    "adhoc": {
      "fullname": "Adhoc",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/test_coll.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The ad hoc retrieval task investigates the performance of systems that search a static set of documents using new questions (called topics in TREC). This task is similar to how a researcher might use a library—the collection is known but the questions likely to be asked are not known. NIST provides the participants approximately 2 gigabytes worth of documents and a set of 50 natural language topic statements. The participants produce a set of queries from the topic statements and run those queries against the documents. The output from this run is the official test result for the ad hoc task. Participants return the best 1000 documents retrieved for each topic to NIST for evaluation.",
      "year": 1999
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": {
        "adaptive": "Adaptive filtering",
        "batch": "Batch filtering",
        "routing": "Routing"
      },
      "webpage": "https://trec.nist.gov/data/filtering.html",
      "coordinators": "D. Hull, Xerox Research Centre Europe:S. Robertson, Microsoft Research",
      "description": "The TREC-8 filtering track measures the ability of systems to build persistent user profiles which successfully separate relevant and non-relevant documents. It consists of three major subtasks: adaptive filtering, batch filtering, and routing. In adaptive filtering, the system begins with only a topic statement and must learn a better profile from on-line feedback. Batch filtering and routing are more traditional machine learning tasks where the system begins with a large sample of evaluated training documents.",
      "year": 1999
    },
    "web": {
      "fullname": "Large Web",
      "tasks": {
        "small": "Small Web Task",
        "large": "Large Web Task"
      },
      "webpage": "https://trec.nist.gov/data/t8.web.html",
      "coordinators": "Martin Braschler, Eurospider Information Tech. AG:Carol Peters, Istituto Elaborazione Informazione:Peter Schäuble, Eurospider Information Tech. AG",
      "description": "The TREC-8 Web Track defined ad hoc retrieval tasks over the 100 gigabyte VLC2 collection (Large Web Task) and a selected 2 gigabyte subset known as WT2g (Small Web Task).",
      "year": 1999
    },
    "query": {
      "fullname": "Query",
      "tasks": {
        "creation": "Query Creation Sub-Task",
        "retrieval": "Retrieval Sub-Task"
      },
      "webpage": "",
      "coordinators": "C. Buckley, SabIr Research, Inc.:J. Walz, SabIr Research, Inc.",
      "description": "The Query Track in TREC-8 is a bit different from all the other tracks. It is a cooperative effort among the participating groups to look at the issue of query variability.",
      "year": 1999
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST):D. Tice, National Institute of Standards and Technology (NIST)",
      "description": "The TREC-8 Question Answering track was the first large scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes).",
      "year": 1999
    },
    "sdr": {
      "fullname": "Spoken Document Retrieval",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/sdr.html",
      "coordinators": "J. Garofolo, National Institute of Standards and Technology (NIST):C. Auzanne, National Institute of Standards and Technology (NIST):E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "SDR involves the search and retrieval of excerpts from spoken audio recordings using a combination of automatic speech recognition and information retrieval technologies. The TREC SDR Track has provided an infrastructure for the development and evaluation of SDR technology and a common forum for the exchange of knowledge between the speech recognition and information retrieval research communities. The SDR Track can be declared a success in that it has provided objective, demonstrable proof that this technology can be successfully applied to realistic audio collections using a combination of existing technologies and that it can be objectively evaluated.",
      "year": 1999
    },
    "xlingual": {
      "fullname": "Cross-Language",
      "tasks": {
        "english": "English",
        "french": "French",
        "italian": "Italian",
        "german": "German"
      },
      "webpage": "",
      "coordinators": "M. Braschler, Eurospider Information Technology AG: P. Schäuble, Eurospider Information Technology AG):C. Peters, Instituto Elaborazione Informazione (CNR)",
      "description": "A cross-language retrieval track was offered for the third time at TREC-8. The main task was the same as that of the previous year: the goal was for groups to use queries written in a single language in order to retrieve documents from a multilingual pool of documents written in many different languages. Compared to the usual definition of cross-language information retrieval, where systems work with a single language pair, retrieving documents in a language L1 using queries in language L2, this is a slightly more comprehensive task, and we feel one that more closely meets the demands of real world applications.",
      "year": 1999
    },
    "girt": {
      "fullname": "GIRT",
      "tasks": "",
      "webpage": "",
      "coordinators": "M. Braschler, Eurospider Information Technology AG: P. Schäuble, Eurospider Information Technology AG:C. Peters, Instituto Elaborazione Informazione (CNR)",
      "description": "",
      "year": 1999
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t8i/t8i.html",
      "coordinators": "W. Hersh, Oregon Health Sciences University:P. Over, National Institute of Standards and Technology (NIST)",
      "description": "For TREC-8 the high-level goal of the Interactive Track remained the investigation of searching as an interactive task by examining the process as well as the outcome.",
      "year": 1999
    }
  },
  "trec9": {
    "web": {
      "fullname": "Web",
      "tasks": {
        "main": "Main Task",
        "large": "Large Web Task"
      },
      "webpage": "https://trec.nist.gov/data/t9.web.html",
      "coordinators": "D. Hawking, CSIRO Mathematical and Information Sciences",
      "description": "TREC-9 marked a broadening of the range of of search task types represented in the Web track and a serious attempt to determine whether hyperlinks could be used to improve retrieval effectiveness on a topic-relevance ad hoc retrieval task. The Large Web Task compared the ability of systems to locate online service pages within the 18.5 million page VLC2 collection. In this case the question is not whether the page is relevant to the topic, but whether it provides direct access to the desired service. In contrast, the Main Web Task compared link-based and non-link methods on a task involving topic relevance queries and a 1.69 million page corpus (WT10g) which was carefully engineered to ensure a high density of inter-server links and (relative) ease of processing. The Main Web task topics were in TREC Ad Hoc form but were reverse engineered from query logs. Ternary relevance judgments were obtained and, in addition, assessors were asked to identify 'best' documents for each topic. As in TREC-8, no significant benefit associated with the use of link information in a topic-relevance retrieval task was demonstrated by any of the participating groups, whether or not additional weight was given to highly relevant documents.",
      "year": 2000
    },
    "sdr": {
      "fullname": "Spoken Document Retrieval",
      "tasks": {
        "b1": "Baseline Retrieval using medium error recognizer transcripts",
        "cr": "Cross-Recognizer Retrieval using other participants' recognizer transcripts",
        "r1": "Reference Retrieval using human-generated 'perfect' transcripts",
        "s1": "Full SDR using own recognizer",
        "s2": "Full SDR using own secondary recognizer"
      },
      "webpage": "https://trec.nist.gov/data/sdr.html",
      "coordinators": "J. Garofolo, National Institute of Standards and Technology (NIST):J. Lard, National Institute of Standards and Technology (NIST):E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The SDR track fosters research on retrieval methodologies for spoken documents (i.e., recordings of speech). The task in the track is an ad hoc task in which the documents are transcriptions of audio signals.",
      "year": 2000
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC question answering track is an effort to bring the benefits of large-scale evaluation to bear on the question answering problem. The track has run twice so far, where the goal both times was to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The best performing system in TREC-9, the Falcon system from Southern Methodist University, was able to answer about 65% of the questions (compared to approximately 42% of the questions for the next best systems) by combining abductive reasoning with various natural language processing techniques. The 65% score is slightly less than the best scores for TREC-8 in absolute terms, but it represents a very significant improvement in question answering systems. The TREC-9 task was considerably harder than the TREC-8 task because the TREC-9 track used actual users' questions rather than questions constructed specifically for the track.",
      "year": 2000
    },
    "xlingual": {
      "fullname": "Cross-Language",
      "tasks": "",
      "webpage": "",
      "coordinators": "F. Gey, A. Chen, University of California, Berkeley",
      "description": "For TREC-9 the cross-language information retrieval task was to utilize English queries against Chinese documents. This aspect of multilingual information access at TREC-9 was the seventh year in which non-English document retrieval was tested and evaluated, and the fourth year for which cross-language information retrieval has been experimented with.",
      "year": 2000
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": {
        "adaptive": "Adaptive filtering",
        "batch": "Batch filtering",
        "routing": "Routing"
      },
      "webpage": "https://trec.nist.gov/data/filtering/README.t9.filtering",
      "coordinators": "S. Robertson, Microsoft Research:D.A. Hull, WhizzBang Labs",
      "description": "The TREC-9 filtering track measures the ability of systems to build persistent user profiles which successfully separate relevant and non-relevant documents. It consists of three major subtasks: adaptive filtering, batch filtering, and routing. In adaptive filtering, the system begins with only a topic statement and a small number of positive examples, and must learn a better profile from on-line feedback. Batch filtering and routing are more traditional machine learning tasks where the system begins with a large sample of evaluated training documents.",
      "year": 2000
    },
    "query": {
      "fullname": "Query",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t8_query.html",
      "coordinators": "Chris Buckley, Sabir Research, Inc.",
      "description": "The Query Track in TREC-9 is unlike the other tracks in TREC. The other tracks attempt to compare systems to determine the best approaches to solve the particular track problem. This comparison is normally done over a given set of topics, with a single query per topic. The Query Track, on the other hand, compares multiple queries on a single topic to determine which queries perform best with which systems. There is no emphasis on system-system comparisons: none of the participating systems were even the most advanced system from that particular participating group. Instead, the goal is to try and understand how the statement (query) of the user's information need (topic) affects retrieval.",
      "year": 2000
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t9i/t9i.html",
      "coordinators": "W. Hersh, Oregon Health Sciences University:Paul Over, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Interactive Track has the goal of investigating interactive information retrieval by examining the process as well as the results. In TREC-9 six research groups ran a total of 12 interactive information retrieval (IR) system variants on a shared problem: a fact-finding task, eight questions, and newspaper/newswire documents from the TREC collections. This report summarizes the shared experimental framework, which for TREC-9 was designed to support analysis and comparison of system performance only within sites.",
      "year": 2000
    }
  },
  "trec10": {
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc",
        "homepage": "Homepage Finding"
      },
      "webpage": "https://trec.nist.gov/data/t10.web.html",
      "coordinators": "D. Hawking, CSIRO:N. Craswell, CSIRO",
      "description": "TREC-2001 saw the falling into abeyance of the Large Web Task but a strengthening and broadening of activities based on the 1.69 million page WT10g corpus. There were two tasks. The topic relevance task was like traditional TREC ad hoc but used queries taken from real web search logs from which description and narrative fields of a topic description were inferred by the topic developers. There were 50 topics. In the homepage finding task queries corresponded to the name of an entity whose home page (site entry page) was included in WT10g. The challenge in this task was to return all of the homepages at the very top of the ranking. Cursory analysis suggests that once again, exploitation of link information did not help on the topic relevance task. By contrast, in the homepage finding task, the best performing run which did not make use of either link information or properties of the document's URL achieved only half of the mean reciprocal rank of the best run.",
      "year": 2001
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main Task",
        "list": "List Task",
        "context": "Context Task"
      },
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC question answering track is an effort to bring the benefits of large-scale evaluation to bear on the question answering problem. In its third year, the track continued to focus on retrieving small snippets of text that contain an answer to a question. However, several new conditions were added to increase the realism, and the difficulty, of the task. In the main task, questions were no longer guaranteed to have an answer in the collection; systems returned a response of 'NIL' to indicate their belief that no answer was present. In the new list task, systems assembled a set of instances as the response for a question, requiring the ability to distinguish among instances found in multiple documents. Another new task, the context task, required systems to track discourse objects through a series of questions.",
      "year": 2001
    },
    "xlingual": {
      "fullname": "Cross-Language",
      "tasks": {
        "arabic": "Arabic",
        "english": "English",
        "french": "French"
      },
      "webpage": "",
      "coordinators": "F.C. Gey, University of California, Berkeley:D.W. Oard, University of Maryland, College Park",
      "description": "Ten groups participated in the TREC-2001 cross-language information retrieval track, which focussed on retrieving Arabic language documents based on 25 queries that were originally prepared in English. French and Arabic translations of the queries were also available. This was the first year in which a large Arabic test collection was available, so a variety of approaches were tried and a rich set of experiments performed using resources such as machine translation, parallel corpora, several approaches to stemming and/or morphology, and both pre-translation and post-translation blind relevance feedback. On average, forty percent of the relevant documents discovered by a participating team were found by no other team, a higher rate than normally observed at TREC. This raises some concern that the relevance judgment pools may be less complete than has historically been the case.",
      "year": 2001
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": {
        "adaptive": "Adaptive filtering",
        "batch": "Batch filtering",
        "routing": "Routing"
      },
      "webpage": "https://trec.nist.gov/data/filtering/T10filter_guide.htm",
      "coordinators": "S. Robertson, Microsoft Research:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "Given a topic description, build a filtering profile which will select the most relevant examples from an incoming stream of documents. As the document stream is processed, the system may be provided with a binary judgement of relevance for some of the retrieved documents. This information can be used to adaptively update the filtering profile.",
      "year": 2001
    },
    "video": {
      "fullname": "Video",
      "tasks": {
        "boundary": "Shot boundary detection",
        "general": "Search Task"
      },
      "webpage": "",
      "coordinators": "A.F. Smeaton, Dublin City Univ.:P. Over, National Institute of Standards and Technology (NIST):R. Taban, National Institute of Standards and Technology (NIST)",
      "description": "New in TREC-2001 was the Video Track, the goal of which was to promote progress in content-based retrieval from digital video via open, metric-based evaluation. The trak built on publily available video provided by the Open Video Project of the University of North Carolina at Chapel Hill under Gary Marchionini, the NIST Digital Video Library, and stock shot video provided for TREC-2001 by the British Broadcasting Corporation. The track used very nice work on shot boundary evaluation done as part of the ISIS Coordinated Researh Project (AIM, 2001).",
      "year": 2001
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t10i/t10i.html",
      "coordinators": "W. Hersh, Oregon Health Sciences Univ.:P. Over, National Institute of Standards and Technology (NIST)",
      "description": "In the TREC 2001 Interactive Track six research teams carried out observational studies which increased the realism of the searching by allowing the use of data and search systems/tools publicly accessible via the Internet. To the extent possible, searchers were allowed to choose tasks and systems/tools for accomplishing those tasks.",
      "year": 2001
    }
  },
  "trec11": {
    "xlingual": {
      "fullname": "Cross-Language",
      "tasks": {
        "arabic": "Arabic",
        "english": "English"
      },
      "webpage": "",
      "coordinators": "D.W. Oard, University of Maryland, College Park:F.C. Gey, University of California, Berkeley",
      "description": "Nine teams participated in the TREC-2002 cross-language information retrieval track, which focused on retrieving Arabic language documents based on 50 topics that were originally prepared in English. Arabic translations of the topic descriptions were also made available to facilitate monolingual Arabic runs. This was the second year in which a large Arabic document collection was available. Three new teams joined the evaluation, and the cross-language aspect of the evaluation received more attention this year than in TREC-2001. A set of standard linguistic resources was made available to facilitate cross-system comparisons, and their use as a contrastive condition was encouraged. Unique contributions to the relevance pools were more typical of previous TREC evaluations then the results of TREC-2001 had been for the same document collection, with no run uniquely contributing more than 6% of the known relevant documents.",
      "year": 2002
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "distillation": "Topic Distillation",
        "namedpage": "Named Page Finding"
      },
      "webpage": "https://trec.nist.gov/data/t11.web.html",
      "coordinators": "N. Craswell, CSIRO:and D. Hawking, CSIRO",
      "description": "The TREC-2002 Web Track moved away from non-Web relevance ranking and towards Webspecific tasks on a 1.25 million page crawl “.GOV”. The topic distillation task involved finding pages which were relevant, but also had characteristics which would make them desirable inclusions in a distilled list of key pages. The named page task is a variant of last year’s homepage finding task. The task is to find a particular page, but in this year’s task the page need not be a home page.",
      "year": 2002
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main Task",
        "list": "List Task"
      },
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC question answering track is an effort to bring the benefits of large-scale evaluation to bear on the question answering problem. The track contained two tasks in TREC 2002, the main task and the list task. Both tasks required that the answer strings returned by the systems consist of nothing more or less than an answer in contrast to the text snippets containing an answer allowed in previous years. A new evaluation measure in the main task, the confidence-weighted score, tested a system’s ability to recognize when it has found a correct answer.",
      "year": 2002
    },
    "filtering": {
      "fullname": "Filtering",
      "tasks": {
        "adaptive": "Adaptive filtering",
        "batch": "Batch filtering",
        "routing": "Routing"
      },
      "webpage": "https://trec.nist.gov/data/filtering/T11filter_guide.html",
      "coordinators": "S. Robertson, Microsoft Research:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "Given a topic description and some example relevant documents, build a filtering profile which will select the most relevant examples from an incoming stream of documents.  In the TREC 2002 filtering task we will continue to stress adaptive filtering. However, the batch filtering and routing tasks will also be available.",
      "year": 2002
    },
    "novelty": {
      "fullname": "Novelty",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t11_novelty/novelty_guidelines.html",
      "coordinators": "D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The TREC 2002 novelty track is designed to investigate systems' abilities to locate relevant AND new information within the ranked set of documents retrieved in answer to a TREC topic. This track is new for TREC 2002 and should be regarded as an interesting (and hopefully fun) learning experience.",
      "year": 2002
    },
    "interactive": {
      "fullname": "Interactive",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/t11_interactive/t11i.html",
      "coordinators": "W. Hersh, Oregon Health and Science University",
      "description": "TREC is organized along several tracks, each of which addresses a particular retrieval problem or scenario. The high-level goal of the Interactive Track is the investigation of searching as an interactive task by examining the process as well as the outcome. The remainder of this page comprises information basic to the definition of the track for TREC-11. For additional information about TREC and its schedules see the TREC homepage.",
      "year": 2002
    },
    "video": {
      "fullname": "Video",
      "tasks": "",
      "webpage": "",
      "coordinators": "A.F. Smeaton, Dublin City University:P. Over, National Institute of Standards and Technology (NIST)",
      "description": "TREC-2002 saw the second running of the Video Track, the goal of which was to promote progress in content-based retrieval from digital video via open, metrics-based evaluation. The track used 73.3 hours of publicly available digital video (in MPEG1/VCD format) downloaded by the participants directly from the Internet Archive (Prelinger Archives) and some from the Open Video Project. The material comprised advertising, educational, industrial, and amateur films produced between the 1930’s and the 1970’s by corporations, nonprofit organizations, trade associations, community and interest groups, educational institutions, and individuals. 17 teams representing 5 companies and 12 universities — 4 from Asia, 9 from Europe, and 4 from the US — participated in one or more of three tasks in the 2001 video track: shot boundary determination, feature extraction, and search (manual or interactive). Results were scored by NIST using manually created truth data for shot boundary determination and manual assessment of feature extraction and search results.",
      "year": 2002
    }
  },
  "trec12": {
    "genomics": {
      "fullname": "Genomics",
      "tasks": {
        "primary": "Primary Task",
        "secondary": "Secondary Task"
      },
      "webpage": "https://dmice.ohsu.edu/trec-gen/",
      "coordinators": "W. Hersh Oregon Health and Science University:R.T. Bhupatiraju, Oregon Health and Science University",
      "description": "The first year of TREC Genomics Track featured two tasks: ad hoc retrieval and information extraction. Both tasks centered around the Gene Reference into Function (GeneRIF) resource of the National Library of Medicine, which was used as both pseudorelevance judgments for ad hoc document retrieval as well as target text for information extraction. The track attracted 29 groups who participated in one or both tasks.",
      "year": 2003
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "distillation": "Topic distillation",
        "knownitem": "Navigational"
      },
      "webpage": "https://trec.nist.gov/data/t12.web.html",
      "coordinators": "N. Craswell, CSIRO ICT Centre:D. Hawking, CSIRO ICT Centre",
      "description": "The TREC 2003 web track consisted of both a non-interactive stream and an interactive stream. Both streams worked with the .GOV test collection. The non-interactive stream continued an investigation into the importance of homepages in Web ranking, via both a Topic Distillation task and a Navigational task. In the topic distillation task, systems were expected to return a list of the homepages of sites relevant to each of a series of broad queries. This differs from previous homepage experiments in that queries may have multiple correct answers. The navigational task required systems to return a particular desired web page as early as possible in the ranking in response to queries. In half of the queries, the target answer was the homepage of a site and the query was derived from the name of the site (Homepage finding) while in the other half, the target answers were not homepages and the queries were derived from the name of the page (Named page finding). The two types of query were arbitrarily mixed and not identified. The interactive stream focused on human participation in a topic distillation task over the .GOV collection. Studies conducted by the two participating groups compared a search engine using automatic topic distillation features with the same engine with those features disabled in order to determine whether the automatic topic distillation features assisted the users in the performance of their tasks and whether humans could achieve better results than the automatic system.",
      "year": 2003
    },
    "hard": {
      "fullname": "HARD",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20031230204716/https://ciir.cs.umass.edu/research/hard/guidelines.html/",
      "coordinators": "J. Allan, University of Massachusetts Amherst",
      "description": "The goal of this track is to bring the user out of hiding, making him or her an integral part of both the search process and the evaluation. Systems do not have just a query to chew on, but also have as much information as possible about the person making the request, ranging from biographical data, through information seeking context, to expected type of result. The HARD track is a variant of the ad-hoc retrieval task from the past. It was a “pilot” track in 2003 because of the substantial extension on past evaluation—i.e., it is not clear how best to evaluate some of the aspects of the track, so at least for this year it was intended to be very open ended.",
      "year": 2003
    },
    "robust": {
      "fullname": "Robust",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/robust.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The robust retrieval track is a new track in TREC 2003. The goal of the track is to improve the consistency of retrieval technology by focusing on poorly performing topics. In addition, the track brings back a classic, ad hoc retrieval task to TREC that provides a natural home for new participants.",
      "year": 2003
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main task",
        "passages": "Passages"
      },
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC 2003 question answering track contained two tasks, the passages task and the main task. In the passages task, systems returned a single text snippet in response to factoid questions; the evaluation metric was the number of snippets that contained a correct answer. The main task contained three separate types of questions, factoid questions, list questions, and definition questions. Each of the questions was tagged as to its type and the different question types were evaluated separately. The final score for a main task run was a combination of the scores for the separate question types.",
      "year": 2003
    },
    "novelty": {
      "fullname": "Novelty",
      "tasks": {
        "task1": "Task 1",
        "task2": "Task 2",
        "task3": "Task 3",
        "task4": "Task 4"
      },
      "webpage": "https://trec.nist.gov/data/t12_novelty/novelty03.guidelines.html",
      "coordinators": "I. Soboroff, National Institute of Standards and Technology (NIST):D. Harman, National Institute of Standards and Technology (NIST)",
      "description": "The Novelty Track is designed to investigate systems' abilities to locate relevant AND new information within a set of documents relevant to a TREC topic. Systems are given the topic and a set of relevant documents ordered by date, and must identify sentences containing relevant and/or new information in those documents.",
      "year": 2003
    }
  },
  "trec13": {
    "genomics": {
      "fullname": "Genomics",
      "tasks": {
        "adhoc": "Adhoc Retrieval",
        "triage": "Triage Subtask",
        "ann": "Annotation Subtask"
      },
      "webpage": "https://dmice.ohsu.edu/trec-gen/",
      "coordinators": "W.R. Hersh, Oregon Health & Science University:R.T. Bhuptiraju, Oregon Health & Science University:L. Ross, Oregon Health & Science University:A.M. Cohen, Oregon Health & Science University:D.F. Kraemer, Oregon Health & Science University:P. Johnson, Biogen Idec Corporation",
      "description": "The TREC 2004 Genomics Track consisted of two tasks. The first task was a standard ad hoc retrieval task using topics obtained from real biomedical research scientists and documents from a large subset of the MEDLINE bibliographic database. The second task focused on categorization of full-text documents, simulating the task of curators of the Mouse Genome Informatics (MGI) system and consisting of three subtasks. One subtask focused on the triage of articles likely to have experimental evidence warranting the assignment of GO terms, while the other two subtasks focused on the assignment of the three top-level GO categories.",
      "year": 2004
    },
    "HARD": {
      "fullname": "HARD",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20051201144933/https://ciir.cs.umass.edu/research/hard/guidelines.html/",
      "coordinators": "J.Allan, University of Massachusetts Amherst",
      "description": "The HARD track of TREC 2004 aims to improve the accuracy of information retrieval through the use of three techniques: (1) query metadata that better describes the information need, (2) focused and time-limited interaction with the searcher through “clarification forms”, and (3) incorporation of passage-level relevance judgments and retrieval. Participation in all three aspects of the track was excellent this year with about 10 groups trying something in each area. No group was able to achieve huge gains in effectiveness using these techniques, but some improvements were found and enthusiasm for the clarification forms (in particular) remains high.",
      "year": 2004
    },
    "novelty": {
      "fullname": "Novelty",
      "tasks": {
        "task1": "Task 1",
        "task2": "Task 2",
        "task3": "Task 3",
        "task4": "Task 4"
      },
      "webpage": "https://trec.nist.gov/data/t13_novelty/novelty04.guidelines.html",
      "coordinators": "I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Novelty Track is designed to investigate systems' abilities to locate relevant AND new information within a set of documents relevant to a TREC topic. Systems are given the topic and a set of relevant documents ordered by date, and must identify sentences containing relevant and/or new information in those documents.",
      "year": 2004
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/qamain.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC 2004 Question Answering track contained a single task in which question series were used to define a set of targets. Each series contained factoid and list questions and related to a single target. The final question in the series was an “Other” question that asked for additional information about the target that was not covered by previous questions in the series. Each question type was evaluated separately with the final score a weighted average of the different component scores. Applying the combined measure on a per-series basis produces a QA task evaluation that more closely mimics classic document retrieval evaluation.",
      "year": 2004
    },
    "robust": {
      "fullname": "Robust",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/robust/04.guidelines.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the Robust track is to improve the consistency of retrieval technology by focusing on poorly performing topics. In addition, the track brings back a classic, ad hoc retrieval task in TREC that provides a natural home for new participants. An ad hoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and submit a ranking of the top 1000 documents for that topic.",
      "year": 2004
    },
    "terabyte": {
      "fullname": "Terabyte",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/terabyte/04/04.guidelines.html",
      "coordinators": "C. Clarke, University of Waterloo:N. Craswell, Microsoft Research:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the Terabyte Track is to develop an evaluation methodology for terabyte-scale document collections. This year's track uses a 426GB collection of Web data from the .gov domain. While this collection is less than a full terabyte in size, it is considerably larger than the collections used in previous TREC tracks. In future years, we plan to expand the collection using data from other sources.",
      "year": 2004
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "mixed": "Mixed query",
        "classification": "Query classification"
      },
      "webpage": "https://trec.nist.gov/data/t13.web.html",
      "coordinators": "N. Craswell, MSR Cambridge:D. Hawking, CSIRO",
      "description": "This year's main experiment involved processing a mixed query stream, with an even mix of each query type studied in TREC-2003: 75 homepage finding queries, 75 named page finding queries and 75 topic distillation queries. The goal was to find ranking approaches which work well over the 225 queries, without access to query type labels.",
      "year": 2004
    }
  },
  "trec14": {
    "spam": {
      "fullname": "Spam",
      "tasks": "",
      "webpage": "https://plg.uwaterloo.ca/~gvcormac/spam/",
      "coordinators": "G. Cormack, University of Waterloo:T. Lynam, University of Waterloo",
      "description": "An automatic spam filter classifies a chronological sequence of email messages as SPAM or HAM (non-spam). The subject filter is run on several email sequences, some public and some private. The performance of the filter is measured with respect to gold standard judgements by a human assessor.",
      "year": 2005
    },
    "terabyte": {
      "fullname": "Terabyte",
      "tasks": {
        "adhoc": "Adhoc retrieval",
        "namedpage": "Named page finding",
        "efficiency": "Efficiency"
      },
      "webpage": "https://plg.uwaterloo.ca/~claclark/TB05.html",
      "coordinators": "C.L.A. Clarke, University of Waterloo:F. Scholer, Royal Melbourne Institute of Technology (RMIT University):I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The primary goal of the Terabyte Track is to develop an evaluation methodology for terabyte-scale document collections. In addition, we are interested in efficiency and scalability issues, which can be studied more easily in the context of a larger collection. Again this year, we are using a 426GB collection of Web data from the gov domain for all tasks. While this collection is less than a full terabyte in size, it is considerably larger than the collections used in previous TREC tracks. In future years, we hope to expand the collection using data from other sources.",
      "year": 2005
    },
    "genomics": {
      "fullname": "Genomics",
      "tasks": {
        "adhoc": "Adhoc Task",
        "categorization": "Categorization Task"
      },
      "webpage": "https://dmice.ohsu.edu/trec-gen/",
      "coordinators": "W. Hersh, Oregon Health & Science University:A. Cohen, Oregon Health & Science University:J. Yang, Oregon Health & Science University:R.T. Bhupatiraju, Oregon Health & Science University:P. Roberts, Biogen Idec Corporation:M. Hearst, University of California, Berkeley",
      "description": "The TREC 2005 Genomics Track featured two tasks, an ad hoc retrieval task and four subtasks in text categorization. The ad hoc retrieval task utilized a 10-year, 4.5-million document subset of the MEDLINE bibliographic database, with 50 topics conforming to five generic topic types. The categorization task used a full-text document collection with training and test sets consisting of about 6,000 biomedical journal articles each. Participants aimed to triage the documents into categories representing data resources in the Mouse Genome Informatics database, with performance assessed via a utility measure.",
      "year": 2005
    },
    "HARD": {
      "fullname": "HARD",
      "tasks": {
        "baseline": "Baseline submission",
        "clarification": "Clarification from",
        "final": "Final submission"
      },
      "webpage": "https://web.archive.org/web/20051201144933/https://ciir.cs.umass.edu/research/hard/guidelines.html/",
      "coordinators": "J. Allan, University of Massachusetts Amherst",
      "description": "TREC 2005 saw the third year of the High Accuracy Retrieval from Documents (HARD) track. The HARD track explores methods for improving the accuracy of document retrieval systems, with particular attention paid to the start of the ranked list.",
      "year": 2005
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main Task",
        "document": "Document Ranking Task",
        "relationship": "Relationship Task"
      },
      "webpage": "https://trec.nist.gov/data/qa/t2005_qadata.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST):H.T. Dang, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the TREC QA track is to foster research on systems that retrieve answers rather than documents in response to a question. The focus is on systems that can function in unrestricted domains.",
      "year": 2005
    },
    "enterprise": {
      "fullname": "Enterprise",
      "tasks": {
        "discussion": "Email search (Discussion search)",
        "knownitem": "Email search (Known item)",
        "expert": "Expert search"
      },
      "webpage": "https://trec.nist.gov/data/enterprise.html",
      "coordinators": "N. Craswell, Microsoft Research Cambridge:A.P. de Vries, CWI:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the enterprise track is to conduct experiments with enterprise data — intranet pages, email archives, document repositories — that reflect the experiences of users in real organisations, such that for example, an email ranking technique that is effective here would be a good choice for deployment in a real multi-user email search application. This involves both understanding user needs in enterprise search and development of appropriate IR techniques.",
      "year": 2005
    },
    "robust": {
      "fullname": "Robust",
      "tasks": "",
      "webpage": "https://trec.nist.gov/data/robust/05/05.guidelines.html",
      "coordinators": "E.M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the Robust track is to improve the consistency of retrieval technology by focusing on poorly performing topics. This year, the track will investigate the effectiveness obtainable on a new document set for topics that are known to be difficult on a separate document set. For each topic, participants create a query and submit a ranking of the top 1000 documents for that topic.",
      "year": 2005
    }
  },
  "trec15": {
    "terabyte": {
      "fullname": "Terabyte",
      "tasks": {
        "adhoc": "Adhoc retrieval",
        "namedpage": "Named page finding",
        "efficiency": "Efficiency"
      },
      "webpage": "https://plg.uwaterloo.ca/~claclark/TB06.html",
      "coordinators": "S. Buttcher, University of Waterloo:C.L.A. Clarke, University of Waterloo:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The primary goal of the Terabyte Track is to develop an evaluation methodology for terabyte-scale document collections. In addition, we are interested in efficiency and scalability issues, which can be studied more easily in the context of a larger collection.",
      "year": 2006
    },
    "spam": {
      "fullname": "Spam",
      "tasks": "",
      "webpage": "https://plg.uwaterloo.ca/~gvcormac/spam/",
      "coordinators": "G. Cormack, University of Waterloo",
      "description": "The 2006 track will reprise the 2005 experiments with new filters and data, and will also investigate delayed feedback and active learning.",
      "year": 2006
    },
    "genomics": {
      "fullname": "Genomics",
      "tasks": "",
      "webpage": "https://dmice.ohsu.edu/trec-gen/",
      "coordinators": "W. Hersh, Oregon Health & Science University:A.M. Cohen, Oregon Health & Science University:P. Roberts, Oregon Health & Science University:H.K. Rekapalli, Oregon Health & Science University",
      "description": "The TREC Genomics Track implemented a new task in 2006 that focused on passage retrieval for question answering using full-text documents from the biomedical literature. A test collection of 162,259 full-text documents and 28 topics expressed as questions was assembled. Systems were required to return passages that contained answers to the questions. Expert judges determined the relevance of passages and grouped them into aspects identified by one or more Medical Subject Headings (MeSH) terms. Document relevance was defined by the presence of one or more relevant aspects. The performance of submitted runs was scored using mean average precision (MAP) at the passage, aspect, and document level. In general, passage MAP was low, while aspect and document MAP were somewhat higher.",
      "year": 2006
    },
    "enterprise": {
      "fullname": "Enterprise",
      "tasks": {
        "expert": "Expert Search Task",
        "discussion": "Email Discussion Search"
      },
      "webpage": "https://trec.nist.gov/data/enterprise.html",
      "coordinators": "I. Soboroff, National Institute of Standards and Technology (NIST):A.P. de Vries, CWI:N. Craswell, Microsoft Cambridge",
      "description": "The enterprise track began in TREC 2005 as the successor to the web track, and this is reflected in the tasks and measures. While the track takes much of its inspiration from the web track, the foci are on search at the enterprise scale, incorporating non-web data and discovering relationships between entities in the organization. As a result, we have created the first test collections for multi-user email search and expert finding.",
      "year": 2006
    },
    "blog": {
      "fullname": "Blog",
      "tasks": {
        "opinion": "Opinion Retrieval",
        "open_task": "Open Task"
      },
      "webpage": "https://www.dcs.gla.ac.uk/wiki/TREC-BLOG",
      "coordinators": "I. Ounis, University of Glasgow:C. Macdonald, University of Glasgow:M. de Rijke, University of Amsterdam:G. Mishne, University of Amsterdam:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Blog track began this year, with the aim to explore the information seeking behaviour in the blogosphere. For this purpose, a new large-scale test collection, namely the TREC Blog06 collection, has been created. In the first pilot run of the track in 2006, we had two tasks, a main task (opinion retrieval) and an open task. The opinion retrieval task focuses on a specific aspect of blogs: the opinionated nature of many blogs. The second task was introduced to allow participants the opportunity to influence the determination of a suitable second task (for 2007) on other aspects of blogs, such as the temporal/event-related nature of many blogs, or the severity of spam in the blogosphere.",
      "year": 2006
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main Task",
        "ciqa": "Complex Interactive Question Answering"
      },
      "webpage": "https://trec.nist.gov/data/qa/2006_qadata/qa.06.guidelines.html",
      "coordinators": "H.T. Dang, National Institute of Standards and Technology (NIST):J. Lin, University of Maryland, College Park:D. Kelly, University of North Carolina, Chapel Hill",
      "description": "The goal of the TREC QA track is to foster research on systems that retrieve answers rather than documents in response to a question. The focus is on systems that can function in unrestricted domains.",
      "year": 2006
    },
    "legal": {
      "fullname": "Legal",
      "tasks": "",
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "J.R. Baron, National Archives and Records Administration:D.D. Lewis, David D. Lewis Consulting:D.W. Oard, University of Maryland",
      "description": "The goal of the Legal Track at the Text Retrieval Conference (TREC) is to assess the ability of information retrieval techniques to meet the needs of the legal profession for tools and methods capable of helping with the retrieval of electronic business records, principally for use as evidence in civil litigation. In the USA, this problem is referred to as e-discovery. Like all TREC tracks, the Legal Track seeks to foster the development of a research community by providing a venue for shared development of evaluation resources (test collections) and baseline results to which future results can be compared.",
      "year": 2006
    }
  },
  "trec16": {
    "million-query": {
      "fullname": "Million Query",
      "tasks": {
        "01": "Task 1",
        "10": "Task 2"
      },
      "webpage": "https://web.archive.org/web/20090311232726/http://ciir.cs.umass.edu/research/million/",
      "coordinators": "J. Allan, University of Massachusetts Amherst:B. Carterette, University of Massachusetts Amherst:B. Dachev, University of Massachusetts Amherst:J. A. Aslam, Northeastern University:V. Pavlu, Northeastern University:E. Kanoulas, Northeastern University",
      "description": "The goal of this track is to run a retrieval task similar to standard ad-hoc retrieval, but to evaluate large numbers of queries incompletely, rather than a small number more completely.   Participants will run 10,000 queries and a random 1,000 or so will be evaluated. The corpus is the terabyte track's GOV2 corpus of roughly 25,000,000 .gov web pages, amounting to just under half a terabyte of data.",
      "year": 2007
    },
    "genomics": {
      "fullname": "Genomics",
      "tasks": "",
      "webpage": "https://dmice.ohsu.edu/trec-gen/",
      "coordinators": "W. Hersh, Oregon Health & Science University:A. Cohen, Oregon Health & Science University:L. Ruslen, Oregon Health & Science University:P. Roberts, Pfizer Corporation",
      "description": "The TREC 2007 Genomics Track employed an entity-based question-answering task. Runs were required to nominate passages of text from a collection of full-text biomedical journal articles to answer the topic questions. Systems were assessed not only for the relevance of passages retrieved, but also how many aspects (entities) of the topic were covered and how many relevant documents were retrieved. We also classified the features of runs to explore which ones were associated with better performance, although the diversity of approaches and the quality of their reporting prevented definitive conclusions from being drawn.",
      "year": 2007
    },
    "spam": {
      "fullname": "Spam",
      "tasks": "",
      "webpage": "https://plg.uwaterloo.ca/~gvcormac/spam/",
      "coordinators": "G. Cormack, University of Waterloo",
      "description": "TREC's Spam Track uses a standard testing framework that presents a set of chronologically ordered email messages a spam filter for classification. In the filtering task, the messages are presented one at at time to the filter, which yields a binary judgment (spam or ham [i.e. non-spam]) which is compared to a humanadjudicated gold standard. The filter also yields a spamminess score, intended to reflect the likelihood that the classified message is spam, which is the subject of post-hoc ROC (Receiver Operating Characteristic) analysis. ",
      "year": 2007
    },
    "legal": {
      "fullname": "Legal",
      "tasks": {
        "main": "Adhoc Task",
        "interactive": "Interactive and Relevance Feedback"
      },
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "S. Tomlinson, Open Text Corporation:D. W. Oard, University of Maryland, College Park:J. R. Baron, National Archives and Records Administration:P. Thompson, Dartmouth College",
      "description": "TREC 2007 was the second year of the Legal Track, which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. The track included three tasks: Ad Hoc (i.e., single-pass automatic) search, Relevance Feedback (two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass) and Interactive (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback).",
      "year": 2007
    },
    "qa": {
      "fullname": "Question Answering",
      "tasks": {
        "main": "Main Task",
        "ciqa": "Complex Interactive Question Answering"
      },
      "webpage": "https://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html",
      "coordinators": "H.T. Dang, National Institute of Standards and Technology (NIST):D. Kelly, University of North Carolina:J. Lin, University of Maryland, College Park",
      "description": "The TREC 2007 question answering (QA) track contained two tasks: the main task consisting of series of factoid, list, and “Other” questions organized around a set of targets, and the complex, interactive question answering (ciQA) task. The main task differed from previous years in that the document collection comprised blogs in addition to newswire documents, requiring systems to process diverse genres of unstructured text. The evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document but not to the overall document collection). The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs. Standing in for surrogate users, assessors interacted with QA systems live over the Web; this setup allowed participants to experiment with more complex interfaces but also revealed limitations in the ciQA design for evaluation of interactive systems.",
      "year": 2007
    },
    "enterprise": {
      "fullname": "Enterprise",
      "tasks": {
        "document": "Document Search",
        "expert": "Expert Search"
      },
      "webpage": "https://trec.nist.gov/data/enterprise.html",
      "coordinators": "P. Bailey, Microsoft, USA:A. P. de Vries, CWI, The Netherlands:N. Craswell, MSR Cambridge, UK:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the enterprise track is to conduct experiments with enterprise data that reflect the experiences of users in real organizations. This year, the track has introduced a new corpus with the goal to be more representative of real-world enterprise search, by involving actual members of the organization in the topic development process, performing their real work tasks.",
      "year": 2007
    },
    "blog": {
      "fullname": "Blog",
      "tasks": {
        "opinion": "Opinion-finding",
        "polarity": "Polarity opinion-finding",
        "feed": "Blog (feed) distillation"
      },
      "webpage": "https://www.dcs.gla.ac.uk/wiki/TREC-BLOG",
      "coordinators": "C. Macdonald, University of Glasgow:I. Ounis, University of Glasgow:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the Blog track is to explore the information seeking behaviour in the blogosphere. It aims to create the required infrastructure to facilitate research into the blogosphere and to study retrieval from blogs and other related applied tasks. The track was introduced in 2006 with a main opinion finding task and an open task, which allowed participants the opportunity to influence the determination of a suitable second task for 2007 on other aspects of blogs besides their opinionated nature.",
      "year": 2007
    }
  },
  "trec17": {
    "blog": {
      "fullname": "Blog",
      "tasks": {
        "baseline": "Baseline adhoc (blog post) retrieval",
        "opinion": "Opinion-finding (blog post) retrieval ",
        "polarity": "Polarity opinion-finding (blog post) retrieval",
        "feed": "Blog (feed) distillation"
      },
      "webpage": "https://www.dcs.gla.ac.uk/wiki/TREC-BLOG",
      "coordinators": "I. Ounis, University of Glasgow:C. Macdonald, University of Glasgow:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Blog track explores the information seeking behaviour in the blogosphere. The track was introduced in 2006, with a main pilot search task, namely the opinion-finding task. In TREC 2007, the track investigated two main tasks inspired by the analysis of a commercial blog-search query log: the opinion-finding task (i.e. “What do people think about X?”) and the blog distillation task (i.e. “Find me a blog with a principal, recurring interest in X.”).",
      "year": 2008
    },
    "million-query": {
      "fullname": "Million Query",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20090311232726/http://ciir.cs.umass.edu/research/million/",
      "coordinators": "J. Allan, University of Massachusetts:J. A. Aslam, Northeastern University:V. Pavlu, Northeastern University:E. Kanoulas, Northeastern University:B. Carterette, University of Delaware",
      "description": "The Million Query (1MQ) track ran for the second time in TREC 2008. The track is designed to serve two purposes: first, it is an exploration of ad-hoc retrieval over a large set of queries and a large collection of documents; second, it investigates questions of system evaluation, in particular whether it is better to evaluate using many shallow judgments or fewer thorough judgments.",
      "year": 2008
    },
    "enterprise": {
      "fullname": "Enterprise",
      "tasks": {
        "document": "Document Search",
        "expert": "Expert Search"
      },
      "webpage": "https://trec.nist.gov/data/enterprise.html",
      "coordinators": "K. Balog, University of Amsterdam:I. Soboroff, National Institute of Standards and Technology (NIST):P. Thomas, CSIRO:P. Bailey, Microsoft:N. Craswell, Microsoft:A. de Vries, CWI",
      "description": "The goal of the enterprise track is to conduct experiments with enterprise data that reflect the experiences of users in real organizations. This year, we continued with the CERC collection introduced in TREC 2007. Topics were developed in conjunction with CSIRO Enquiries, who field email and telephone questions about CSIRO research from the public.",
      "year": 2008
    },
    "legal": {
      "fullname": "Legal",
      "tasks": {
        "adhoc": "Adhoc",
        "feedback": "Relevance Feedback",
        "interactive": "Interactive"
      },
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "D. W. Oard, University of Maryland, College Park:B. Hedin, H5:S. Tomlinson, Open Text Corporation:J. R. Baron, National Archives and Records Administration",
      "description": "TREC 2008 was the third year of the Legal Track, which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. The track included three tasks: Ad Hoc (i.e., single-pass automatic search), Relevance Feedback (two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass) and Interactive (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback).",
      "year": 2008
    },
    "relfdbk": {
      "fullname": "Relevance Feedback",
      "tasks": {
        "A": "no relevance info (baseline retrieval)",
        "B": "1 relevant document",
        "C": "3 relevant documents and 3 non-relevant documents",
        "D": "10 judged documents",
        "E": "large amounts of judged documents"
      },
      "webpage": "https://trec.nist.gov/data/relevance.feedback08.html",
      "coordinators": "C. Buckley, Sabir Research:S. Robertson, Microsoft",
      "description": "There were 3 main goals for this track: 1. Target evaluating and comparing just the RF algorithm - all groups will work with exactly the same relevance judgments (for the most part).  (Next year, the relevance evidence groups can use will expand).  Hopefully compare both statistical and NLP-intensive use of relevance information (what makes a document relevant). 2. Establish good baseline RF results for multiple amounts of relevance info. 3. Try to establish, for these runs, the amount of improvement possible with more relevance info.",
      "year": 2008
    }
  },
  "trec18": {
    "relfdbk": {
      "fullname": "Relevance Feedback",
      "tasks": {
        "phase1": "Phase 1",
        "phase2": "Phase 2"
      },
      "webpage": "https://groups.google.com/g/trec-relfeed",
      "coordinators": "",
      "description": "In the second year, the concentration shifted to finding good sets of docs to base their retrieval on. Each participant submitted one or two sets of 5 docs for each topic, and 3-5 other participants ran with those docs, thus getting a nonsystem dependent score on how good those docs were.",
      "year": 2009
    },
    "chemical": {
      "fullname": "Chemical",
      "tasks": {
        "priorart": "Prior Art Task",
        "techsurv": "Technology Survey Task"
      },
      "webpage": "https://trec.nist.gov/data/chemical09.html",
      "coordinators": "M. Lupu, Information Retrieval Facility (IRF):F. Piroi, Information Retrieval Facility (IRF):J. Tait, Information Retrieval Facility (IRF):J. Huang, York University:J. Zhu, University College London",
      "description": "TREC 2009 was the first year of the Chemical IR Track, which focuses on evaluation of search techniques for discovery of digitally stored information on chemical patents and academic journal articles. The track included two tasks: Prior Art (PA) and Technical Survey (TS) tasks.",
      "year": 2009
    },
    "legal": {
      "fullname": "Legal",
      "tasks": {
        "batch": "Batch",
        "interactive": "Interactive"
      },
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "B. Hedin, H5:S. Tomlinson, Open Text Corporation:J. R. Baron, National Archives and Records Administration:D. W. Oard, University of Maryland, College Park",
      "description": "TREC 2009 was the fourth year of the Legal Track, which focuses on evaluation of search technology for “discovery” (i.e., responsive review) of electronically stored information in litigation and regulatory settings. The track included two tasks: an Interactive task (in which real users could iteratively refine their queries and/or engage in multi-pass relevance feedback) and a Batch task (two-pass search in a controlled setting with some relevant and nonrelevant documents manually marked after the first pass).",
      "year": 2009
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc",
        "diversity": "Diversity"
      },
      "webpage": "https://plg.uwaterloo.ca/~trecweb/2009.html",
      "coordinators": "C.L.A. Clarke, University of Waterloo:N. Craswell, Microsoft:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Web Track explores and evaluates Web retrieval technologies. Currently, the Web Track conducts experiments using the new billion-page ClueWeb09 collection. The TREC 2009 track is the successor to the Terabyte Retrieval Track, which ran from 2004 to 2006, and to the older Web Track, which ran from 1999 to 2003. The TREC 2009 Web Track includes both a traditional adhoc retrieval task and a new diversity task.",
      "year": 2009
    },
    "million-query": {
      "fullname": "Million Query",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20100610074201/https://udel.edu/mailman/listinfo/trec-million09",
      "coordinators": "B. Carterette, University of Delaware:H. Fang, University of Delaware:V. Pavlu, Northeastern University:E. Kanoulas, University of Sheffield",
      "description": "The Million Query Track ran for the third time in 2009. The track is designed to serve two purposes: first, it is an exploration of ad hoc retrieval over a large set of queries and a large collection of documents; second, it investigates questions of system evaluation, in particular whether it is better to evaluate using many queries judged shallowly or fewer queries judged thoroughly",
      "year": 2009
    },
    "blog": {
      "fullname": "Blog",
      "tasks": {
        "topstories": "Top Stories Identification",
        "feed": "Faceted Blog Distillation"
      },
      "webpage": "https://www.dcs.gla.ac.uk/wiki/TREC-BLOG",
      "coordinators": "C. Macdonald, University of Glasgow:I. Ounis, University of Glasgow:I. Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Blog track explores the information seeking behaviour in the blogosphere. Thus far, since its inception in 2006, the Blog track addressed two main search tasks based on the analysis of a commercial blog search engine: the opinion-finding task (i.e. “What do people think about X?”) and the blog distillation task (i.e. “Find me a blog with a principal, recurring interest in X.”)",
      "year": 2009
    },
    "entity": {
      "fullname": "Entity",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20110811014305/http://ilps.science.uva.nl/trec-entity/",
      "coordinators": "K. Balog, University of Amsterdam:A. de Vries, CWI:P. Serdyukov, TU Delft:P. Thomas, CSIRO:T. Westerveld, Teezir",
      "description": "The goal of the entity track is to perform entity-oriented search tasks on the World Wide Web. Many user information needs would be better answered by specific entities instead of just any type of documents.",
      "year": 2009
    }
  },
  "trec19": {
    "blog": {
      "fullname": "Blog",
      "tasks": {
        "blfeed": "Baseline Blog Distillation",
        "feed": "Faceted Blog Distillation",
        "topstories": "Top Stories Identification",
        "newsblogpost": "News Blog Post Ranking"
      },
      "webpage": "https://www.dcs.gla.ac.uk/wiki/TREC-BLOG",
      "coordinators": "Iadh Ounis, University of Glasgow:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Blog track aims to investigate the information seeking behaviour in the blogosphere. The track was initiated in 2006, and has used an incremental approach in tackling several search tasks by their level of difficulty.",
      "year": 2010
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc Task",
        "diversity": "Diversity Task",
        "spam": "Spam Task"
      },
      "webpage": "https://plg.uwaterloo.ca/~trecweb/2010.html",
      "coordinators": "Charles L.A. Clarke, University of Waterloo:Nick Craswell, Microsoft:Ian Soboroff, National Institute of Standards and Technology (NIST):Gordon V. Cormack, University of Waterloo",
      "description": "The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active for two years. For TREC 2010, the track includes three tasks: 1) an adhoc retrieval task, 2) a diversity task, and 3) a spam task. As we did for TREC 2009, we based our experiments on the billion-page ClueWeb09 data set created by the Language Technologies Institute at Carnegie Mellon University.",
      "year": 2010
    },
    "chemical": {
      "fullname": "Chemical",
      "tasks": {
        "priorart": "Prior Art Task",
        "techsurv": "Technology Survey Task"
      },
      "webpage": "https://trec.nist.gov/data/chemical10.html",
      "coordinators": "Mihai Lupu, York University:John Tait, York University:Jimmy Huang, York University:Jianhan Zhu, York University",
      "description": "The TREC Chemical IR Track is a domain-specific evaluation campaign working with documents containing specific lexica, including chemical formulas and specific names. The 2010 edition of the track also included supporting material in addition to text: images and structure information files. As in the previous year, we had two tasks: a patent focused prior-art (PA) task and a user-focused Technology Survey task (TS). The data collection includes patent files as well as scientific articles, together with their attachments, if any. Topics and relevance judgments were either automatically or manually created.",
      "year": 2010
    },
    "relfdbk": {
      "fullname": "Relevance Feedback",
      "tasks": {
        "baseline": "Baseline",
        "required": "First Submission",
        "required2": "Second Submission",
        "opt": "Optional Submission",
        "subtask": "Subtask Submission"
      },
      "webpage": "https://trec.nist.gov/data/relevance.feedback10.html",
      "coordinators": "Chris Buckley:Mark Smucker:Matt Lease",
      "description": "This is the third year of the TREC relevance feedback track. The first year concentrated on the RF algorithm itself. All participants were given the same sets of judged docs, and used their own algorithms to retrieve a new set of docs. In the second year, the concentration shifted to finding good sets of docs to base their retrieval on. Each participant submitted one or two sets of 5 docs for each topic, and 3-5 other participants ran with those docs, thus getting a nonsystem dependent score on how good those docs were.",
      "year": 2010
    },
    "legal": {
      "fullname": "Legal",
      "tasks": {
        "learning": "Learning",
        "interactive": "Interactive"
      },
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "Gordon V. Cormack, University of Waterloo:Maura R. Grossman, Wachtell, Lipton, Rosen & Katz:Bruce Hedin, H5:Douglas W. Oard, University of Maryland, College Park",
      "description": "TREC 2010 was the fifth year of the Legal Track, which focuses on evaluation of search technology for discovery of electronically stored information in litigation and regulatory settings. The TREC 2010 Legal Track consisted of two distinct tasks: the Learning task, in which participants were required to estimate the probability of relevance for each document in a large collection, given a seed set of documents, each coded as responsive or non-responsive; and the Interactive task, in which participants were required to identify all relevant documents using a human-in-the-loop process.",
      "year": 2010
    },
    "session": {
      "fullname": "Session",
      "tasks": {
        "RL1": "RL1",
        "RL2": "RL2",
        "RL3": "RL3"
      },
      "webpage": "http://ir.cis.udel.edu/sessions",
      "coordinators": "Evangelos Kanoulas, University of Sheffield:Paul Clough, University of Sheffield:Ben Carterette, University of Delaware:Mark Sanderson, Royal Melbourne Institute of Technology (RMIT University)",
      "description": "Research in Information Retrieval has traditionally focused on serving the best results for a single query. In practice however users often enter queries in sessions of reformulations. The Sessions Track at TREC 2010 implements an initial experiment to evaluate the effectiveness of retrieval systems over single query reformulations.",
      "year": 2010
    },
    "entity": {
      "fullname": "Entity",
      "tasks": {
        "main": "Related Entity Finding",
        "elc": "Entity List Completion"
      },
      "webpage": "https://web.archive.org/web/20110811014305/http://ilps.science.uva.nl/trec-entity/",
      "coordinators": "Krisztian Balog, Norwegian University of Science and Technology (NTNU):Pavel Serdyukov, Yandex, Russia:Arjen P. de Vries, CWI",
      "description": "The overall goal of the track is to perform entity-oriented search tasks on the World Wide Web. Many user information needs concern entities (people, organizations, locations, products, ...); these are better answered by returning specific objects instead of just any type of documents.",
      "year": 2010
    }
  },
  "trec20": {
    "entity": {
      "fullname": "Entity",
      "tasks": {
        "ref": "Related Entity Finding ",
        "reflod": "Related Entity Finding using Linked Open Data",
        "elc": "Entity List Completion"
      },
      "webpage": "https://web.archive.org/web/20110811014305/http://ilps.science.uva.nl/trec-entity/",
      "coordinators": "Krisztian Balog, NTNU:Pavel Serdyukov, Yandex:Arjen P. de Vries, CWI",
      "description": "The TREC Entity track aimed to build test collections to evaluate entity-oriented search on Web data. In 2011, the track worked with two corpora: the ClueWeb 2009 web corpus and the new Sindice-2011 dataset.",
      "year": 2011
    },
    "microblog": {
      "fullname": "Microblog",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20120604072950/http://sites.google.com/site/microblogtrack/",
      "coordinators": "Iadh Ounis, University of Glasgow:Craig Macdonald, University of Glasgow:Jimmy Lin, Twitter and University of Maryland, College Park:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Microblog track examines search tasks and evaluation methodologies for information seeking behaviours in microblogging environments such as Twitter. It was first introduced in 2011, addressing a real-time adhoc search task, whereby the user wishes to see the most recent but relevant information to the query. In particular, systems should respond to a query by providing a list of relevant tweets ordered from newest to oldest, starting from the time the query was issued.",
      "year": 2011
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc",
        "diversity": "Diversity"
      },
      "webpage": "https://plg.uwaterloo.ca/~trecweb/2011.html",
      "coordinators": "Charles L. A. Clarke, University of Waterloo:Nick Craswell, Microsoft:Ian Soboroff, National Institute of Standards and Technology (NIST):Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active since TREC 2009, where it included both a traditional adhoc retrieval task and a new diversity task. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For TREC 2010 the track introduced a new Web spam task and Web-style, six-level relevance assessment for the adhoc task. For TREC 2011, as recommended by participants at the track planning session held during TREC 2010, we dropped the spam task but continued the other tasks essentially unchanged. As we did for TREC 2009 and TREC 2010, we based our TREC 2011 experiments on the billion-page ClueWeb09 collection created by the Language Technologies Institute at Carnegie Mellon University",
      "year": 2011
    },
    "legal": {
      "fullname": "Legal",
      "tasks": "",
      "webpage": "http://trec-legal.umiacs.umd.edu/",
      "coordinators": "Maura R. Grossman, Wachtell, Lipton, Rosen & Katz:Gordon V. Cormack, University of Waterloo:Bruce Hedin, H5:Douglas W. Oard, University of Maryland",
      "description": "The TREC 2011 Legal Track consisted of a single task: the learning task, which captured elements of both the TREC 2010 learning and interactive tasks. Participants were required to rank the entire corpus of 685,592 documents by their estimate of the probability of responsiveness to each of three topics, and also to provide a quantitative estimate of that probability. Participants were permitted to request up to 1,000 responsiveness determinations from a Topic Authority for each topic. Participants elected either to use only these responsiveness determinations in preparing automatic submissions, or to augment these determinations with their own manual review in preparing technologyassisted submissions.",
      "year": 2011
    },
    "chemical": {
      "fullname": "Chemical",
      "tasks": {
        "priorart": "Prior Art Task",
        "techsurv": "Technology Survey Task",
        "image2struct": "Image-to-Structure Task"
      },
      "webpage": "https://trec.nist.gov/data/chemical11.html",
      "coordinators": "Mihai Lupu, Vienna University of Technology:Zhao Jiashu, York University:Jimmy Huang, York University:Harsha Gurulingappa, Fraunhofer SCAI:Juliane Fluck, Fraunhofer SCAI:Marc Zimmerman, Fraunhofer SCAI:Igor Filippov, National Institutes of Health:John Tait, johntait.net Ltd.",
      "description": "The third year of the Chemical IR evaluation track benefitted from the support of many more people interested in the domain, as shown by the number of co-authors of this overview paper. We continued the two tasks we had before, and introduced a new task focused on chemical image recognition. The objective is to gradually move towards systems really useful to the practitioners, and in chemistry, this involves both text and images.",
      "year": 2011
    },
    "medical": {
      "fullname": "Medical",
      "tasks": "",
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "",
      "description": "The search task within the TREC 2011 Medical Records track was an ad hoc search task that modeled the clinical task of finding cohorts for comparative effectiveness research. The document set used in the track was based on a set of de-identified clinical reports made available by the University of Pittsburgh's BLULab NLP Repository.  This document set is not available to non-participants. There is a many-to-one mapping between reports and ''visits'', where a visit is an individual patient's single stay at a hospital. The visit was used as the unit of retrieval in the track (so a document for the purposes of the track was the union of the content in all records mapped to that visit).",
      "year": 2011
    },
    "session": {
      "fullname": "Session",
      "tasks": {
        "RL1": "RL1",
        "RL2": "RL2",
        "RL3": "RL3",
        "RL4": "RL4"
      },
      "webpage": "http://ir.cis.udel.edu/sessions",
      "coordinators": "Evangelos Kanoulas, University of Sheffield:Mark Hall, University of Sheffield:Paul Clough, University of Sheffield:Ben Carterette, University of Delaware:Mark Sanderson, Royal Melbourne Institute of Technology (RMIT University)",
      "description": "The TREC Session track ran for the second time in 2011. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions",
      "year": 2011
    },
    "crowd": {
      "fullname": "Crowdsourcing",
      "tasks": {
        "task1": "Task 1",
        "task2": "Task 2"
      },
      "webpage": "https://trec.nist.gov/data/crowd.html",
      "coordinators": "",
      "description": "",
      "year": 2011
    }
  },
  "trec21": {
    "microblog": {
      "fullname": "Microblog",
      "tasks": {
        "adhoc": "Real-time Adhoc Task",
        "filtering": "Filtering Task"
      },
      "webpage": "https://web.archive.org/web/20120604072950/http://sites.google.com/site/microblogtrack/",
      "coordinators": "Ian Soboroff, National Institute of Standards and Technology (NIST):Iadh Ounis, Craig Macdonald, University of Glasgow:Jimmy Lin, University of Maryland",
      "description": "The Microblog track examines search tasks and evaluation methodologies for information seeking behaviours in microblogging environments such as Twitter. It was first introduced in 2011, addressing a real-time adhoc search task, whereby the user wishes to see the most recent relevant information to the query. In 2012, the realtime adhoc task was changed slightly, and a new filtering task was added. The filtering task models a standing query where the user wishes to see relevant tweets as they are posted.",
      "year": 2012
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc",
        "diversity": "Diversity"
      },
      "webpage": "https://plg.uwaterloo.ca/~trecweb/2012.html",
      "coordinators": "Charles L. A. Clarke, University of Waterloo:Nick Craswell, Microsoft:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Web Track explores and evaluates Web retrieval technology over large collections of Web data. In its current incarnation, the Web Track has been active since TREC 2009, where it included both a traditional adhoc retrieval task and a new diversity task. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. For TREC 2010 the track introduced a new Web spam task. For both TREC 2011 and 2012, we dropped the spam task but continued the other two tasks essentially unchanged. As we did since TREC 2009, we based our TREC 2012 experiments on the billion-page ClueWeb09 collection created by the Language Technologies Institute at Carnegie Mellon University",
      "year": 2012
    },
    "context": {
      "fullname": "Contextual Suggestion",
      "tasks": "",
      "webpage": "https://sites.google.com/site/treccontext/",
      "coordinators": "Adriel Dean-Hall, University of Waterloo:Charles L. A. Clarke, University of Waterloo:Jaap Kamps, University of Amsterdam:Paul Thomas, CSIRO:Ellen Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests.",
      "year": 2012
    },
    "medical": {
      "fullname": "Medical",
      "tasks": "",
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William Hersh, Oregon Health & Science University",
      "description": "The TREC Medical Records track fosters research that allows electronic health records to be retrieved based on the semantic content of free-text fields. The ability to find records by matching semantic content will enhance clinical care and support the secondary use of medical records in clinical trials and epidemiological studies. TREC 2012 is the sophomore year of the track, which attracted 24 participating research groups.",
      "year": 2012
    },
    "session": {
      "fullname": "Session",
      "tasks": {
        "RL1": "RL1",
        "RL2": "RL2",
        "RL3": "RL3",
        "RL4": "RL4"
      },
      "webpage": "http://ir.cis.udel.edu/sessions",
      "coordinators": "Evangelos Kanoulas, Google:Ben Carterette, University of Delaware:Mark Hall, University of Sheffield:Paul Clough, University of Sheffield:Mark Sanderson, Royal Melbourne Institute of Technology (RMIT University)",
      "description": "The TREC Session track ran for the third time in 2012. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions",
      "year": 2012
    },
    "crowd": {
      "fullname": "Crowdsourcing",
      "tasks": {
        "trat": "Text Relevance Assessing Task",
        "irat": "Image Relevance Assessing Task"
      },
      "webpage": "https://trec.nist.gov/data/crowd.html",
      "coordinators": "Mark D. Smucker, University of Waterloo:Gabriella Kazai, Microsoft Research:Matthew Lease, University of Texas at Austin",
      "description": "In 2012, the Crowdsourcing track had two separate tasks: a text relevance assessing task (TRAT) and an image relevance assessing task (IRAT).",
      "year": 2012
    },
    "kba": {
      "fullname": "Knowledge Base Acceleration",
      "tasks": "",
      "webpage": "https://trec-kba.org/",
      "coordinators": "John R. Frank, Massachusetts Institute of Technology:Max Kleiman-Weiner, Massachusetts Institute of Technology:Daniel A. Roberts, Massachusetts Institute of Technology:Feng Niu, University of Wisconsin-Madison:Ce Zhang, University of Wisconsin-Madison:Christopher Re, University of Wisconsin-Madison:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Knowledge Base Acceleration track in TREC 2012 focused on a single task: filter a time-ordered corpus for documents that are highly relevant to a predefined list of entities. KBA differs from previous filtering evaluations in two primary ways: the stream corpus is >100x larger than previous filtering collections, and the use of entities as topics enables systems to incorporate structured knowledge bases (KB), such as Wikipedia, as external data sources. A successful KBA system must do more than resolve the meaning of entity mentions by linking documents to the KB: it must also distinguish centrally relevant documents that are worth citing in the entity’s WP article. This combines thinking from natural language processing (NLP) and information retrieval (IR). ",
      "year": 2012
    }
  },
  "trec22": {
    "kba": {
      "fullname": "Knowledge Base Acceleration",
      "tasks": {
        "kba-ssf-2013": "Streaming Slot Filling",
        "kba-ccr-2013": "Cumulative Citation Recommendation"
      },
      "webpage": "https://trec-kba.org/",
      "coordinators": "John R. Frank, Massachusetts Institute of Technology:Steven J. Bauer, Massachusetts Institute of Technology:Max Kleiman Weinre, Massachusetts Institute of Technology:Daniel A. Roberts, Massachusetts Institute of Technology:Nilesh Tripuraneni, Massachusetts Institute of Technology:Ce Zhang, Stanford University:Christopher Re, University of Wisconsin:Ellen Voorhees, Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Knowledge Base Acceleration (KBA) track in TREC 2013 expanded the entity-centric filtering evaluation from TREC KBA 2012. This track evaluates systems that filter a time-ordered corpus for documents and slot fills that would change an entity profile in a predefined list of entities. We doubled the size of the KBA streamcorpus to twelve thousand contiguous hours and a billion documents from blogs, news, and Web content",
      "year": 2013
    },
    "context": {
      "fullname": "Contextual Suggestion",
      "tasks": "",
      "webpage": "https://sites.google.com/site/treccontext/",
      "coordinators": "Adriel Dean-Hall, University of Waterloo:Charles L.A. Clarke, University of Waterloo:Nicole Simone, University of Waterloo:Jaap Kamps, University of Amsterdam:Paul Thomas, CSIRO:Ellen Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. For example, imagine an information retrieval researcher with a November evening to spend in Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse, dinner at the Flaming Pit, or even a trip into Washington on the metro to see the National Mall. The primary goal of this track is to develop evaluation methodologies for such systems.",
      "year": 2013
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc Retrieval",
        "risk": "Risk-sensitive Retrieval"
      },
      "webpage": "https://www.microsoft.com/en-us/research/project/trec-web-track-2013/",
      "coordinators": "Kevyn Collins-Thompson, University of Michigan:Paul Bennett, Fernando Diaz, Microsoft Research:Charlie Clarke, University of Waterloo:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the TREC Web track is to explore and evaluate retrieval approaches over large-scale subsets of the Web – currently on the order of one billion pages.",
      "year": 2013
    },
    "federated": {
      "fullname": "Federated Web Search",
      "tasks": {
        "selection": "Resource Selection",
        "merging": "Results Merging"
      },
      "webpage": "http://sites.google.com/site/trecfedweb",
      "coordinators": "Thomas Demeester, Ghent Univesity:Dolf Trieschnigg, University of Twente:Dong Nguyen, University of Twente:Djoerd Hiemstra, University of Twente",
      "description": "The TREC Federated Web Search track is intended to promote research related to federated search in a realistic web setting, and hereto provides a large data collection gathered from a series of online search engines. This overview paper discusses the results of the first edition of the track, FedWeb 2013. The focus was on basic challenges in federated search: (1) resource selection, and (2) results merging.",
      "year": 2013
    },
    "microblog": {
      "fullname": "Microblog",
      "tasks": "",
      "webpage": "https://github.com/lintool/twitter-tools/wiki",
      "coordinators": "Jimmy Lin, University of Maryland:Miles Efron, University of Illinois",
      "description": "This year represents the third iteration of the TREC Microblog track, which began in 2011. There was no substantive change in the task definition, which remains nominally real-time search, best summarized as “At time T, give me the most relevant tweets about topic X.” However, we introduced a radically different evaluation methodology, dubbed “evaluation as a service”, which attempted to address deficiencies in how the document collection was distributed in previous years. This is the first time such an approach has been deployed at TREC. Overall, we believe that the evaluation methodology was successful, drawing participation from twenty groups around the world.",
      "year": 2013
    },
    "tempsumm": {
      "fullname": "Temporal Summarization",
      "tasks": {
        "sus": "Sequential Update Summarization",
        "vt": "Value Tracking"
      },
      "webpage": "https://web.archive.org/web/20170618023232/http://www.trec-ts.org/",
      "coordinators": "Javed Aslam, Northeastern University:Matthew Ekstrand-Abueg, Northeastern University:Virgil Pavlu, Northeastern University:Fernando Diaz, Microsoft Research:Tetsuya Sakai, Waseda University",
      "description": "Unexpected news events such as earthquakes or natural disasters represent a unique information access problem where traditional approaches fail. For example, immediately after an event, the corpus may be sparsely populated with relevant content. Even when, after a few hours, relevant content is available, it is often inaccurate or highly redundant. At the same time, crisis events demonstrate a scenario where users urgently need information, especially if they are directly affected by the event. The goal of this track is to develop systems for efficiently monitoring the information associated with an event over time. Specifically, we are interested in developing systems which (1) can broadcast short, relevant, and reliable sentence-length updates about a developing event and (2) can track the value of important event-related attributes (e.g. number of fatalities).",
      "year": 2013
    },
    "session": {
      "fullname": "Session",
      "tasks": {
        "RL1": "RL1",
        "RL2": "RL2",
        "RL3": "RL3"
      },
      "webpage": "http://ir.cis.udel.edu/sessions",
      "coordinators": "Ben Carterette, University of Delaware:Ashraf Bah, University of Delaware:Evangelos Kanoulas, Google:Mark Hall, Edge Hill University:Paul Clough, University of Sheffield",
      "description": "The TREC Session track ran for the fourth time in 2013. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions.",
      "year": 2013
    },
    "crowd": {
      "fullname": "Crowdsourcing",
      "tasks": {
        "standard": "Standard",
        "basic": "Basic"
      },
      "webpage": "",
      "coordinators": "Mark D. Smucker, University of Waterloo:Gabriella Kazai, Microsoft Research:Matthew Lease, University of Texas at Austin",
      "description": "In 2013, the Crowdsourcing track partnered with the TREC Web Track and had a single task to crowdsource relevance judgments for a set of Web pages and search topics shared by the Web Track.",
      "year": 2013
    }
  },
  "trec23": {
    "clinical": {
      "fullname": "Clinical Decision Support",
      "tasks": "",
      "webpage": "http://www.trec-cds.org/",
      "coordinators": "Matthew S. Simpson, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William Hersh, Oregon Health and Science University",
      "description": "To make biomedical information more accessible and to meet the requirements for the meaningful use of electronic health records, a goal of modern clinical decision support systems is to anticipate the needs of physicians by linking electronic health records with information relevant for patient care. The Clinical Decision Support Track aims to simulate the requirements of such systems and to encourage the creation of tools and resources necessary for their implementation. The focus of the 2014 track was the retrieval of biomedical articles relevant for answering generic clinical questions about medical records. In the absence of a reusable, de-identified collection of medical records, we used short case reports, such as those published in biomedical articles, as idealized representations of actual medical records. A case report typically describes a challenging medical case, and it is often organized as a well-formed narrative summarizing the portions of a patient’s medical record that are pertinent to the case.",
      "year": 2014
    },
    "context": {
      "fullname": "Contextual Suggestion",
      "tasks": "",
      "webpage": "https://sites.google.com/site/treccontext/",
      "coordinators": "Adriel Dean-Hall, University of Waterloo:Charles L. A. Clarke, University of Waterloo:Jaap Kamps, University of Amsterdam:Paul Thomas, CSIRO:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The contextual suggestion track investigates search techniques for complex information needs that are highly dependent on context and user interests. For example, imagine an information retrieval researcher with a November evening to spend in Gaithersburg, Maryland. A contextual suggestion system might recommend a beer at the Dogfish Head Alehouse, dinner at the Flaming Pit, or even a trip into Washington on the metro to see the National Mall. The primary goal of this track is to develop evaluation methodologies for such systems.",
      "year": 2014
    },
    "microblog": {
      "fullname": "Microblog",
      "tasks": {
        "adhoc": "Adhoc",
        "ttg": "Timeline"
      },
      "webpage": "https://github.com/lintool/twitter-tools/wiki",
      "coordinators": "Jimmy Lin, University of Maryland:Yulu Wang, University of Maryland:Miles Efron- University of Illinois:Garrick Sherman, University of Illinois",
      "description": "This year represents the fourth iteration of the TREC Microblog track, which has been running since 2011. The track continued using the “evaluation as a service” model, in which participants had access to the document collection only through an API. In addition to the temporallyanchored ad hoc retrieval task, which has been running since the inception of the track, we introduced a new task called tweet timeline generation (TTG), where the goal is to produce concise “summaries” about a particular topic for human consumption.",
      "year": 2014
    },
    "web": {
      "fullname": "Web",
      "tasks": {
        "adhoc": "Adhoc Retrieval",
        "risk": "Risk-sensitive Retrieval"
      },
      "webpage": "https://www-personal.umich.edu/~kevynct/trec-web-2014/",
      "coordinators": "Kevyn Collins-Thompson, University of Michigan:Craig Macdonald, University of Glasgow:Paul Bennett, Microsoft Research:Fernando Diaz, Microsoft Research:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the TREC Web track over the past few years has been to explore and evaluate innovative retrieval approaches over large-scale subsets of the Web – currently using ClueWeb12, on the order of one billion pages. For TREC 2014, the sixth year of the Web track, we implemented the following significant updates compared to 2013. First, the risk-sensitive retrieval task was modified to assess the ability of systems to adaptively perform risk-sensitive retrieval against multiple baselines, including an optional selfprovided baseline. In general, the risk-sensitive task explores the tradeoffs that systems can achieve between effectiveness (overall gains across queries) and robustness (minimizing the probability of significant failure, relative to a particular provided baseline). Second, we added query performance prediction as an optional aspect of the risk-sensitive task. The Adhoc task continued as for TREC 2013, evaluated using both adhoc and diversity relevance criteria.",
      "year": 2014
    },
    "federated": {
      "fullname": "Federated Web Search",
      "tasks": {
        "resource": "Resource Selection",
        "vertical": "Vertical Selection",
        "merging": "Results Merging"
      },
      "webpage": "http://sites.google.com/site/trecfedweb",
      "coordinators": "Thomas Demeester, Ghent University:Dolf Trieschnigg, University of Twente:Dong Nguyen, University of Twente:Djoerd Hiemstra, University of Twente:Ke Zhou, Yahoo Labs London",
      "description": "The TREC Federated Web Search track facilitates research on federated web search, by providing a large realistic data collection sampled from a multitude of online search engines. The FedWeb 2013 Resource Selection and Results Merging tasks are again included in FedWeb 2014, and we additionally introduced the task of vertical selection. Other new aspects are the required link between the Resource Selection and Results Merging tasks, and the importance of diversity in the merged results. After an overview of the new data collection and relevance judgments, the individual participants’ results for the tasks are introduced, analyzed, and compared.",
      "year": 2014
    },
    "kba": {
      "fullname": "Knowledge Base Acceleration",
      "tasks": {
        "kba-ccr-2014": "Cumulative Citation Recommendation",
        "kba-ssf-2014": "Streaming Slot Filling"
      },
      "webpage": "https://trec-kba.org/",
      "coordinators": "John R. Frank, Diffeo:Max Kleiman-Weiner, Diffeo:Daniel A. Roberts, Diffeo:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Knowledge Base Acceleration (KBA) track ran in TREC 2012, 2013, and 2014 as an entitycentric filtering evaluation. This track evaluates systems that filter a time-ordered corpus for documents and slot fills that would change an entity profile in a predefined list of entities. Compared with the 2012 and 2013 evaluations, the 2014 evaluation introduced several refinements, including high-quality community metadata from running Raytheon/BBN’s Serif named entity recognizer, sentence parser, and relation extractor on 579,838,246 English documents in the corpus.",
      "year": 2014
    },
    "tempsumm": {
      "fullname": "Temporal Summarization",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20170618023232/http://www.trec-ts.org/",
      "coordinators": "Javed Aslam, Northeastern University:Matthew Ekstrand-Abueg, Northeastern University:Virgil Pavlu, Northeastern University:Fernando Diaz, Microsoft Research:Richard McCreadie, University of Glasgow:Tetsuya Sakai, Waseda University",
      "description": "News events such as protests, accidents or natural disasters represent a unique information access problem where traditional approaches fail. For example, immediately after an event, the corpus may be sparsely populated with relevant content. Even when, after a few hours, relevant content becomes available, it is often inaccurate or highly redundant. At the same time, crisis events demonstrate a scenario where users urgently need information, especially if they are directly affected by the event. The goal of this track is to develop systems for efficiently monitoring the information associated with an event over time. Specifically, we are interested in developing systems which can broadcast short, relevant, and reliable sentencelength updates about a developing event.",
      "year": 2014
    },
    "session": {
      "fullname": "Session",
      "tasks": {
        "RL1": "RL1",
        "RL2": "RL2",
        "RL3": "RL3"
      },
      "webpage": "http://ir.cis.udel.edu/sessions",
      "coordinators": "Ben Carterette, University of Delaware:Evangelos Kanoulas, Google:Mark Hall, Edge Hill University:Paul Clough, University of Sheffield",
      "description": "The TREC Session track ran for the fourth time in 2014. The track has the primary goal of providing test collections and evaluation measures for studying information retrieval over user sessions rather than one-time queries. These test collections are meant to be portable, reusable, statistically powerful, and open to anyone that wishes to work on the problem of retrieval over sessions.",
      "year": 2014
    }
  },
  "trec24": {
    "clinical": {
      "fullname": "Clinical Decision Support",
      "tasks": {
        "a": "Task A",
        "b": "Task B"
      },
      "webpage": "http://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, U.S. National Library of Medicine (NIH):Matthew S. Simpson, U.S. National Library of Medicine (NIH):Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William R. Hersh, Oregon Health and Science University",
      "description": "In making clinical decisions, physicians often seek out information about how to best care for their patients. Information relevant to a physician can be related to a variety of clinical tasks such as (i) determining a patient’s most likely diagnosis given a list of symptoms, (ii) determining if a particular test is indicated for a given situation, and (iii) deciding on the most effective treatment plan for a patient having a known condition. In some cases, physicians can find the information they seek in published biomedical literature. However, given the volume of the existing literature and the rapid pace at which new research is published, locating the most relevant and timely information for a particular clinical need can be a daunting and timeconsuming task. In order to make biomedical information more accessible and to meet the requirements for the meaningful use of electronic health records (EHRs), a goal of modern clinical decision support systems is to anticipate the needs of physicians by linking EHRs with information relevant for patient care. The goal of the 2015 TREC Clinical Decision Support (CDS) track was to evaluate biomedical literature retrieval systems for providing answers to generic clinical questions about patient cases. Short case reports, such as those published in biomedical articles and used in medical lectures, acted as idealized representations of medical records. A case report typically describes a challenging medical case. It is organized as a well-formed narrative summarizing the pertient portions of the patient’s medical record. Given a case, participants were challenged with retrieving full-text biomedical articles relevant for answering questions related to one of three generic clinical information needs. The three needs were: Diagnosis (i.e., “What is this patient’s diagnosis?”), Test (“What diagnostic test is appropriate for this patient?”), and Treatment (“What treatment is appropriate for this patient?”). Retrieved articles were judged relevant if they provided information of the specified type useful for the given case. The assessment was performed by physicians with training in biomedical informatics. The evaluation of individual submissions followed standard TREC procedures. The 2015 CDS track differed from the 2014 CDS track (Simpson et al., 2014) by offering two tasks. Task A mirrored the 2014 CDS track, only with 30 new topics/cases. Task B used the same topics from Task A, but included the patient diagnosis for the Test and Treatment topics. Since the diagnosis was not guaranteed to be written in the case (consistent with how physicians often write cases in practice), we theorized that providing the diagnosis may improve retrieval systems by (a) providing additional relevant information if the diagnosis is not stated in the case, or (b) emphasizing a key piece of information in the case if the diagnosis is stated.",
      "year": 2015
    },
    "microblog": {
      "fullname": "Microblog",
      "tasks": {
        "a": "Push notifications",
        "b": "Email digests"
      },
      "webpage": "https://github.com/lintool/twitter-tools/wiki",
      "coordinators": "Jimmy Lin, University of Waterloo:Miles Efron, University of Illinois:Garrick Sherman, University of Illinois:Yulu Wang, University of Marland:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC 2015 Microblog track introduced a single realtime filtering task broken down into two scenarios. Our goal is to explore techniques for monitoring streams of social media posts with respect to users’ interest profiles. An interest profile describes a topic about which the user wishes to receive information updates in real time, and is different from a typical ad hoc topic in that the profile represents a prospective (as opposed to a retrospective) information need. Thus, the nature of the desired information is qualitatively different. In real-time filtering, the goal is for a system to “push” (i.e., recommend, suggest) interesting and novel content to users in a timely fashion.",
      "year": 2015
    },
    "context": {
      "fullname": "Contextual Suggestion",
      "tasks": {
        "batch": "Batch task",
        "live": "Live task"
      },
      "webpage": "https://sites.google.com/site/treccontext/",
      "coordinators": "Adriel Dean-Hall, University of Waterloo:Charles L. A. Clarke, University of Waterloo:Jaap Kamps, University of Amsterdam:Julia Kiseleva Eindhoven, University of Technology:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Contextual Suggestion Track evaluates pointof-interest (POI) recommendation systems, with the goal of creating open and reusable test collections for this purpose. The track imagines a traveler in a unknown city seeking sites to see and things to do that reflect his or her own personal interests, as inferred from their interests in their home city. Given a user's profile, consisting of a POI list and rating from a home city, participants make recommendations for attractions in a target city (i.e., a new context).",
      "year": 2015
    },
    "tempsumm": {
      "fullname": "Temporal Summarization",
      "tasks": {
        "fs": "Full Filtering and Summarization",
        "ps": "Partial Filtering and Summarization",
        "so": "Summarization Only"
      },
      "webpage": "https://web.archive.org/web/20170618023232/http://www.trec-ts.org/",
      "coordinators": "Javed Aslam:Fernando Diaz:Matthew Ekstrand-Abueg:Richard McCreadie:Virgil Pavlu:Tetsuya Sakai",
      "description": "There are many summarization scenarios that require updates to be issued to users over time. For example, during unexpected news events such as natural disasters or mass protests new information rapidly emerges. The TREC Temporal Summarization track aims to investigate how to effectively summarize these types of event in real-time. In particular, the goal is to develop systems which can detect useful, new, and timely sentence-length updates about a developing event to return to the user. In contrast to classical summarization challenges (such as DUC or TAC), the summaries produced by the participant systems are evaluated against a ground truth list of information nuggets representing the space of information that a user might want to know about each event. An optimal summary will cover all of the information nuggets in the minimum number of sentences. Also in contrast to classic summarization and newer timeline generation tasks, the Temporal Summarization track focuses on performing this analysis online as documents are indexed.",
      "year": 2015
    },
    "task": {
      "fullname": "Tasks",
      "tasks": {
        "understanding": "Task Understanding",
        "completion": "Task Completion",
        "web": "Adhoc Retrieval"
      },
      "webpage": "http://www.cs.ucl.ac.uk/tasks-track-2015/",
      "coordinators": "Emine Yilmaz, University College London:Manisha Verma, University College London:Rishabh Mehrotra, University College London:Evangelos Kanoulas, University of Amsterdam:Ben Carterette, University of Delaware:Nick Craswell, Microsoft",
      "description": "Research in Information Retrieval has traditionally focused on serving the best results for a single query, ignoring the reasons (or the task) that might have motivated the user to submit that query. Often times search engines are used to complete complex tasks (information needs); achieving these tasks with current search engines requires users to issue multiple queries. For example, booking travel to a location such as London could require the user to submit various queries such as flights to London, hotels in London, points of interest around London, etc. Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefullness of the system in helping the user complete the actual task that led the user issue the query. The TREC 2015 Tasks Track is an attempt in devising mechanisms for evaluating quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks. In this overview, we first summarise the three categories of evaluation mechanisms used in the track and briefly describe the corpus, topics, and tasks that comprise the test collections. We then give an overview of the runs submitted to the Tasks Track and present evaluation results and analysis",
      "year": 2015
    },
    "recall": {
      "fullname": "Total Recall",
      "tasks": {
        "full": "At-Home Task",
        "sandbox": "Sandbox Task"
      },
      "webpage": "https://plg.uwaterloo.ca/~gvcormac/trecvm/",
      "coordinators": "Adam Roegiest, University of Waterloo:Gordon V. Cormack, University of Waterloo:Charles L.A. Clarke, University of Waterloo:Maura R. Grossman, Wachtell, Lipton, Rosen & Katz",
      "description": "The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall - as close as practicable to 100% - with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings, systematic review in evidencebased medicine, and the creation of fully labeled test collections for information retrieval (“IR”) evaluation. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which IR systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a “black box,” affording participants confidence that their proprietary systems cannot easily be reverse engineered.",
      "year": 2015
    },
    "domain": {
      "fullname": "Dynamic Domain",
      "tasks": "",
      "webpage": "http://trec-dd.org",
      "coordinators": "Hui Yang, Georgetown University:John Frank, Diffeo and MIT:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "Search tasks for professional searchers, such as law enforcement agencies, police officers, and patent examiners, are often more complex than open domain Web search tasks. When professional searchers look for relevant information, it is often the case that they need to go through multiple iterations of searches to interact with a system. The Dynamic Domain Track supports research in dynamic, exploratory search within complex information domains. By providing real-time fine-grained feedback with relevance judgments that was collected during assessing time to the participating systems, we create a dynamic and iterative search process that lasts until the system decides to stop. The search systems are expected to be able to adjust their retrieval algorithms based on the feedback and find quickly relevant information for a multi-faceted information need. This document reports the task, datasets, topic and assessment creation, submissions, and evaluation results for the TREC 2015 Dynamic Domain (DD) Track.",
      "year": 2015
    },
    "qa": {
      "fullname": "LiveQA",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20160219234556/https://sites.google.com/site/trecliveqa2015/",
      "coordinators": "Eugene Agichtein, Emory University:David Carmel, Yahoo Labs:Dan Pelleg, Yahoo Labs:Yuval Pinter, Yahoo Labs:Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The automated question answering (QA) track, one of the most popular tracks in TREC for many years, has focused on the task of providing automatic answers for human questions. The track primarily dealt with factual questions, and the answers provided by participants were extracted from a corpus of News articles. While the task evolved to model increasingly realistic information needs, addressing question series, list questions, and even interactive feedback, a major limitation remained: the questions did not directly come from real users, in real time. The LiveQA track, conducted for the first time this year, focused on realtime question answering for real-user questions. Real user questions, i.e., fresh questions submitted on the Yahoo Answers (YA) site that have not yet been answered, were sent to the participant systems, which provided an answer in real time. Returned answers were judged by TREC editors on a 4-level Likert scale.",
      "year": 2015
    }
  },
  "trec25": {
    "clinical": {
      "fullname": "Clinical Decision Support",
      "tasks": "",
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, The University of Texas Health Science Center:Dina Demner-Fushman, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William R. Hersh, Oregon Health and Science University",
      "description": "In handling challenging cases, clinicians often seek out information to make better decisions in patient care. Typically, these information sources combine clinical experience with scientific medical research in a process known as evidence-based medicine (EBM). Information relevant to a physician can be related to a variety of clinical tasks, such as (i) determining a patient's most likely diagnosis given a list of symptoms, (ii) determining if a particular test is indicated for a given situation, and (iii) deciding on the most effective treatment plan for a patient having a known condition. Finding the most relevant and recent research, however, can be quite challenging due to the volume of scientific literature and the pace at which new research is published. As such, the time-consuming nature of information seeking means that most clinician questions go unanswered. In order to better enable access to the scientific literature in the clinical setting, research is necessary to evaluate information retrieval methods that connect clinical notes with the published literature. The TREC Clinical Decision Support (CDS) track simulates the information retrieval requirements of such systems to encourage the creation of tools and resources necessary for their implementation. In 2014 and 2015, the CDS tracks used simulated patient cases presented as if they were typical case reports used in medical education. However, in an actual electronic health record (EHR), patient notes are written in a much different manner, notably with terse language and heavy use of abbreviations and clinical jargon. To address the challenge specific to EHR notes, the 2016 CDS track used de-identified notes for actual patients. This enabled participants to experiment with a realistic topic/query and develop methods to handle the challenging nature of clinical text. For a given EHR note, participants were challenged with retrieving full-text biomedical articles relevant for answering questions related to one of three generic clinical information needs: Diagnosis (i.e., “What is this patient's diagnosis?”), Test (“What diagnostic test is appropriate for this patient?”), and Treatment (“What treatment is appropriate for this patient?”). Retrieved articles were judged relevant if they provided information of the specified type useful for the given case. The assessment was performed by physicians with training in biomedical informatics using a 3-point scale: relevant, partially relevant, not relevant.",
      "year": 2016
    },
    "qa": {
      "fullname": "LiveQA",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20160220151945/https://sites.google.com/site/trecliveqa2016/",
      "coordinators": "Eugene Agichtein, Emory University:David Carmel, Yahoo Research:Dan Pelleg, Yahoo Research:Yuval Pinter, Yahoo Research:Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The LiveQA track, now in its second year, is focused on real-time question answering for real-user questions. During the test period, real user questions are drawn from those newly submitted on a popular community question answering site, Yahoo Answers (YA), that have not yet been answered. These questions are sent to the participating systems, who provide an answer in real time. Returned answers are judged by the NIST assessors on a 4-level Likert scale. The most challenging aspects of this task are that the questions can be on any one of many popular topics, are informally stated, and are often complex and at least partly subjective. Furthermore, the participant systems must return an answer in under 60 seconds, which places additional, and realistic, constraints on the kind of processing that a system can do. In addition to the main real-time question answering task, this year we introduced a pilot task aimed at identifying the question intent. As human questions submitted on forums and CQA sites are verbose in nature and contain many redundant or unnecessary terms, participants were challenged to identify the significant parts of the question. The main theme of the question is marked by the systems by specifying a list of spans that capture its main intent. This automatic “summary” of the question was evaluated by measuring its ROUGEand METEOR-based similarity to a succinct rephrase of the question, manually provided by NIST assessors.",
      "year": 2016
    },
    "context": {
      "fullname": "Contextual Suggestion",
      "tasks": {
        "phase1": "Phase 1 Experiments",
        "phase2": "Phase 2 Experiments"
      },
      "webpage": "https://sites.google.com/site/treccontext/",
      "coordinators": "Seyyed Hadi Hashemi,University of Amsterdam:Jaap KampsUniversity of Amsterdam:Julia Kiseleva, University of Amsterdam:Charles L.A. Clarke, University of Waterloo:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The TREC Contextual Suggestion Track offers a personalized point of interest (POI) recommendation task, in which participants develop systems to give a ranked list of suggestions related to a profile and a context pair available in the tasks' requests provided by the track organizers. Previously, reusability of the contextual suggestion track suffered from using dynamic collections and a shallow pool depth. The main innovations at TREC 2016 are the following. First, the TREC CS web corpus, consisting of a web crawl of the TREC contextual suggestion collection, was made available. The rich textual descriptions of the web pages makes far more information available for each candidate POI in the collection. Second, we released endorsements (end user tags) of the attractions as given by NIST assessors, potentially matching the endorsements of POIs in another city as given by the person issuing the request as part of her profile. Third, a multi-depth pooling approach extending beyond the shallow top 5 pool was used. The multi-depth pooling approach has created a test collection that provides more reliable evaluation results in ranks deeper than the traditional pool cut-off.",
      "year": 2016
    },
    "realtime": {
      "fullname": "Real-time Summarization",
      "tasks": {
        "a": "Push notifications",
        "b": "Email digests"
      },
      "webpage": "https://trecrts.github.io/",
      "coordinators": "Jimmy Lin, University of Waterloo:Adam Roegiest, University of Waterloo:Luchen Tan, University of Waterloo:Richard McCreadie, University of Glasgow:Ellen Voorhees, National Institute of Standards and Technology (NIST):Fernando Diaz, Microsoft Research",
      "description": "The TREC 2016 Real-Time Summarization (RTS) Track aims to explore techniques and systems that automatically monitor streams of social media posts such as Twitter to keep users up to date on topics of interest. We might think of these topics as “interest profiles”, specifying the user's prospective information needs. In real-time summarization, the goal is for a system to “push” (i.e., recommend or suggest) interesting and novel content to users in a timely fashion. For example, the user might be interested in poll results for the 2016 U.S. presidential elections and wishes to be notified whenever new results are published.",
      "year": 2016
    },
    "recall": {
      "fullname": "Total Recall",
      "tasks": {
        "full": "At-Home Task",
        "sandbox": "Sandbox Task"
      },
      "webpage": "https://plg.uwaterloo.ca/~gvcormac/total-recall/",
      "coordinators": "Maura R. Grossman, University of Waterloo:Gordon V. Cormack, University of Waterloo:Adam Roegiest, University of Waterloo",
      "description": "The primary purpose of the Total Recall Track is to evaluate, through controlled simulation, methods designed to achieve very high recall - as close as practicable to 100% - with a human assessor in the loop. Motivating applications include, among others, electronic discovery in legal proceedings, systematic review in evidencebased medicine, and the creation of fully labeled test collections for information retrieval (“IR”) evaluation. A secondary, but no less important, purpose is to develop a sandboxed virtual test environment within which IR systems may be tested, while preventing the disclosure of sensitive test data to participants. At the same time, the test environment also operates as a “black box,” affording participants confidence that their proprietary systems cannot easily be reverse engineered.",
      "year": 2016
    },
    "task": {
      "fullname": "Tasks",
      "tasks": {
        "understanding": "Task Understanding",
        "completion": "Task Completion",
        "adhoc": "Adhoc Retrieval"
      },
      "webpage": "http://www.cs.ucl.ac.uk/tasks-track-2016/",
      "coordinators": "Manisha Verma, University College London:Emine Yilmaz, University College London:Rishabh Mehrotra, University College London:Evangelos Kanoulas, University of Amsterdam:Ben Carterette, University of Delaware:Nick Craswell, Microsoft:Peter Bailey, Microsoft",
      "description": "Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefulness of the system in helping the user complete the actual task that led the user issue the query. Similar to Tasks Track 2015, Tasks Track 2016 is an attempt investigate quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks.",
      "year": 2016
    },
    "domain": {
      "fullname": "Dynamic Domain",
      "tasks": "",
      "webpage": "https://infosense.cs.georgetown.edu/trec_dd/index.html",
      "coordinators": "Grace Hui Yang, Georgetown University:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The main goal of TREC dynamic domain track is to explore and evaluate systems for interactive information retrieval, which reflects real-world search scenarios. Due to the importance of learning from user interactions, this track has been held for the second year. The name of the track contains two parts. “Dynamic” means that the search system dynamically adapts the provided ranking to the user through interactions. “Domain” stems from the fact that the search task in the track is on the domains of special interests, which tend to bring information needs that would not be met within a single interaction. The task is inspired by interested groups in government, including the DARPA Memex program. Each search task in the DD track involves some interactions between a user and a search system. In the first iteration, the user submits a query and the target domain of interest to the search system. The system provides the user with an initial ranking of documents, and receives feedback from the user on the provided ranking. This interaction, providing the user with a ranking and receiving the user’s feedback, continues until the search system stops the search task. The DD track introduces a new challenging search problem with the following important assumptions.",
      "year": 2016
    },
    "open": {
      "fullname": "OpenSearch",
      "tasks": {
        "round1": "Round 1",
        "round2": "Round 2"
      },
      "webpage": "https://web.archive.org/web/20170617095056/http://trec-open-search.org/",
      "coordinators": "Krisztian Balog, University of Stavanger:Anne Schuth, Blendle:Peter Dekker, University of Amsterdam:Narges Tavakolpoursaleh, GESIS – Leibniz Institute for the Social Sciences:Philipp Schaer, TH Köln (University of Applied Sciences):Po-Yu Chuang, Pennsylvania State University",
      "description": "We present the TREC Open Search track, which represents a new evaluation paradigm for information retrieval. It offers the possibility for researchers to evaluate their approaches in a live setting, with real, unsuspecting users of an existing search engine. The first edition of the track focuses on the academic search domain and features the ad-hoc scientific literature search task. We report on experiments with three different academic search engines: CiteSeerX, SSOAR, and Microsoft Academic Search.",
      "year": 2016
    }
  },
  "trec26": {
    "core": {
      "fullname": "Common Core",
      "tasks": "",
      "webpage": "https://trec-core.github.io/2017/",
      "coordinators": "Evangelos Kanoulas, University of Amsterdam:James Allan, University of Massachusetts:Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The primary goal of the TREC Common Core track is three-fold: (a) bring the information retrieval community back into a traditional ad-hoc search task; (b) attract a diverse set of participating runs and build a new test collections using more recently created documents; (c) establish a (new) test collection construction methodology that avoids the pitfalls of depth-k pooling. A number of side-goals are also set, including studying the shortcomings of test collections constructed in the past; experimenting with new ideas for constructing test collections; expand test collections by new participant tasks (ad-hoc/interactive), new relevance judgments (binary/multilevel), new pooling methods, new assessment resources (NIST / crowd-sourcing) and new retrieval systems contributing documents (manual/neural/strong baselines).",
      "year": 2017
    },
    "pm": {
      "fullname": "Precision Medicine",
      "tasks": {
        "trials": "Clinical Trials",
        "abstracts": "Scientific Abstracts"
      },
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, The University of Texas Health Science Center:Dina Demner-Fushman, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William R. Hersh, Oregon Health & Science University:Steven Bedrick, Oregon Health & Science University:Alexander J. Lazar, The University of Texas MD Anderson Cancer Center:Shubham Pant, The University of Texas MD Anderson Cancer Center",
      "description": "For many complex diseases, there is no “one size fits all” solutions for patients with a particular diagnosis. The proper treatment for a patient depends upon genetic, environmental, and lifestyle choices. The ability to personalize treatment in a scientifically rigorous manner based on these factors is the hallmark of the emerging “precision medicine” paradigm. Nowhere is the potential impact of precision medicine more closely felt than in cancer, where lifesaving treatments for particular patients could prove ineffective or even deadly for other patients based entirely upon the particular genetic mutations in the patient’s tumor(s). Significant effort, therefore, has been devoted to deepening the scientific research surrounding precision medicine. This includes a Precision Medicine Initiative launched by former President Barack Obama in 2015, now known as the All of Us Research Program.",
      "year": 2017
    },
    "qa": {
      "fullname": "LiveQA",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20170729204820/https://sites.google.com/site/trecliveqa2017/",
      "coordinators": "Asma Ben Abacha, U.S. National Library of Medicine:Eugene Agichtein, Emory University:Yuval Pinter, Georgia Institute of Technology:Dina Demner-Fushman, U.S. National Library of Medicine",
      "description": "The task addresses the automatic answering of consumer health questions received by the U.S. National Library of Medicine. We provided both training question-answer pairs, and test questions with reference answers. All questions were manually annotated with the main entities (foci) and question types. The medical task received eight runs from five participating teams. Different approaches have been applied, including classical answer retrieval based on question analysis and similar question retrieval. In particular, several deep learning approaches were tested, including attentional encoder-decoder networks, long short-term memory networks and convolutional neural networks. The training datasets were both from the open domain and the medical domain.",
      "year": 2017
    },
    "rts": {
      "fullname": "Real-time Summarization",
      "tasks": {
        "a": "Push notifications",
        "b": "Email digests"
      },
      "webpage": "https://trecrts.github.io/",
      "coordinators": "Jimmy Lin, University of Waterloo:Salman Mohammed, University of Waterloo:Royal Sequiera, University of Waterloo:Luchen Tan, University of Waterloo:Nimesh Ghelani, University of Waterloo:Mustafa Abualsaud, University of Waterloo:Richard McCreadie, University of Glasgow:Dmitrijs Milajevs, National Institute for Standards and Technology (NIST):Ellen Voorhees, National Institute for Standards and Technology (NIST)",
      "description": "The TREC 2017 Real-Time Summarization (RTS) Track is the second iteration of a community effort to explore techniques, algorithms, and systems that automatically monitor streams of social media posts such as tweets on Twier to address users’ prospective information needs. These needs are articulated as “interest profiles”, akin to topics in ad hoc retrieval. In real-time summarization, the goal is for a system to deliver interesting and novel content to users in a timely fashion. We refer to these messages generically as “updates”.",
      "year": 2017
    },
    "car": {
      "fullname": "Complex Answer Retrieval",
      "tasks": {
        "passages": "Passage Task",
        "entities": "Entity Task"
      },
      "webpage": "https://trec-car.cs.unh.edu/",
      "coordinators": "Laura Dietz, University of New Hampshire:Manisha Verma, University College London:Filip Radlinski, Google:Nick Craswell, Microsoft",
      "description": "The SWIRL 2012 workshop on frontiers, challenges, and opportunities for information retrieval report [1] noted many important challenges. Among them, challenges such as conversational answer retrieval, subdocument retrieval, and answer aggregation share commonalities: We desire answers to complex needs, and wish to find them in a single and well-presented source. Advancing the state of the art in this area is the goal of this TREC track. Consider a user investigating a new and unfamiliar topic. This user would often be best served with a single summary, rather than being required to synthesize his or her own summary from multiple sources. This is especially the case in mobile environments with restricted interaction capabilities. While these have led to extensive work on finding the best short answer, the target in this track is the retrieval of comprehensive answers that are composed of multiple text fragments from multiple sources. Retrieving high-quality longer answers is challenging as it is not sufficient to choose a lower rank-cutoff with the same techniques as for short answers. Instead, we need new approaches for finding relevant information in a complex answer space. Many examples of manually created complex answers exist on the Web. Famous examples are articles from how-stuff-works.com, travel guides, or fanzines. These are collections of articles, that each constitutes a long answer to an information need represented by the title of the article. The fundamental task of collecting references, facts, and opinions into a single coherent summary has traditionally been a manual process. We envision that automated information retrieval systems can relieve users from a large amount of manual work through sub-document retrieval, consolidation and organization. Ultimately, the goal is to retrieve synthesized information rather than documents.",
      "year": 2017
    },
    "task": {
      "fullname": "Tasks",
      "tasks": {
        "understanding": "Task Understanding",
        "completion": "Task Completion",
        "adhoc": "Adhoc Retrieval"
      },
      "webpage": "http://www.cs.ucl.ac.uk/tasks-track-2017/",
      "coordinators": "Evangelos Kanoulas, University of Amsterdam:Emine Yilmaz, University College London:Rishabh Mehrotra, University College London:Ben Carterette, University of Delaware:Nick Craswell and Peter Bailey, Microsoft",
      "description": "Research in Information Retrieval has traditionally focused on serving the best results for a single query, ignoring the reasons (or the task) that might have motivated the user to submit that query. Often times search engines are used to complete complex tasks; achieving these tasks with current search engines requires users to issue multiple queries. For example, booking travel to a location such as London could require the user to submit various queries such as flights to London, hotels in London, points of interest around London, etc. Standard evaluation mechanisms focus on evaluating the quality of a retrieval system in terms of the topical relevance of the results retrieved, completely ignoring the fact that user satisfaction mainly depends on the usefulness of the system in helping the user complete the actual task that led the user issue the query. The TREC Tasks Track is an attempt in devising mechanisms for evaluating quality of retrieval systems in terms of (1) how well they can understand the underlying task that led the user submit a query, and (2) how useful they are for helping users complete their tasks.",
      "year": 2017
    },
    "domain": {
      "fullname": "Dynamic Domain",
      "tasks": "",
      "webpage": "https://infosense.cs.georgetown.edu/trec_dd/index.html",
      "coordinators": "Grace Hui Yang, Georgetown University:Zhiwen Tang, Georgetown University:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The goal of dynamic domain track is promoting the research of dynamic, exploratory search within complex information domains, where the search process is usually interactive and user’s information need is also complex. Dynamic Domain (DD) track has been held in the past three years. This track's name includes two parts. “Dynamic” means the search process may contain multiple runs of iteration, and the participating system is expected to adapt its search algorithm based on the relevance feedback. “Domain” means the search task focuses on special domains, where user’s information need consists of multiple aspects, and the participating system is expected to help the user explore the domain through rich interaction. This task has received great attention and this track is inspired by interested groups in government, including DARPA MEMEX program.",
      "year": 2017
    },
    "open": {
      "fullname": "OpenSearch",
      "tasks": "",
      "webpage": "https://web.archive.org/web/20170617095056/http://trec-open-search.org/",
      "coordinators": "Rolf Jagerman, University of Amsterdam:Martin de Rijke, University of Amsterdam:Krisztian Balog, University of Stavanger:Phillip Schaer, TH Köln:Johann Schaible, GESIS - Leibniz Institute for the Social Sciences:Narges Tavakolpoursaleh, GESIS - Leibniz Institute for the Social Sciences",
      "description": "The OpenSearch track provides researchers the opportunity to have their retrieval approaches evaluated in a live setting with real users. We focus on the academic search domain with the Social Science Open Access Repository (SSOAR) search engine and report our results.",
      "year": 2017
    }
  },
  "trec27": {
    "pm": {
      "fullname": "Precision Medicine",
      "tasks": {
        "trials": "Clinical Trials",
        "abstracts": "Scientific Abstracts"
      },
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, The University of Texas Health Science Center:Dina Demner-Fushman, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William R. Hersh, Oregon Health & Science University:Steven Bedrick, Oregon Health & Science University:Alexander J. Lazar, The University of Texas MD Anderson Cancer Center",
      "description": "The fundamental philosophy behind precision medicine is that for many complex diseases, there is no “one size fits all” solutions for patients with a particular diagnosis. The proper treatment for a patient depends upon genetic, environmental, and lifestyle choices. The ability to personalize treatment in a scientifically rigorous manner based on these factors is thus the hallmark of the emerging precision medicine paradigm. Nowhere is the potential impact of precision medicine more closely focused at the moment than in cancer, where lifesaving treatments for particular patients could prove ineffective or even deadly for other patients based entirely upon the particular genetic mutations in the patient’s tumor(s). Significant effort, therefore, has been devoted to deepening the scientific research surrounding precision medicine. This includes the Precision Medicine Initiative launched by former President Barack Obama in 2015, now known as the All of Us Research Program.",
      "year": 2018
    },
    "core": {
      "fullname": "Common Core",
      "tasks": "",
      "webpage": "https://trec-core.github.io/2018/",
      "coordinators": "Evangelos Kanoulas, University of Amsterdam:James Allan, University of Massachusetts:Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The primary goals of the proposed core track are three-fold: (a) to bring together the community in a common track that could lead to a diverse set of participating runs, (b) to build one or more new test collections using more recently created documents, and (c) to establish a (new) test collection construction methodology that avoids the pitfalls of depth-k pooling.",
      "year": 2018
    },
    "rts": {
      "fullname": "Real-time Summarization",
      "tasks": {
        "a": "Real-time updates",
        "b": "Email digests"
      },
      "webpage": "https://trecrts.github.io/",
      "coordinators": "Royal Sequiera, University of Waterloo:Luchen Tan, University of Waterloo:Jimmy Lin, University of Waterloo",
      "description": "The TREC 2018 Real-Time Summarization (RTS) Track is the third iteration of a community eort to explore techniques, algorithms, and systems that automatically monitor streams of social media posts such as tweets on Twier to address users’ prospective information needs. ese needs are articulated as “interest proles”, akin to topics in ad hoc retrieval. In our formulation of real-time summarization, the goal is for a system to deliver relevant and novel content to users in a timely fashion. We refer to these messages generically as “updates”.",
      "year": 2018
    },
    "car": {
      "fullname": "Complex Answer Retrieval",
      "tasks": {
        "passages": "Passage Task",
        "entities": "Entity Task"
      },
      "webpage": "https://trec-car.cs.unh.edu/",
      "coordinators": "Laura Dietz:Ben Gamari:Jeff Dalton:Nick Craswell",
      "description": "Current retrieval systems provide good solutions towards phrase-level retrieval for simple fact and entity-centric needs. This track encourages research for answering more complex information needs with longer answers. Much like Wikipedia pages synthesize knowledge that is globally distributed, we envision systems that collect relevant information from an entire corpus, creating synthetically structured documents by collating retrieved results.",
      "year": 2018
    },
    "news": {
      "fullname": "News",
      "tasks": {
        "background": "Background Linking",
        "entity": "Entity Ranking"
      },
      "webpage": "http://trec-news.org/",
      "coordinators": "Ian Soboroff, National Institute of Standards and Technology (NIST):Shudong Huang, National Institute of Standards and Technology (NIST):Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The News track is a new track for TREC 2019, focused on information retrieval in the service of helping people read the news. In cooperation with the Washington Post, we released a new collection of 600,000 news articles, and crafted two tasks related to how news is presented on the web.",
      "year": 2018
    },
    "incident": {
      "fullname": "Incident Streams",
      "tasks": "",
      "webpage": "http://trecis.org/",
      "coordinators": "Richard McCreadie, University of Glasgow:Cody Buntain, New York University:Ian Soborof, National Institute of Standards and Technology (NIST)",
      "description": "The Text Retrieval Conference (TREC) Incident Streams track is a new initiative that aims to mature social media-based emergency response technology. This initiative advances the state of the art in this area through an evaluation challenge, which attracts researchers and developers from across the globe. The 2018 edition of the track provides a standardized evaluation methodology, an ontology of emergency-relevant social media information types, proposes a scale for information criticality, and releases a dataset containing fifteen test events and approximately 20,000 labeled tweets. Analysis of this dataset reveals a significant amount of actionable information on social media during emergencies (> 10%). While this data is valuable for emergency response efforts, analysis of the 39 state-of-the-art systems demonstrate a performance gap in identifying this data. We therefore find the current state-of-the-art is insufficient for emergency responders’ requirements, particularly for rare actionable information for which there is little prior training data available.",
      "year": 2018
    },
    "centre": {
      "fullname": "CENTRE",
      "tasks": "",
      "webpage": "https://www.centre-eval.org/trec2018/index.html",
      "coordinators": "Ian Soboroff, National Institute of Standards and Technology (NIST):Nicola Ferro, University of Padua:Maria Maistro, University of Padua:Tetsuya Sakai, Waseda University",
      "description": "The CLEF-NTCIR-TREC Reproducibility track (CENTRE) is a research replication and reproduction effort spanning three major information retrieval evaluation venues. In the TREC edition, CENTRE participants were asked to reproduce runs from either the TREC 2016 clinical decision support track, the 2013 web track, or the 2014 web track. Only one group participated in the track, and unfortunately the track will not continue in 2019.",
      "year": 2018
    }
  },
  "trec28": {
    "car": {
      "fullname": "Complex Answer Retrieval",
      "tasks": "",
      "webpage": "https://trec-car.cs.unh.edu/",
      "coordinators": "Laura Dietz, University of New Hampshire:John Foley, Smith College",
      "description": "The vision of TREC Complex Answer Retrieval is to create complex long-form answers in response to a wide-variety information needs. In general, we aspire to create answers that are reminiscent to Wikipedia articles or school text books (e.g. TQA). However, while the vast majority of Wikipedia articles are about people, in TREC CAR we aim at information needs that are off the beaten path, covering topics in popular science, technology, and illnesses.",
      "year": 2019
    },
    "deep": {
      "fullname": "Deep Learning",
      "tasks": {
        "docs": "Document Ranking",
        "passages": "Passage Ranking"
      },
      "webpage": "https://microsoft.github.io/msmarco/TREC-Deep-Learning",
      "coordinators": "Nick Craswell,  Microsoft AI and Research:Bhaskar Mitra, Microsoft AI and Research:Daniel Campos,  Microsoft AI and Research:Emine Yilmaz, University College London:Ellen M. Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the first track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs significantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",
      "year": 2019
    },
    "pm": {
      "fullname": "Precision Medicine",
      "tasks": {
        "trials": "Clinical Trials",
        "abstracts": "Scientific Abstracts"
      },
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, The University of Texas Health Science Center:Dina Demner-Fushman, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):William R. Hersh, Oregon Health & Science University:Steven Bedrick, Oregon Health & Science University:Alexander J. Lazar, The University of Texas MD Anderson Cancer Center:Shubham Pant, The University of Texas MD Anderson Cancer Center:Funda Meric-Bernstam, The University of Texas MD Anderson Cancer Center",
      "description": "Precision medicine is a medical paradigm in which treatments are customized entirely to the individual patient. The underlying issue that drives precision medicine is that for many complex diseases, there are no “one size fits all” solutions for patients with a particular diagnosis. The proper treatment for a patient depends upon genetic, environmental, and lifestyle choices. The ability to personalize treatment in a scientifically rigorous manner based on these factors is thus the hallmark of the emerging precision medicine paradigm. Nowhere is the potential impact of precision medicine more closely focused at the moment than in cancer, where lifesaving treatments for particular patients could prove ineffective or even deadly for other patients based entirely upon the particular genetic mutations in the patient’s tumor(s). Significant effort, therefore, has been devoted to deepening the scientific research surrounding precision medicine. This includes the Precision Medicine Initiative launched by President Barack Obama in 2015, now known as the All of Us Research Program.",
      "year": 2019
    },
    "cast": {
      "fullname": "Conversational Assistance",
      "tasks": "",
      "webpage": "https://www.treccast.ai/",
      "coordinators": "Jeffrey Dalton, University of Glasgow:Chenyan Xiong, Microsoft Research:Jamie Callan, Carnegie Mellon University",
      "description": "The importance of conversation and conversational models for complex information seeking tasks is well-established within information retrieval, initially to understand user behavior during interactive search and later to improve search accuracy during search sessions. The rapid adoption of a new generation of conversational assistants such as Alexa, Siri, Cortana, Bixby, and Google Assistant increase the scope and importance of conversational approaches to information seeking and also introduce a broad range of new research problems. The TREC Conversational Assistance Track (CAsT) is a new initiative to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. We define it as a task in which effective response selection requires understanding a question’s context (the dialogue history). It focuses attention on user modeling, analysis of prior retrieval results, transformation of questions into effective queries, and other topics that have been difficult to study with previous datasets. To make this tractable and reusable for the first year of CAsT, we begin with pre-determined conversation trajectories and passage responses. Our target conversations include several rounds of utterances that are coherent in topic and explore relevant information. The primary initial focus is on system understanding of information needs in a conversational format and finding relevant passages leveraging conversational context. The long-term vision of CAsT is to allow natural conversions with mixed-initiative, where the system performs a variety of information actions, e.g., providing information (INFORM), asking clarifying questions (CLARIFY), leading conversations with more interactions (SUGGEST), and others. For the first year we focus on context understanding and use simple INFORM actions, where systems return text passages to the user. In the future, we plan to explore richer sets of information actions, richer response formats, and more interactions between users and conversational agents.",
      "year": 2019
    },
    "news": {
      "fullname": "News",
      "tasks": {
        "background": "Background Linking",
        "entity": "Entity Ranking"
      },
      "webpage": "http://trec-news.org/",
      "coordinators": "Ian Soboroff, National Institute of Standards and Technology (NIST):Shudong Huang, National Institute of Standards and Technology (NIST):Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The News track focuses on information retrieval in the service of helping people read the news. In 2018, in cooperation with the Washington Post, we released a new collection of nearly 600,000 news articles, and crafted two tasks related to how news is presented on the web: background linking and entity ranking. This second iteration of the track continues these two tasks with minor updates.",
      "year": 2019
    },
    "decisions": {
      "fullname": "Decision",
      "tasks": "",
      "webpage": "https://trec-health-misinfo.github.io/2019.html",
      "coordinators": "Mustafa Abualsaud, University of Waterloo:Mark D. Smucker, University of Waterloo:Christina Lioma, University of Copenhagen:Maria Maistro, University of Copenhagen:Guido Zuccon, University of Queensland ",
      "description": "Search engine results underpin many consequential decision making tasks. Examples include people using search technologies to seek health advice online, or time-pressured clinicians relying on search results to decide upon the best treatment/diagnosis/test for a patient. A key problem when using search engines in order to complete such decision making tasks, is whether users are able to discern authoritative from unreliable information and correct from incorrect information. This problem is further exacerbated when the search occurs within uncontrolled data collections, such as the web, where information can be unreliable, generally misleading, too technical, and can lead to unfounded escalations. Information from search engine results can significantly influence decisions, and research shows that increasing the amount of incorrect information about a topic presented in a Search Engine Result Page (SERP) can impel users to take incorrect decisions. As noted in the SWIRL III report, decision making with search engines is poorly understood, and likewise, evaluation measures for these search tasks need to be developed and improved. In this context, the TREC 2019 Decision track aims to (1) foster research on retrieval methods that promote better decision making with search engines, and (2) develop new online and offline evaluation methods to predict the decision making quality induced by search results.",
      "year": 2019
    },
    "fair": {
      "fullname": "Fair Ranking",
      "tasks": "",
      "webpage": "https://fair-trec.github.io/",
      "coordinators": "Asia J. Biega, Microsoft Research Montréal:Fernando Diaz, Microsoft Research Montréal:Michael D. Ekstrand, Boise State University:Sebastian Kohlmeier, Allen Institute for Artificial Intelligence",
      "description": "The goal of the TREC Fair Ranking track was to develop a benchmark for evaluating retrieval systems in terms of fairness to different content providers in addition to classic notions of relevance. As part of the benchmark, we defined standardized fairness metrics with evaluation protocols and released a dataset for the fair ranking problem. The 2019 task focused on reranking academic paper abstracts given a query. The objective was to fairly represent relevant authors from several groups that were unknown at the system submission time. Thus, the track emphasized the development of systems which have robust performance across a variety of group definitions. Participants were provided with querylog data (queries, documents, and relevance) from Semantic Scholar.",
      "year": 2019
    },
    "incident": {
      "fullname": "Incident Streams",
      "tasks": "",
      "webpage": "https://www.dcs.gla.ac.uk/~richardm/TREC_IS/",
      "coordinators": "Richard McCreadie, University of Glasgow:Cody Buntain, InfEco Lab, New Jersey Institute of Technology (NJIT):Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The ubiquity of mobile internet-enabled devices combined with wide-spread social media use during emergencies is posing new challenges for response personnel. In particular, service operators are now expected to monitor these online channels to extract actionable insights and answer questions from the public. A lack of adequate tools makes this monitoring impractical at the scale of many emergencies. The TREC Incident Streams (TREC-IS) track drives research into solving this technology gap by bringing together academia and industry to develop techniques for extracting actionable insights from social media streams during emergencies.",
      "year": 2019
    }
  },
  "trec29": {
    "news": {
      "fullname": "News",
      "tasks": {
        "background": "Background Linking",
        "wikification": "Wikification"
      },
      "webpage": "http://trec-news.org/",
      "coordinators": "Ian Soboroff, National Institute of Standards and Technology (NIST):Shudong Huang, National Institute of Standards and Technology (NIST):Donna Harman, National Institute of Standards and Technology (NIST)",
      "description": "The News track focuses on information retrieval in the service of helping people read the news. In 2018, in cooperation with the Washington Post, we released a new collection of nearly 600,000 news articles, and crafted two tasks related to how news is presented on the web: background linking and entity ranking. For 2020, we added more documents to the collection and retired the entity ranking task in favor of a new wikification task.",
      "year": 2020
    },
    "deep": {
      "fullname": "Deep Learning",
      "tasks": {
        "docs": "Document Ranking",
        "passages": "Passage Ranking"
      },
      "webpage": "https://microsoft.github.io/msmarco/TREC-Deep-Learning",
      "coordinators": "Nick Craswell, Microsoft AI & Research:Bhaskar Mitra, Microsoft AI & Research:Bhaskar Mitra, University College London:Emine Yilmaz, University College London: Daniel Campos, University of Illinois Urbana-Champaign",
      "description": "This is the second year of the TREC Deep Learning Track, with the goal of studying ad hoc ranking in the large training data regime. We again have a document retrieval task and a passage retrieval task, each with hundreds of thousands of human-labeled training queries. We evaluate using singleshot TREC-style evaluation, to give us a picture of which ranking methods work best when large data is available, with much more comprehensive relevance labeling on the small number of test queries. This year we have further evidence that rankers with BERT-style pretraining outperform other rankers in the large data regime.",
      "year": 2020
    },
    "incident": {
      "fullname": "Incident Streams",
      "tasks": {
        "2020A-task1": "All High-Level Information Type Classification",
        "2020A-task2": "Selected High-Level Information Type Classification",
        "2020A-task3": "COVID-19-Specific Information-Type Classification",
        "2020b-task1": "All High-Level Information Type Classification",
        "2020b-task2": "Selected High-Level Information Type Classification",
        "2020b-task3": "COVID-19-Specific Information-Type Classification"
      },
      "webpage": "https://www.dcs.gla.ac.uk/~richardm/TREC_IS/",
      "coordinators": "Cody Buntain, New Jersey Institute of Technology:Richard McCreadie, University of Glasgow: Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "Between 2018 and 2019, the Incident Streams track (TREC-IS) has developed standard approaches for classifying the types and criticality of information shared in online social spaces during crises, but the introduction of SARS-CoV-2 has shifted the landscape of online crises substantially. While prior editions of TREC-IS have lacked data on large-scale public-health emergencies as these events are exceedingly rare, COVID-19 has introduced an over-abundance of potential data, and significant open questions remain about how existing approaches to crisis informatics and datasets built on other emergencies adapt to this new context.",
      "year": 2020
    },
    "misinfo": {
      "fullname": "Health Misinformation",
      "tasks": {
        "adhoc": "Adhoc Retrieval",
        "recall": "Total Recall"
      },
      "webpage": "https://trec-health-misinfo.github.io/",
      "coordinators": "Charles L. A. Clarke, University of Waterloo:Saira Rizvi, University of Waterloo:Mark D. Smucker, University of Waterloo:Maria Maistro, University of Copenhagen:Guido Zuccon, University of Queensland",
      "description": "TREC 2020 was the second year for the Health Misinformation track, which was named the Decision Track in 2019. Information retrieval using document collections that contain misinformation are problematic. When a search engine returns documents that contain misinformation, users may have difficulty discerning correct from incorrect information and the incorrect information can lead to incorrect decisions. Decisions regarding health-related topics can be consequential, and as such we want search engines that enable users to make correct decisions. The track is designed to address the problem of health misinformation in three areas: 1) adhoc retrieval, 2) the total recall of misinformation in the collection, and 3) the evaluation of retrieval in the presence of misinformation. The 2020 Health Misinformation track had both a recall task and an adhoc task for participants.",
      "year": 2020
    },
    "cast": {
      "fullname": "Conversational Assistance",
      "tasks": "",
      "webpage": "https://www.treccast.ai/",
      "coordinators": "Jeffrey Dalton, University of Glasgow:Chenyan Xiong, Microsoft Research:Jamie Callan, Carnegie Mellon University",
      "description": "CAsT 2020 is the second year of the Conversational Assistance Track and builds on the lessons from the first year. Teams tried a wide range of techniques to address conversational search challenges. Some methods used proven techniques such as query difficulty prediction and query expansion. Given the text understanding challenges in the task, teams also used traditional NLP models that incorporate coreference resolution. One important development was the application of generative query models and ranking models using pre-trained neural language models. The results showed that both traditional and neural techniques provided complementary effectiveness.",
      "year": 2020
    },
    "pm": {
      "fullname": "Precision Medicine",
      "tasks": "",
      "webpage": "https://www.trec-cds.org/",
      "coordinators": "Kirk Roberts, The University of Texas Health Science Center:Dina Demner-Fushman, U.S. National Library of Medicine:Ellen M. Voorhees, National Institute of Standards and Technology (NIST):Steven Bedrick and William R. Hersh, Oregon Health & Science University",
      "description": "The precision medicine paradigm focuses on identifying treatments that are best suited to an individual patient’s unique attributes. The reasoning behind this paradigm is that diseases do not uniformly manifest in people and thus “one size fits all” treatments are often not appropriate. For many diseases, such as cancer, proper selection of a treatment strategy can drastically improve results compared to the standard, frontline treatment. Generally speaking, the issues that are taken into consideration for precision medicine are the genomic, environmental, and lifestyle contexts of the patient.",
      "year": 2020
    },
    "podcast": {
      "fullname": "Podcast",
      "tasks": {
        "summarization": "Summarization",
        "retrieval": "Retrieval"
      },
      "webpage": "https://trecpodcasts.github.io/",
      "coordinators": "Rosie Jones, Spotify Research:Ben Carterette, Spotify Research:Ann Clifton, Spotify Research:Jussi Karlgren, Spotify Research:Aasish Pappu, Spotify Research: Sravana Reddy, Spotify Research:Yongze Yu, Spotify Research:Maria Eskevich, CLARIN ERIC:Gareth J. F. Jones, Dublin City University",
      "description": "The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The podcast track was designed to encourage research into podcasts in the information retrieval and NLP research communities. The track consisted of two shared tasks: segment retrieval and summarization, both based on a dataset of over 100,000 podcast episodes (metadata, audio, and automatic transcripts) which was released concurrently with the track. The track generated considerable interest, aracted hundreds of new registrations to TREC and fifteen teams, mostly disjoint between search and summarization, made final submissions for assessment. Deep learning was the dominant experimental approach for both search experiments and summarization.",
      "year": 2020
    },
    "fair": {
      "fullname": "Fair Ranking",
      "tasks": {
        "rerank": "Rerank",
        "retrieval": "Retrieval"
      },
      "webpage": "https://fair-trec.github.io/",
      "coordinators": "Asia J. Biega, Microsoft Research Montreal:Fernando Diaz, Montreal Institute for Learning Algorithms:Michael D. Ekstrand, Boise State University:Sergey Feldman, Allen Institute for Artificial Intelligence:Sebastian Kohlmeier, Allen Institute for Artificial Intelligence",
      "description": "For 2020, we again adopted an academic search task, where we have a corpus of academic article abstracts and queries submitted to a production academic search engine. The central goal of the Fair Ranking track is to provide fair exposure to different groups of authors (a group fairness framing). We recognize that there may be multiple group definitions (e.g. based on demographics, stature, topic) and hoped for the systems to be robust to these. We expected participants to develop systems that optimize for fairness and relevance for arbitrary group definitions, and did not reveal the exact group definitions until after the evaluation runs were submitted. The track contains two tasks, reranking and retrieval, with a shared evaluation.",
      "year": 2020
    }
  },
  "trec30": {
    "incident": {
      "fullname": "Incident Streams",
      "tasks": "",
      "webpage": "https://www.dcs.gla.ac.uk/~richardm/TREC_IS/",
      "coordinators": "Cody Buntain, New Jersey Institute of Technology:Richard McCreadie, University of Glasgow:Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The Incident Streams track is designed to bring together academia and industry to research technologies to automatically process social media streams during emergency situations with the aim of categorizing information and aid requests made on social media for emergency service operators.",
      "year": 2021
    },
    "news": {
      "fullname": "News",
      "tasks": {
        "background": "Background Linking",
        "wikification": "Wikification"
      },
      "webpage": "http://trec-news.org/",
      "coordinators": "Donna Harman, National Institute of Standards and Technology (NIST):Shudong Huang, National Institute of Standards and Technology (NIST):Ian Soboroff, National Institute of Standards and Technology (NIST)",
      "description": "The News track features modern search tasks in the news domain. In partnership with The Washington Post, the track develops test collections that support the search needs of news readers and news writers in the current news environment.",
      "year": 2021
    },
    "fair": {
      "fullname": "Fair Ranking",
      "tasks": {
        "editors": "Editors",
        "coordinators": "Coordinators"
      },
      "webpage": "https://fair-trec.github.io/",
      "coordinators": "Michael Ekstrand, Boise State University:Isaac Johnson, Wikimedia:Graham McDonald, University of Glasgow:Amifa Raj, Boise State University",
      "description": "The Fair Ranking track focuses on building two-sided systems that offer fair exposure to ranked content producers while ensuring high results quality for ranking consumers.",
      "year": 2021
    },
    "deep": {
      "fullname": "Deep Learning",
      "tasks": {
        "docs": "Document Ranking",
        "passages": "Passage Ranking"
      },
      "webpage": "https://microsoft.github.io/msmarco/TREC-Deep-Learning",
      "coordinators": "Daniel Campos, University of Illinois:Nick Craswell, Microsoft:Jimmy Lin, Microsoft:Bhaskar Mitra, Microsoft:Emine Yilmaz, University College London",
      "description": "The Deep Learning track focuses on IR tasks where a large training set is available, allowing us to compare a variety of retrieval approaches including deep neural networks and strong non-neural approaches, to see what works best in a large-data regime.",
      "year": 2021
    },
    "trials": {
      "fullname": "Clinical Trials",
      "tasks": "",
      "webpage": "http://www.trec-cds.org/",
      "coordinators": "Dina Demner-Fushman, U.S. National Library of Medicine:William Hersh, Oregon Health and Science University:Kirk Roberts, University of Texas Health Science Center:Ellen Voorhees, National Institute of Standards and Technology (NIST)",
      "description": "The goal of the new Clinical Trials track is to focus research on the clinical trials matching problem: given a free text summary of a patient health record, find suitable clinical trials for that patient.",
      "year": 2021
    },
    "cast": {
      "fullname": "Conversational Assistance",
      "tasks": "",
      "webpage": "https://www.treccast.ai/",
      "coordinators": "Jamie Callan, Carnegie Mellon University:Jeff Dalton, University of Glasgow:Chenyan Xiong, Microsoft Research",
      "description": "The main aim of Conversational Assistance Track (CAsT) is to advance research on conversational search systems. The goal of the track is to create reusable benchmarks for open-domain information centric conversational dialogues.",
      "year": 2021
    },
    "misinfo": {
      "fullname": "Health Misinformation",
      "tasks": {
        "adhoc": "Adhoc Retrieval",
        "recall": "Total Recall"
      },
      "webpage": "https://trec-health-misinfo.github.io/",
      "coordinators": "Charlie Clarke, University of Waterloo:Maria Maistro, University of Copenhagen:Mark Smucker, University of Waterloo",
      "description": "The Health Misinformation track aims to (1) provide a venue for research on retrieval methods that promote better decision making with search engines, and (2) develop new online and offline evaluation methods to predict the decision making quality induced by search results. Consumer health information is used as the domain of interest in the track.",
      "year": 2021
    },
    "podcast": {
      "fullname": "Podcast",
      "tasks": {
        "retrieval": "Retrieval",
        "summarization": "Summarization"
      },
      "webpage": "https://trecpodcasts.github.io/",
      "coordinators": "Ann Clifton, Spotify:Ben Carterette, Spotify:Maria Eskevich, CLARIN ERIC:Gareth Jones, Dublin City University:Rosie Jones, Spotify:Jussi Karlgren, Spotify:Sravana Reddy, Spotify:Md Iftekhar Tanveer, Spotify",
      "description": "The aim of the Podcasts track is to develop methods for information retrieval and content understanding from open-domain podcast transcripts and audio.",
      "year": 2021
    }
  },
  "trec31": {
    "neuclir": {
      "fullname": "NeuCLIR",
      "tasks": {
        "fas": "Farsi Retrieval",
        "rus": "Russian Retrieval",
        "zho": "Chinese Retrieval"
      },
      "webpage": "https://neuclir.github.io/",
      "coordinators": "Dawn Lawrie, Johns Hopkins University:Sean MacAvaney, University of Glasgow:James Mayfield, Johns Hopkins University:Paul McNamee, Johns Hopkins University:Douglas W. Oard, University of Maryland:Luca Soldaini, Amazon Alexa AI:Eugene Yang, Johns Hopkins University",
      "description": "Cross-language Information Retrieval (CLIR) has been studied at TREC and subsequent evaluation forums for more than twenty years, but recent advances in the application of deep learning to information retrieval (IR) warrant a new, large-scale effort that will enable exploration of classical and modern IR techniques for this task.",
      "year": 2022
    },
    "misinfo": {
      "fullname": "Health Misinformation",
      "tasks": {
        "retrieval": "Retrieval",
        "prediction": "Prediction"
      },
      "webpage": "https://trec-health-misinfo.github.io/",
      "coordinators": "Charlie Clarke, University of Waterloo:Maria Maistro, University of Copenhagen:Mark Smucker, University of Waterloo",
      "description": "The Health Misinformation track aims to (1) provide a venue for research on retrieval methods that promote better decision making with search engines, and (2) develop new online and offline evaluation methods to predict the decision making quality induced by search results. Consumer health information is used as the domain of interest in the track.",
      "year": 2022
    },
    "deep": {
      "fullname": "Deep Learning",
      "tasks": {
        "docs": "Document Ranking",
        "passages": "Passage Ranking"
      },
      "webpage": "https://microsoft.github.io/msmarco/TREC-Deep-Learning",
      "coordinators": "Daniel Campos, University of Illinois at Urbana-Champaign:Nick Craswell, Microsoft:Jimmy Lin, University of Waterloo:Bhaskar Mitra, Microsoft:Emine Yilmaz, University College London",
      "description": "The Deep Learning track focuses on IR tasks where a large training set is available, allowing us to compare a variety of retrieval approaches including deep neural networks and strong non-neural approaches, to see what works best in a large-data regime.",
      "year": 2022
    },
    "cast": {
      "fullname": "Conversational Assistance",
      "tasks": {
        "mixed": "Mixed Initiative",
        "primary": "Primary"
      },
      "webpage": "https://www.treccast.ai/",
      "coordinators": "Leif Azzopardi, University of Strathclyde:Jeff Dalton, University of Glasgow:Mohammed Alian Nejadi, University of Amsterdam:Paul Ogbonoko, University of Glasgow:Johanne Trippas, University of Melbourne:Svitlana Vakulenko, University of Amsterdam",
      "description": "The main aim of Conversational Assistance Track (CAsT) is to advance research on conversational search systems. The goal of the track is to create reusable benchmarks for open-domain information centric conversational dialogues.",
      "year": 2022
    },
    "trials": {
      "fullname": "Clinical Trials",
      "tasks": "",
      "webpage": "http://www.trec-cds.org/",
      "coordinators": "Dina Demner-Fushman, U.S. National Library of Medicine:William Hersh, Oregon Health and Science University:Kirk Roberts, University of Texas Health Science Center",
      "description": "The goal of the Clinical Trials track is to focus research on the clinical trials matching problem: given a free text summary of a patient health record, find suitable clinical trials for that patient.",
      "year": 2022
    },
    "fair": {
      "fullname": "Fair Ranking",
      "tasks": {
        "editors": "Editors",
        "coordinators": "Coordinators"
      },
      "webpage": "https://fair-trec.github.io/",
      "coordinators": "Michael Ekstrand, Boise State University:Isaac Johnson, Wikimedia Foundation:Graham McDonald, University of Glasgow:Amifa Raj, Boise State University",
      "description": "The Fair Ranking track focuses on building two-sided systems that offer fair exposure to ranked content producers while ensuring high results quality for ranking consumers.",
      "year": 2022
    },
    "crisis": {
      "fullname": "CrisisFACTs",
      "tasks": "",
      "webpage": "https://crisisfacts.github.io/",
      "coordinators": "Cody Buntain, University of Maryland:Richard McCreadie, University of Glasgow",
      "description": "The CrisisFACTS track focuses on temporal summarization for first responders in emergency situations. These summaries differ from traditional summarization in that they order information by time and produce a series of short updates instead of a longer narrative.",
      "year": 2022
    }
  },
  "trec32": {
    "trials": {
      "fullname": "Clinical Trials",
      "tasks": "",
      "webpage": "http://www.trec-cds.org/",
      "coordinators": "Dina Demner-Fushman, U.S. National Library of Medicine:William Hersh, Oregon Health and Science University:Kirk Roberts, University of Texas Health Science Center",
      "description": "The goal of the Clinical Trials track is to focus research on the clinical trials matching problem: given a free text summary of a patient health record, find suitable clinical trials for that patient.",
      "year": 2023
    },
    "crisis": {
      "fullname": "CrisisFACTs",
      "tasks": "",
      "webpage": "https://crisisfacts.github.io/",
      "coordinators": "Cody Buntain, University of Maryland:Benjamin Horne, University of Tennessee-Knoxville:Amanda Hughes, Brigham Young University:Muhammad Imran, QCRI:Richard McCreadie, University of Glasgow:Hemant Purohit, George Mason University",
      "description": "The CrisisFACTS track focuses on temporal summarization for first responders in emergency situations. These summaries differ from traditional summarization in that they order information by time and produce a series of short updates instead of a longer narrative.",
      "year": 2023
    },
    "deep": {
      "fullname": "Deep Learning",
      "tasks": {
        "docs": "Document Ranking",
        "passages": "Passage Ranking"
      },
      "webpage": "https://microsoft.github.io/msmarco/TREC-Deep-Learning",
      "coordinators": "Nick Craswell, Microsoft:Bhaskar Mitra, Microsoft Research:Emine Yilmaz, University College London:Daniel Campos, University of Illinois at Urbana-Champaign:Jimmy Lin, University of Waterloo",
      "description": "The Deep Learning track focuses on IR tasks where a large training set is available, allowing us to compare a variety of retrieval approaches including deep neural networks and strong non-neural approaches, to see what works best in a large-data regime.",
      "year": 2023
    },
    "ikat": {
      "fullname": "Interactive Knowledge Assistance",
      "tasks": "",
      "webpage": "https://trecikat.com/",
      "coordinators": "Mohammed Aliannejadi, University of Amsterdam:Zahra Abbasiantaeb, University of Amsterdam:Shubham Chatterjee, University of Glasgow:Jeff Dalton, University of Glasgow:Leif Azzopardi, University of Strathclyde",
      "description": "iKAT is the successor to the Conversational Assistance Track (CAsT). The fourth year of CAST aimed to add more conversational elements to the interaction streams, by introducing mixed initiatives (clarifications, and suggestions) to create multi-path, multi-turn conversations for each topic. TREC iKAT evolves CAsT into a new track to signal this new trajectory. iKAT aims to focus on supporting multi-path, multi-turn, multi-perspective conversations. That is for a given topic, the direction and the conversation that evolves depends not only on the prior responses but also on the user.",
      "year": 2023
    },
    "neuclir": {
      "fullname": "NeuCLIR",
      "tasks": {
        "fas": "Farsi Retrieval",
        "rus": "Russian Retrieval",
        "zho": "Chinese Retrieval",
        "mlir": "Multilingual Information Retrieval"
      },
      "webpage": "https://neuclir.github.io/",
      "coordinators": "Dawn Lawrie, Johns Hopkins University:Sean MacAvaney, University of Glasgow:James Mayfield, Johns Hopkins University:Paul McNamee, Johns Hopkins University:Douglas W. Oard, University of Maryland:Luca Soldaini, Allen Institute for AI:Eugene Yang, Johns Hopkins University",
      "description": "Cross-language Information Retrieval (CLIR) has been studied at TREC and subsequent evaluation forums for more than twenty years, but recent advances in the application of deep learning to information retrieval (IR) warrant a new, large-scale effort that will enable exploration of classical and modern IR techniques for this task.",
      "year": 2023
    },
    "atomic": {
      "fullname": "AToMiC",
      "tasks": {
        "suggest": "Image Suggestion (T2M)",
        "promote": "Image Promotion (M2T)"
      },
      "webpage": "https://trec-atomic.github.io/",
      "coordinators": "Jheng-Hong (Matt) Yang, University of Waterloo:Jimmy Lin, University of Waterloo:Carlos Lassance, Naver Labs Europe:Rafael S. Rezende, Naver Labs Europe:Stéphane Clinchant, Naver Labs Europe:Krishna Srinivasan, Google Research:Miriam Redi, Wikimedia Foundation",
      "description": "The Authoring Tools for Multimedia Content (AToMiC) Track aims to build reliable benchmarks for multimedia search systems. The focus of this track is to develop and evaluate IR techniques for text-to-image and image-to-text search problems.",
      "year": 2023
    },
    "product": {
      "fullname": "Product Search",
      "tasks": "",
      "webpage": "https://trec-product-search.github.io/",
      "coordinators": "Daniel Campos, University of Illinois at Urbana-Champaign:Corby Rosset, Microsoft:Surya Kallumadi, Lowes:ChengXiang Zhai, University of Illinois at Urbana-Champaign:Alessandro Magnani, Walmart",
      "description": "The product search track focuses on IR tasks in the world of product search and discovery. This track seeks to understand what methods work best for product search, improve evaluation methodology, and provide a reusable dataset which allows easy benchmarking in a public forum. ",
      "year": 2023
    },
    "tot": {
      "fullname": "Tip-of-the-Tongue",
      "tasks": "",
      "webpage": "https://trec-tot.github.io/",
      "coordinators": "Jaime Arguello, University of North Carolina:Samarth Bhargav, University of Amsterdam:Bhaskar Mitra, Microsoft Research:Fernando Diaz, Google:Evangelos Kanoulas, University of Amsterdam",
      "description": "The Tip-of-the-Tongue (ToT) Track focuses on the known-item identification task where the searcher has previously experienced or consumed the item (e.g., a movie) but cannot recall a reliable identifier (i.e., It's on the tip of my tongue...). Unlike traditional ad-hoc keyword-based search, these information requests tend to be natural-language, verbose, and complex containing a wide variety of search strategies such as multi-hop reasoning, and frequently express uncertainty and suffer from false memories.",
      "year": 2023
    }
  },
  "trec33": {
    "avs": {
      "fullname": "Adhoc Video Search",
      "tasks": {
        "main": "Main task",
        "progress": "Progress task"
      },
      "webpage": "https://www-nlpir.nist.gov/projects/tv2024/avs.html",
      "coordinators": "Georges Quenot, University of Grenoble:George Awad, NIST",
      "description": "The Ad-hoc search task goal is to model the end user search use-case, who is searching (using textual sentence queries) for segments of video containing persons, objects, activities, locations, etc. and combinations of the former. While the Internet Archive (IACC.3) dataset was adopted between 2016 to 2018, from 2019 to 2021 a new data collection (V3C1) based on Vimeo Creative Commons (V3C) datset was adopted. Starting in 2022 the task started to utilize a new sub-collection V3C2 to test systems on a new set of queries in addition to common (fixed) progress query set to measure system progress from 2022 to 2024.",
      "year": 2024
    },
    "atomic": {
      "fullname": "AToMiC",
      "tasks": {
        "suggest": "Image Suggestion (T2M)",
        "promote": "Image Promotion (M2T)"
      },
      "webpage": "https://trec-atomic.github.io/",
      "coordinators": "Jheng-Hong (Matt) Yang, University of Waterloo:Jimmy Lin, University of Waterloo:Carlos Lassance, Naver Labs Europe:Rafael S. Rezende, Naver Labs Europe:Stéphane Clinchant, Naver Labs Europe:Krishna Srinivasan, Google Research:Miriam Redi, Wikimedia Foundation",
      "description": "The Authoring Tools for Multimedia Content (AToMiC) Track aims to build reliable benchmarks for multimedia search systems. The focus of this track is to develop and evaluate IR techniques for text-to-image and image-to-text search problems.",
      "year": 2024
    },
    "biogen": {
      "fullname": "Biomedical Generative Retrieval (BioGen) Track",
      "tasks": "",
      "webpage": "https://dmice.ohsu.edu/trec-biogen/task.html",
      "coordinators": "Bill Hersh, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:Deepak Gupta, National Library of Medicine:Steven Bedrick, Oregon Health & Science University:Kirk Roberts, University of Texas Houston",
      "description": "Large language models (LLMs) adapted for the biomedical domain show exceptional performance on many tasks, but are also known to provide false information, i.e., hallucinations or confabulations. Inaccuracies may be particularly harmful in high-risk situations, such as making clinical decisions or appraising biomedical research. The TREC 2024 BioGen task will focus on reference attribution as a means to mitigate generation of false statements by LLMs.  The goal of the TREC 2024 BioGen task will be to cite references to support the text of the sentences and the overall answer from LLM output for each topic. Each run will be scored by the proportion of sentences and overall answer that have correctly supporting attributions. ",
      "year": 2024
    },
    "ikat": {
      "fullname": "Interactive Knowledge Assistance",
      "tasks": "",
      "webpage": "https://trecikat.com/",
      "coordinators": "Mohammed Aliannejadi, University of Amsterdam:Zahra Abbasiantaeb, University of Amsterdam:Simon Lupart, University of Amsterdam:Shubham Chatterjee, University of Glasgow:Jeff Dalton, University of Glasgow:Leif Azzopardi, University of Strathclyde",
      "description": "iKAT is the successor to the Conversational Assistance Track (CAsT). The fourth year of CAST aimed to add more conversational elements to the interaction streams, by introducing mixed initiatives (clarifications, and suggestions) to create multi-path, multi-turn conversations for each topic. TREC iKAT evolves CAsT into a new track to signal this new trajectory. iKAT aims to focus on supporting multi-path, multi-turn, multi-perspective conversations. That is for a given topic, the direction and the conversation that evolves depends not only on the prior responses but also on the user.",
      "year": 2024
    },
    "lateral": {
      "fullname": "Lateral Reading",
      "tasks": {
        "qgen": "Question generation",
        "retrieval": "Answer retrieval"
      },
      "coordinators": "Dake Zhang, University of Waterloo:Mark Smucker, University of Waterloo:Charles L. A. Clarke, University of Waterloo",
      "description": "The TREC Lateral Reading Track is for researchers interested in addressing the problems of misinformation and trust in search and online content. The current web landscape requires the ability to make judgments about the trustworthiness of information, which is a difficult task for most people. Meanwhile, automated detection of misinformation is likely to remain limited to well-defined domains or be limited to simple fact-checking.",
      "year": 2024
    },
    "medvidqa": {
      "fullname": "Medical Video Question Answering",
      "tasks": {
        "vcval": "Video Corpus Visual Answer Localization",
        "qfisc": "Query-Focused Instructional Step Captioning"
      },
      "coordinators": "Deepak Gupta, National Library of Medicine, NIH:Dina Demner-Fushman, National Library of Medicine, NIH",
      "description": "The recent surge in the availability of online videos has changed the way of acquiring information and knowledge. Many people prefer instructional videos to teach or learn how to accomplish a particular task in an effective and efficient manner with a series of step-by-step procedures. Similarly, medical instructional videos are more suitable and beneficial for delivering key information through visual and verbal communication to consumers' healthcare questions that demand instruction. We aim to extract the visual information from the video corpus for consumers' first aid, medical emergency, and medical educational questions. Extracting the relevant information from the video corpus requires relevant video retrieval, moment localization, video summarization, and captioning skills. Toward this, the TREC task, Medical Video Question Answering, focuses on developing systems capable of understanding medical videos and providing visual answers (from single and multiple videos) and instructional step captions to answer natural language questions. Emphasizing the importance of multimodal capabilities, the task requires systems to generate instructional questions and captions based on medical video content. Following the MedVidQA 2023, TREC 2024 expanded the tasks considering language-video understanding and generation. This track is comprised of two main tasks: Video Corpus Visual Answer Localization (VCVAL) and Query-Focused Instructional Step Captioning (QFISC).",
      "year": 2024
    },
    "neuclir": {
      "fullname": "NeuCLIR",
      "tasks": {
        "fas": "Farsi Retrieval",
        "rus": "Russian Retrieval",
        "zho": "Chinese Retrieval",
        "mlir": "Multilingual Information Retrieval",
        "tech": "Chinese Technical Document Retrieval",
        "report": "Report Generation Pilot"
      },
      "webpage": "https://neuclir.github.io/",
      "coordinators": "Dawn Lawrie, Johns Hopkins University:Sean MacAvaney, University of Glasgow:James Mayfield, Johns Hopkins University:Paul McNamee, Johns Hopkins University:Douglas W. Oard, University of Maryland:Luca Soldaini, Allen Institute for AI:Eugene Yang, Johns Hopkins University",
      "description": "Cross-language Information Retrieval (CLIR) has been studied at TREC and subsequent evaluation forums for more than twenty years, but recent advances in the application of deep learning to information retrieval (IR) warrant a new, large-scale effort that will enable exploration of classical and modern IR techniques for this task.",
      "year": 2024
    },
    "plaba": {
      "fullname": "Plain-Language Adaptation of Biomedical Abstracts",
      "tasks": {
        "term": "Term replacement",
        "adapt": "Complete abstract adaptation"
      },
      "coordinators": "Brian Ondov, Yale School of Medicine:Bill Xia, U.S. National Library of Medicine:Ishita Unde, U.S. National Library of Medicine:Dina Demner-Fushman, U.S. National Library of Medicine:Hoa T. Dang, NIST",
      "description": "The goal of the PLABA track is to improve health literacy by adapting biomedical abstracts for the general public using plain language. When adapting, source sentences may be split, in which case the output for one source sentence will be multiple target sentences. However, source sentences may not be merged, and the output for a given source sentence should not contain information from other source sentences. Both source and output will be in English. An example of adaptation is below.",
      "year": 2024
    },
    "product": {
      "fullname": "Product Search",
      "tasks": "",
      "webpage": "https://trec-product-search.github.io/",
      "coordinators": "Daniel Campos, University of Illinois at Urbana-Champaign:Corby Rosset, Microsoft:Surya Kallumadi, Lowes:ChengXiang Zhai, University of Illinois at Urbana-Champaign:Alessandro Magnani, Walmart",
      "description": "The product search track focuses on IR tasks in the world of product search and discovery. This track seeks to understand what methods work best for product search, improve evaluation methodology, and provide a reusable dataset which allows easy benchmarking in a public forum. ",
      "year": 2023
    },
    "rag": {
      "fullname": "Retrieval-Augmented Generation",
      "webpage": "https://trec-rag.github.io",
      "tasks": {
        "retrieve": "Retrieval",
        "auggen": "Augmented Generation",
        "gen": "Generation"
      },
      "coordinators": "Ronak Pradeep, University of Waterloo:Nandan Thakur, University of Waterloo:Jimmy Lin, University of Waterloo:Nick Craswell, Microsoft",
      "description": "The (TREC) Retrieval-Augmented Generation Track is intended to foster innovation and research within the field of retrieval-augmented generation systems. This area of research focuses on combining retrieval methods - techniques for finding relevant information within large corpora with Large Language Models (LLMs) to enhance the ability of systems to produce relevant, accurate, updated and contextually appropriate content.",
      "year": 2024
    },
    "tot": {
      "fullname": "Tip-of-the-Tongue",
      "tasks": "",
      "webpage": "https://trec-tot.github.io/",
      "coordinators": "Jaime Arguello, University of North Carolina:Samarth Bhargav, University of Amsterdam:Bhaskar Mitra, Microsoft Research:Fernando Diaz, Google:Evangelos Kanoulas, University of Amsterdam",
      "description": "The Tip-of-the-Tongue (ToT) Track focuses on the known-item identification task where the searcher has previously experienced or consumed the item (e.g., a movie) but cannot recall a reliable identifier (i.e., It's on the tip of my tongue...). Unlike traditional ad-hoc keyword-based search, these information requests tend to be natural-language, verbose, and complex containing a wide variety of search strategies such as multi-hop reasoning, and frequently express uncertainty and suffer from false memories.",
      "year": 2023
    },
    "vtt": {
      "fullname": "Video-To-Text",
      "tasks": "",
      "webpage": "https://www-nlpir.nist.gov/projects/tv2024/vtt.html",
      "coordinators": "George Awad, NIST:Yvette Graham, Trinity College Dublin:Afzal Godil, NIST",
      "description": "Automatic annotation of videos using natural language text descriptions has been a long-standing goal of computer vision. The task involves understanding of many concepts such as objects, actions, scenes, person-object relations, temporal order of events and many others. In recent years there have been major advances in computer vision techniques that enabled researchers to try to solve this problem. A lot of use case application scenarios can greatly benefit from such technology such as video summarization in the form of natural language, facilitating the search and browsing of video archives using such descriptions, describing videos to the blind, etc. In addition, learning video interpretation and temporal relations of events in the video will likely contribute to other computer vision tasks, such as prediction of future events from videos.",
      "year": 2024
    }
  },
  "trec-covid": {
    "round1": {
      "fullname": "Round 1",
      "tasks": "",
      "webpage": "https://ir.nist.gov/trec-covid/",
      "coordinators": "Steven Bedrick, Oregon Health & Science University:Aaron Cohen, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:William Hersh, Oregon Health & Science University:Kyle Lo, Allen Institute for Artificial Intelligence:Kirk Roberts, University of Texas Health Science Center at Houston:Ian Soboroff, National Institute of Standards and Technology:Ellen Voorhees, National Institute of Standards and Technology:Lucy Lu Wang, Allen Institute for Artificial Intelligence",
      "description": "Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presented a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge identify answers for some of today's questions and create infrastructure to improve tomorrow's search systems. TREC-COVID followed the TREC model for building IR test collections through community evaluations of search systems. The document set used in the challenge is the COVID-19 Open Research Dataset (CORD-19). This is a collection of biomedical literature articles that is updated regularly. Accordingly, TREC-COVID consisted of a series of rounds, with each round using a later version of the document set and a larger set of COVID-related topics. Participants in a round created ranked lists of documents for each topic ('runs') and submitted their runs to NIST. Based on the collective set of participants' runs, NIST created sets of documents to be assessed for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, were then used to score the submitted runs. The final document and topic sets together with the cumulative relevance judgments comprise a COVID test collection called TREC-COVID Complete. The incremental nature of the collection as viewed through the successive rounds supports research on search systems for dynamic environments.",
      "year": 2020
    },
    "round2": {
      "fullname": "Round 2",
      "tasks": "",
      "webpage": "https://ir.nist.gov/trec-covid/",
      "coordinators": "Steven Bedrick, Oregon Health & Science University:Aaron Cohen, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:William Hersh, Oregon Health & Science University:Kyle Lo, Allen Institute for Artificial Intelligence:Kirk Roberts, University of Texas Health Science Center at Houston:Ian Soboroff, National Institute of Standards and Technology:Ellen Voorhees, National Institute of Standards and Technology:Lucy Lu Wang, Allen Institute for Artificial Intelligence",
      "description": "Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presented a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge identify answers for some of today's questions and create infrastructure to improve tomorrow's search systems. TREC-COVID followed the TREC model for building IR test collections through community evaluations of search systems. The document set used in the challenge is the COVID-19 Open Research Dataset (CORD-19). This is a collection of biomedical literature articles that is updated regularly. Accordingly, TREC-COVID consisted of a series of rounds, with each round using a later version of the document set and a larger set of COVID-related topics. Participants in a round created ranked lists of documents for each topic ('runs') and submitted their runs to NIST. Based on the collective set of participants' runs, NIST created sets of documents to be assessed for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, were then used to score the submitted runs. The final document and topic sets together with the cumulative relevance judgments comprise a COVID test collection called TREC-COVID Complete. The incremental nature of the collection as viewed through the successive rounds supports research on search systems for dynamic environments.",
      "year": 2020
    },
    "round3": {
      "fullname": "Round 3",
      "tasks": "",
      "webpage": "https://ir.nist.gov/trec-covid/",
      "coordinators": "Steven Bedrick, Oregon Health & Science University:Aaron Cohen, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:William Hersh, Oregon Health & Science University:Kyle Lo, Allen Institute for Artificial Intelligence:Kirk Roberts, University of Texas Health Science Center at Houston:Ian Soboroff, National Institute of Standards and Technology:Ellen Voorhees, National Institute of Standards and Technology:Lucy Lu Wang, Allen Institute for Artificial Intelligence",
      "description": "Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presented a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge identify answers for some of today's questions and create infrastructure to improve tomorrow's search systems. TREC-COVID followed the TREC model for building IR test collections through community evaluations of search systems. The document set used in the challenge is the COVID-19 Open Research Dataset (CORD-19). This is a collection of biomedical literature articles that is updated regularly. Accordingly, TREC-COVID consisted of a series of rounds, with each round using a later version of the document set and a larger set of COVID-related topics. Participants in a round created ranked lists of documents for each topic ('runs') and submitted their runs to NIST. Based on the collective set of participants' runs, NIST created sets of documents to be assessed for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, were then used to score the submitted runs. The final document and topic sets together with the cumulative relevance judgments comprise a COVID test collection called TREC-COVID Complete. The incremental nature of the collection as viewed through the successive rounds supports research on search systems for dynamic environments.",
      "year": 2020
    },
    "round4": {
      "fullname": "Round 4",
      "tasks": "",
      "webpage": "https://ir.nist.gov/trec-covid/",
      "coordinators": "Steven Bedrick, Oregon Health & Science University:Aaron Cohen, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:William Hersh, Oregon Health & Science University:Kyle Lo, Allen Institute for Artificial Intelligence:Kirk Roberts, University of Texas Health Science Center at Houston:Ian Soboroff, National Institute of Standards and Technology:Ellen Voorhees, National Institute of Standards and Technology:Lucy Lu Wang, Allen Institute for Artificial Intelligence",
      "description": "Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presented a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge identify answers for some of today's questions and create infrastructure to improve tomorrow's search systems. TREC-COVID followed the TREC model for building IR test collections through community evaluations of search systems. The document set used in the challenge is the COVID-19 Open Research Dataset (CORD-19). This is a collection of biomedical literature articles that is updated regularly. Accordingly, TREC-COVID consisted of a series of rounds, with each round using a later version of the document set and a larger set of COVID-related topics. Participants in a round created ranked lists of documents for each topic ('runs') and submitted their runs to NIST. Based on the collective set of participants' runs, NIST created sets of documents to be assessed for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, were then used to score the submitted runs. The final document and topic sets together with the cumulative relevance judgments comprise a COVID test collection called TREC-COVID Complete. The incremental nature of the collection as viewed through the successive rounds supports research on search systems for dynamic environments.",
      "year": 2020
    },
    "round5": {
      "fullname": "Round 5",
      "tasks": "",
      "webpage": "https://ir.nist.gov/trec-covid/",
      "coordinators": "Steven Bedrick, Oregon Health & Science University:Aaron Cohen, Oregon Health & Science University:Dina Demner-Fushman, National Library of Medicine:William Hersh, Oregon Health & Science University:Kyle Lo, Allen Institute for Artificial Intelligence:Kirk Roberts, University of Texas Health Science Center at Houston:Ian Soboroff, National Institute of Standards and Technology:Ellen Voorhees, National Institute of Standards and Technology:Lucy Lu Wang, Allen Institute for Artificial Intelligence",
      "description": "Researchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presented a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge identify answers for some of today's questions and create infrastructure to improve tomorrow's search systems. TREC-COVID followed the TREC model for building IR test collections through community evaluations of search systems. The document set used in the challenge is the COVID-19 Open Research Dataset (CORD-19). This is a collection of biomedical literature articles that is updated regularly. Accordingly, TREC-COVID consisted of a series of rounds, with each round using a later version of the document set and a larger set of COVID-related topics. Participants in a round created ranked lists of documents for each topic ('runs') and submitted their runs to NIST. Based on the collective set of participants' runs, NIST created sets of documents to be assessed for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, were then used to score the submitted runs. The final document and topic sets together with the cumulative relevance judgments comprise a COVID test collection called TREC-COVID Complete. The incremental nature of the collection as viewed through the successive rounds supports research on search systems for dynamic environments.",
      "year": 2020
    }
  }
}
