{
  "trec33": {
    "ikat": {
      "infosenselab-trec2024-papers-proc-1": {
        "file": "infosenselab.ikat.pdf",
        "pid": "infosenselab",
        "title": "Passage Query Methods for Retrieval and Reranking in Conversational Agents",
        "author": "Victor De Lima (Georgetown InfoSense), Grace Hui Yang (Georgetown InfoSense)",
        "abstract": "This paper presents our approach to the TREC Interactive Knowledge Assistance Track (iKAT), which focuses on improving conversational information-seeking (CIS) systems. While recent advancements in CIS have improved conversational agents' ability to assist users, significant challenges remain in understanding context and retrieving relevant documents across domains and dialogue turns. To address these issues, we extend the Generate-Retrieve-Generate pipeline by developing passage queries (PQs) that align with the target document's expected format to improve query-document matching during retrieval. We propose two variations of this approach: Weighted Reranking and Short and Long Passages. Each method leverages a Meta Llama model for context understanding and generating queries and responses. Passage ranking evaluation results show that the Short and Long Passages approach outperformed the organizers' baselines, performed best among Llama-based systems in the track, and achieved results comparable to GPT-4-based systems. These results indicate that the method effectively balances efficiency and performance. Findings suggest that PQs improve semantic alignment with target documents and demonstrate their potential to improve multi-turn dialogue systems.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/infosenselab.ikat.pdf",
        "key": "infosenselab-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{infosenselab-trec2024-papers-proc-1,\n   author = {Victor De Lima (Georgetown InfoSense), Grace Hui Yang (Georgetown InfoSense)},\n   title = {Passage Query Methods for Retrieval and Reranking in Conversational Agents},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {infosenselab},\n   trec_runs = {infosense_llama_pssgqrs_wghtdrerank_2, infosense_llama_pssgqrs_wghtdrerank_1, infosense_llama_short_long_qrs_2, infosense_llama_short_long_qrs_3},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/infosenselab.ikat.pdf}\n}"
      },
      "nii-trec2024-papers-proc-1": {
        "file": "nii.ikat.pdf",
        "pid": "nii",
        "title": "NII@TREC IKAT 2024:LLM-Based Pipelines for Personalized Conversational Information Seeking",
        "author": "Xiao Fu (UCL), Navdeep Singh Bedi (USI), Praveen Acharya (DCU), Noriko Kando (NII)",
        "abstract": "In this paper, we propose two novel pipelines\u2014Retrieve-then-Generate (RtG) and Generate-then-Retrieve (GtR)\u2014to enhance conversational information seeking (CIS) systems, evaluated within the TREC iKAT 2023 framework. The RtG pipeline emphasizes brevity in rewriting user utterances and generates multiple query groups to maximize the retrieval of relevant documents. This approach leads to improved recall in the final results compared to the best submission in 2023. Additionally, it incorporates a chain-of-thought methodology through a two-stage response generation process. In a zero-shot setting, the GtR pipeline introduces a hybrid approach by ensembling state-of-the-art Large Language Models (LLMs), specifically GPT-4o and Claude-3-opus. By leveraging the strengths of multiple LLMs, the GtR pipeline achieves high recall while maintaining competitive precision and ranking performance in both document retrieval and Personal Task Knowledge Base (PTKB) statement classification tasks. Our experimental results demonstrate that both pipelines significantly enhance retrieval effectiveness, offering robust solutions for future CIS systems.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/nii.ikat.pdf",
        "key": "nii-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{nii-trec2024-papers-proc-1,\n   author = {Xiao Fu (UCL), Navdeep Singh Bedi (USI), Praveen Acharya (DCU), Noriko Kando (NII)},\n   title = {NII@TREC IKAT 2024:LLM-Based Pipelines for Personalized Conversational Information Seeking},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {nii},\n   trec_runs = {nii_res_gen, nii_auto_base, nii_manu_base, nii_auto_ptkb_rr, nii_manu_ptkb_rr, NII_automatic_GeRe},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/nii.ikat.pdf}\n}"
      },
      "rali lab-trec2024-papers-proc-1": {
        "file": "rali lab.ikat.pdf",
        "pid": "rali lab",
        "title": "RALI@TREC iKAT 2024: Achieving Personalization via Retrieval Fusion in Conversational Search",
        "author": "Yuchen Hui (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Fengran Mo (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Milan Mao (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Jian-Yun Nie (RALI Lab, Universit\u00e9 de Montr\u00e9al)",
        "abstract": "The Recherche Appliqu\u00e9e en Linguistique Informatique (RALI) team participated in the 2024 TREC Interactive Knowledge Assistance (iKAT) Track. In personalized conversational search, effectively capturing a user's complex search intent requires incorporating both contextual information and key elements from the user profile into query reformulation. The user profile often contains many relevant pieces, and each could potentially complement the user's information needs. It is difficult to disregard any of them, whereas introducing an excessive number of these pieces risks drifting from the original query and hinders search performance. This is a challenge we denote as over-personalization. In this paper, we tackle the problem via employing different strategies based on fusing ranking lists generated from the queries with different levels of personalization.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/rali lab.ikat.pdf",
        "key": "rali lab-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{rali lab-trec2024-papers-proc-1,\n   author = {Yuchen Hui (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Fengran Mo (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Milan Mao (RALI Lab, Universit\u00e9 de Montr\u00e9al) , Jian-Yun Nie (RALI Lab, Universit\u00e9 de Montr\u00e9al)},\n   title = {RALI@TREC iKAT 2024: Achieving Personalization via Retrieval Fusion in Conversational Search},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {rali lab},\n   trec_runs = {RALI_gpt4o_fusion_rerank, RALI_gpt4o_no_personalize_fusion_rerank, RALI_gpt4o_no_personalize_fusion_norerank, RALI_gpt4o_fusion_norerank, RALI_manual_monot5, RALI_manual_rankllama},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/rali lab.ikat.pdf}\n}"
      },
      "ii_research-trec2024-papers-proc-1": {
        "file": "ii_research.ikat.pdf",
        "pid": "ii_research",
        "title": "IIUoT at TREC 2024 Interactive Knowledge Assistance Track",
        "author": "Yating Zhang (University of Tsukuba), Haitao Yu (University of Tsukuba)",
        "abstract": "In conversational information-seeking (CIS), the ability to tailor responses to individual user contexts is essential for enhancing relevance and accuracy. The TREC Interactive Knowledge Assistance Track addresses this need by advancing research in personalized conversational agents that adapt dynamically to user-specific details and preferences. Our study aligns with this framework, which involves three core tasks: personal textual knowledge base (PTKB) statement ranking, passage ranking, and response generation. To address these tasks, we propose a comprehensive framework that incorporates user context at each stage. For PTKB statement ranking, we integrate embedding models with large language models (LLMs) to optimize relevance-based ranking precision, allowing for more nuanced alignment of user characteristics with retrieved information. In the passage ranking stage, our adaptive retrieval strategy combines BM25 with iterative contextual refinement, enhancing the relevance and accuracy of retrieved passages. Finally, our response generation module leverages a Retrieval-Augmented Generation (RAG) model that dynamically synthesizes user-specific context and external knowledge, producing responses that are both precise and contextually relevant. Experimental results demonstrate that our framework effectively addresses the complexities of personalized CIS, achieving notable improvements over traditional static retrieval methods.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/ii_research.ikat.pdf",
        "key": "ii_research-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{ii_research-trec2024-papers-proc-1,\n   author = {Yating Zhang (University of Tsukuba), Haitao Yu (University of Tsukuba)},\n   title = {IIUoT at TREC 2024 Interactive Knowledge Assistance Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {ii_research},\n   trec_runs = {iiresearch_ikat2024_rag_top5_bge_reranker, iiresearch_ikat2024_rag_top5_monot5_reranker},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/ii_research.ikat.pdf}\n}"
      },
      "DCU-ADAPT-trec2024-papers-proc-1": {
        "file": "DCU-ADAPT.ikat.pdf",
        "pid": "DCU-ADAPT",
        "title": "DCU-ADAPT@TREC iKAT 2024: Incorporating Retrieved Knowledge for Enhanced Conversational Search",
        "author": "Praveen Acharya (Dublin City University), Xiao Fu (University College London), Noriko Kando (National Institute of Informatics), Gareth J. F. Jones (Dublin City University)",
        "abstract": "Users of search applications often encounter difficulties in expressing their information needs effectively. Conversational search (CS) can potentially support users in creating effective queries by enabling a multi-turn, iterative dialogue between a User and the search System. These dialogues help users to refine and build their understanding of their information need through a series of query-response exchanges. However, current CS systems generally do not accumulate knowledge about the user's information needs or the content with which they have engaged during this dialogue. This limitation can hinder the system's ability to support users effectively. To address this issue, we propose an approach that seeks to model and utilize knowledge gained from each interaction to enhance future user queries. Our method focuses on incorporating knowledge from retrieved documents to enrich subsequent user queries, ultimately improving query comprehension and retrieval outcomes. We test the effectiveness of our proposed approach in our TREC iKAT 2024 participation.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/DCU-ADAPT.ikat.pdf",
        "key": "DCU-ADAPT-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{DCU-ADAPT-trec2024-papers-proc-1,\n   author = {Praveen Acharya (Dublin City University), Xiao Fu (University College London), Noriko Kando (National Institute of Informatics), Gareth J. F. Jones (Dublin City University)},\n   title = {DCU-ADAPT@TREC iKAT 2024: Incorporating Retrieved Knowledge for Enhanced Conversational Search},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {DCU-ADAPT},\n   trec_runs = {dcu_manual_qe_summ_TopP_3, dcu_manual_qe_summ_ptkb_TopP_3, dcu_auto_qe_key_topP-50_topK-5, dcu_auto_qre_sim, dcu_auto_qe_summ_TopP_3, dcu_auto_qe_summ_ptkb_TopP_},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/DCU-ADAPT.ikat.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-4": {
        "file": "Overview_ikat.pdf",
        "pid": "coordinators",
        "title": "TREC iKAT 2024: The Interactive Knowledge Assistance Track Overview",
        "author": "Mohammad Aliannejadi (University of Amsterdam), Zahra Abbasiantaeb (University of Amsterdam), Simon Lupart (University of Amsterdam), Shubham Chatterjee (University of Edinburgh), Jeffrey Dalton (University of Edinburgh), Leif Azzopardi (University of Strathclyde)",
        "abstract": "Conversational information seeking has evolved rapidly in the last few years with the development of large language models (LLMs) providing the basis for interpreting and responding in a naturalistic manner to user requests. iKAT emphasizes the creation and research of conversational search agents that adapt responses based on the user's prior interactions and present context, maintaining a long-term memory of user-system interactions. This means that the same question might yield varied answers, contingent on the user\u2019s profile and preferences. The challenge lies in enabling conversational search agents (CSA) to incorporate personalized context to guide users through the relevant information effectively. iKAT's second year attracted seven teams and a total of 31 runs. Most of the runs leveraged LLMs in their pipelines with some LLMs to do a single query rewrite, while others leveraged LLMs to do multiple query rewrites.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_ikat.pdf",
        "key": "coordinators-trec2024-papers-proc-4",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-4,\n   author = {Mohammad Aliannejadi (University of Amsterdam), Zahra Abbasiantaeb (University of Amsterdam), Simon Lupart (University of Amsterdam), Shubham Chatterjee (University of Edinburgh), Jeffrey Dalton (University of Edinburgh), Leif Azzopardi (University of Strathclyde)},\n   title = {TREC iKAT 2024: The Interactive Knowledge Assistance Track Overview},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_ikat.pdf}\n}"
      },
      "uva-trec2024-papers-proc-1": {
        "file": "uva.ikat.pdf",
        "pid": "uva",
        "title": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search",
        "author": "Simon Lupart (University of Amsterdam), Zahra Abbasiantaeb (University of Amsterdam), Mohammad Aliannejadi (University of Amsterdam)",
        "abstract": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing conversational assistants, able to adapt their interaction and responses from personalized user knowledge. The track incorporates a Personal Textual Knowledge Base (PTKB) alongside Conversational AI tasks, such as passage ranking and response generation. Query Rewrite being an effective approach for resolving conversational context, we explore Large Language Models (LLMs), as query rewriters. Specifically, our submitted runs explore multi-aspect query generation using the MQ4CS framework, which we further enhance with Learned Sparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder models. We also propose an alternative to the previous interleaving strategy, aggregating multiple aspects during the reranking phase. Our findings indicate that multi-aspect query generation is effective in enhancing performance when integrated with advanced retrieval and reranking models. Our results also lead the way for better personalization in Conversational Search, relying on LLMs to integrate personalization within query rewrite, and outperforming human rewrite performance.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/uva.ikat.pdf",
        "key": "uva-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{uva-trec2024-papers-proc-1,\n   author = {Simon Lupart (University of Amsterdam), Zahra Abbasiantaeb (University of Amsterdam), Mohammad Aliannejadi (University of Amsterdam)},\n   title = {IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {uva},\n   trec_runs = {gpt4-MQ-debertav3, gpt4-mq-rr-fusion, gpt-single-QR-rr-debertav3, qd1, manual-splade-fusion, manual-splade-debertav3},\n   trec_tracks = {ikat}\n   url = {https://trec.nist.gov/pubs/trec33/papers/uva.ikat.pdf}\n}"
      }
    },
    "medvidqa": {
      "DoshishaUzlDfki-trec2024-papers-proc-1": {
        "file": "DoshishaUzlDfki.medvidqa.pdf",
        "pid": "DoshishaUzlDfki",
        "title": "Doshisha University, Universit\u00e4t zu L\u00fcbeck and German Research Center for Artificial Intelligence at TRECVID 2024: QFISC Task",
        "author": "Zihao Chen (Doshisha University), Falco Lentzsch (German Research Center for Artificial Intelligence), Nele S. Br\u00fcgge (German Research Center for Artificial Intelligence), Fr\u00e9d\u00e9ric Li (German Research Center for Artificial Intelligence), Miho Ohsaki (Doshisha University), Heinz Handels (German Research Center for Artificial Intelligence, University of Luebeck), Marcin Grzegorzek (German Research Center for Artificial Intelligence, University of Luebeck), Kimiaki Shirahama (Doshisha University)",
        "abstract": "This paper presents the approaches proposed by the DoshishaUzlDfki team to address the Query-Focused Instructional Step Captioning (QFISC) task of TRECVID 2024. Given some RGB videos containing stepwise instructions, we explored several techniques to automatically identify the boundaries of each step, and provide a caption to it. More specifically, two different types of methods were investigated for temporal video segmentation. The first uses the CoSeg approach proposed by Wang et al. [9] based on Event Segmentation Theory, which hypothesises that video frames at the boundaries of steps are harder to predict since they tend to contain more significant visual changes. In detail, CoSeg detects event boundaries in the RGB video stream by finding the local maxima in the reconstruction error of a model trained to reconstruct the temporal contrastive embeddings of video snippets. The second type of approaches we tested exclusively relies on the audio modality, and is based on the hypothesis that information about step transitions is often semantically contained in the verbal transcripts of the videos. In detail, we used the WhisperX model [3] that isolates speech parts in the audio tracks of the videos, and converts them into timestamped text transcripts. The latter were then sent as input of a Large Language Model (LLM) with a carefully designed prompt requesting the LLM to identify step boundaries. Once the temporal video segmentation performed, we\r\nsent the WhisperX transcripts corresponding to the video segments determined by both methods to a LLM instructed to caption them. The GPT4o and Mistral Large 2 LLMs were employed in our experiments for both segmentation and captioning. Our results show that the temporal segmentation methods based on audioprocessing significantly outperform the video-based one. More specifically, the best performances we obtained are yielded by our approach using GPT4o with zero-shot prompting for temporal segmentation. It achieves the top global performances of all runs submitted to the QFISC task in all evaluation metrics, except for precision whose best performance is obtained by our run using Mistral Large 2 with chain-of-thoughts prompting.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/DoshishaUzlDfki.medvidqa.pdf",
        "key": "DoshishaUzlDfki-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{DoshishaUzlDfki-trec2024-papers-proc-1,\n   author = {Zihao Chen (Doshisha University), Falco Lentzsch (German Research Center for Artificial Intelligence), Nele S. Br\u00fcgge (German Research Center for Artificial Intelligence), Fr\u00e9d\u00e9ric Li (German Research Center for Artificial Intelligence), Miho Ohsaki (Doshisha University), Heinz Handels (German Research Center for Artificial Intelligence, University of Luebeck), Marcin Grzegorzek (German Research Center for Artificial Intelligence, University of Luebeck), Kimiaki Shirahama (Doshisha University)},\n   title = {Doshisha University, Universit\u00e4t zu L\u00fcbeck and German Research Center for Artificial Intelligence at TRECVID 2024: QFISC Task},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {DoshishaUzlDfki},\n   trec_runs = {chatGPT_zeroshot_prompt, mistral_meta_prompt, mistral_fewshot_prompt, GPT_meta_prompt, CoSeg_meta_prompt},\n   trec_tracks = {medvidqa}\n   url = {https://trec.nist.gov/pubs/trec33/papers/DoshishaUzlDfki.medvidqa.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-2": {
        "file": "Overview_medvidqa.pdf",
        "pid": "coordinators",
        "title": "Overview of TREC 2024 Medical video Question Answering (MedVidQA) Track",
        "author": "Deepak Gupta and Dina Demner-Fushman",
        "abstract": "One of the key goals of artificial intelligence (AI) is the development of a multimodal system that facilitates communication with the visual world (image and video) using a natural language query. Earlier works on medical question answering primarily focused on textual and visual (image) modalities, which may be inefficient in answering questions requiring demonstration. In recent years, significant progress has been achieved due to the introduction of large-scale language-vision datasets and the development of efficient deep neural techniques that bridge the gap between language and visual understanding. Improvements have been made in numerous vision-and-language tasks, such as visual captioning visual question answering, and natural language video localization. Most of the existing work on language vision focused on creating datasets and developing solutions for open-domain applications. We believe medical videos may provide the best possible answers to many first aid, medical emergency, and medical education questions. With increasing interest in AI to support clinical decision-making and improve patient engagement, there is a need to explore such challenges and develop efficient algorithms for medical language-video understanding and generation. Toward this, we introduced new tasks to foster research toward designing systems that can understand medical videos to provide visual answers to natural language questions, and are equipped with multimodal capability to generate instruction steps from the medical video. These tasks have the potential to support the development of sophisticated downstream applications that can benefit the public and medical professionals.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_medvidqa.pdf",
        "key": "coordinators-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-2,\n   author = {Deepak Gupta and Dina Demner-Fushman},\n   title = {Overview of TREC 2024 Medical video Question Answering (MedVidQA) Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {medvidqa}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_medvidqa.pdf}\n}"
      }
    },
    "rag": {
      "TREMA-UNH-trec2024-papers-proc-1": {
        "file": "TREMA-UNH.rag.pdf",
        "pid": "TREMA-UNH",
        "title": "TREMA-UNH at TREC: RAG Systems and RUBRIC-style Evaluation",
        "author": "Naghmeh Farzi, Laura Dietz",
        "abstract": "The TREMA-UNH team participated in the TREC Retrieval-Augmented Genera-\r\ntion track (RAG). In Part 1 we describe the RAG systems submitted to the Augmented\r\nGeneration Task (AG) and the Retrieval-Augmented Generation Task (RAG), the lat-\r\nter using a BM25 retrieval model. In Part 2 we describe an alternative LLM-based\r\nevaluation method for this track using the RUBRIC Autograder Workbench approach,\r\nwhich won the SIGIR\u201924 best paper award.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/TREMA-UNH.rag.pdf",
        "key": "TREMA-UNH-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{TREMA-UNH-trec2024-papers-proc-1,\n   author = {Naghmeh Farzi, Laura Dietz},\n   title = {TREMA-UNH at TREC: RAG Systems and RUBRIC-style Evaluation},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {TREMA-UNH},\n   trec_runs = {Ranked_Iterative_Fact_Extraction_and_Refinement, Enhanced_Iterative_Fact_Refinement_and_Prioritization, Ranked_Iterative_Fact_Extraction_and_Refinement_RIFER_-_bm25},\n   trec_tracks = {rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/TREMA-UNH.rag.pdf}\n}"
      },
      "CIR-trec2024-papers-proc-1": {
        "file": "CIR.rag.pdf",
        "pid": "CIR",
        "title": "CIR at TREC 2024 RAG: Task 2 - Augmented Generation with Diversified Segments and Knowledge Adaption",
        "author": "J\u00fcri Keller (TH K\u00f6ln - University of Applied) , Bj\u00f6rn Engelmann (TH K\u00f6ln - University of Applied) , Fabian Haak (TH K\u00f6ln - University of Applied) , Philipp Schaer (TH K\u00f6ln - University of Applied) , Hermann Kroll (TU Braunschweig) , Christin Katharina Kreutz (TH Mittelhessen - University of Applied Sciences, Herder Institute)",
        "abstract": "This paper describes the CIR team\u2019s participation in the TREC 2024 RAG track for task 2, augmented generation. With our approach, we intended to explore the effects of diversification of the segments that are considered in the generation as well as variations in the depths of users\u2019 knowledge on a query topic. We describe a two-step approach that first reranks input segments such that they are as similar as possible to a query while also being as dissimilar as possible from higher ranked relevant segments. In the second step, these reranked segments are relayed to an LLM, which uses them to generate an answer to the query while referencing the segments that have contributed to specific parts of the answer. The LLM considers the varying background knowledge of potential users through our prompts.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/CIR.rag.pdf",
        "key": "CIR-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{CIR-trec2024-papers-proc-1,\n   author = {J\u00fcri Keller (TH K\u00f6ln - University of Applied) , Bj\u00f6rn Engelmann (TH K\u00f6ln - University of Applied) , Fabian Haak (TH K\u00f6ln - University of Applied) , Philipp Schaer (TH K\u00f6ln - University of Applied) , Hermann Kroll (TU Braunschweig) , Christin Katharina Kreutz (TH Mittelhessen - University of Applied Sciences, Herder Institute)},\n   title = {CIR at TREC 2024 RAG: Task 2 - Augmented Generation with Diversified Segments and Knowledge Adaption},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {CIR},\n   trec_runs = {cir_gpt-4o-mini_Jaccard_50_0.5_100_301_p0, cir_gpt-4o-mini_Jaccard_50_1.0_100_301_p0, cir_gpt-4o-mini_Cosine_50_0.5_100_301_p1, cir_gpt-4o-mini_Cosine_50_0.25_100_301_p1, cir_gpt-4o-mini_Cosine_50_0.75_100_301_p1, cir_gpt-4o-mini_Cosine_50_1.0_100_301_p1, cir_gpt-4o-mini_Cosine_20_0.5_100_301_p1, cir_gpt-4o-mini_Cosine_50_0.5_100_301_p2, cir_gpt-4o-mini_Cosine_50_0.5_100_301_p3, cir_gpt-4o-mini_no_reranking_50_0.5_100_301_p1},\n   trec_tracks = {rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/CIR.rag.pdf}\n}"
      },
      "WaterlooClarke-trec2024-papers-proc-1": {
        "file": "WaterlooClarke.lateral.rag.pdf",
        "pid": "WaterlooClarke",
        "title": "Monster Ranking",
        "author": "Charles L. A. Clarke (University of Waterloo), Siqing Huo (University of Waterloo), Negar Arabzadeh (University of Waterloo)",
        "abstract": "Participating as the UWClarke group, we focused on the RAG track; we also submitted runs for the Lateral Reading Track. For the retrieval task (R) of the RAG Track, we attempted what we have come to call \u201cmonster ranking\u201d. Largely ignoring cost and computational resources, monster ranking attempts to determine the best possible ranked list for a query by whatever means possible, including explicit LLM-based relevance judgments, both pointwise and pairwise. While a monster ranker could never be deployed in a production environment, its output may be valuable for evaluating cheaper and faster rankers. For the full retrieval augmented generation (RAG) task we explored two general approaches, depending on if generation happens first or second: 1) Generate an Answer and support with Retrieved Evidence (GARE). 2) Retrieve And Generate with Evidence (RAGE).",
        "url": "https://trec.nist.gov/pubs/trec33/papers/WaterlooClarke.lateral.rag.pdf",
        "key": "WaterlooClarke-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{WaterlooClarke-trec2024-papers-proc-1,\n   author = {Charles L. A. Clarke (University of Waterloo), Siqing Huo (University of Waterloo), Negar Arabzadeh (University of Waterloo)},\n   title = {Monster Ranking},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {WaterlooClarke},\n   trec_runs = {uwclarke_auto, uwclarke_auto_summarized, UWCrag, UWCrag_stepbystep, UWCgarag, monster, uwc1, uwc2, uwc0, uwcCQAR, uwcCQA, uwcCQR, uwcCQ, uwcBA, uwcBQ, UWClarke_rerank},\n   trec_tracks = {lateral.rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/WaterlooClarke.lateral.rag.pdf}\n}"
      },
      "softbank-meisei-trec2024-papers-proc-2": {
        "file": "softbank-meisei.rag.pdf",
        "pid": "softbank-meisei",
        "title": "softbank-meisei-trec2024-papers-proc-2",
        "author": "Aiswariya Manoj Kumar\uff08Softbank Corp.\uff09, Hiroki Takushima\uff08Softbank Corp.\uff09, Yuma Suzuki\uff08Softbank Corp.\uff09, Hayato Tanoue\uff08Softbank Corp.\uff09, Hiroki Nishihara\uff08Softbank Corp.\uff09, Yuki Shibata\uff08Softbank Corp.\uff09, Haruki Sato\uff08Agoop Corp.\uff09, Takumi Takada\uff08SB Intuitions Corp.\uff09, Takayuki Hori\uff08Softbank Corp.\uff09, Kazuya Ueki\uff08Meisei Univ.\uff09",
        "abstract": "The SoftBank-Meisei team participated in the Retrieval (R), Augmented Generation (AG), and Retrieval Augmented Generation (RAG) tasks at TREC RAG 2024. In the retrieval task, we employed the hierarchical retrieval process of combining the sparse and dense retrieval methods. We submitted two runs for the task; one with the baseline implementation with additional preprocessing on the topic list and the other with the hierarchical retrieval results.\r\nIn the Augmented Generation task, we used the GPT-4o API, as well as the LLama3-70b model along with our custom prompt for the generation. As for the Retrieval Augmented Generation task, we submitted two runs same as the R-task. The prompt used for the AG-task was used for the generation stage of the RAG-task too.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.rag.pdf",
        "key": "softbank-meisei-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{softbank-meisei-trec2024-papers-proc-2,\n   author = {Aiswariya Manoj Kumar\uff08Softbank Corp.\uff09, Hiroki Takushima\uff08Softbank Corp.\uff09, Yuma Suzuki\uff08Softbank Corp.\uff09, Hayato Tanoue\uff08Softbank Corp.\uff09, Hiroki Nishihara\uff08Softbank Corp.\uff09, Yuki Shibata\uff08Softbank Corp.\uff09, Haruki Sato\uff08Agoop Corp.\uff09, Takumi Takada\uff08SB Intuitions Corp.\uff09, Takayuki Hori\uff08Softbank Corp.\uff09, Kazuya Ueki\uff08Meisei Univ.\uff09},\n   title = {softbank-meisei-trec2024-papers-proc-2},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {softbank-meisei},\n   trec_runs = {rtask-bm25-colbert_faiss, rtask-bm25-rank_zephyr, rag_bm25-colbert_faiss-gpt4o-llama70b, ragtask-bm25-rank_zephyr-gpt4o-llama70b, agtask-bm25-colbert_faiss-gpt4o-llama70b},\n   trec_tracks = {rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.rag.pdf}\n}"
      },
      "ncsu-las-trec2024-papers-proc-1": {
        "file": "ncsu-las.rag.pdf",
        "pid": "ncsu-las",
        "title": "Laboratory for Analytic Sciences in TREC 2024 Retrieval Augmented Generation Track",
        "author": "Yue Wang (UNC at Chapel Hill), John M. Conroy (IDA Center for Computing Sciences), Neil Molino (IDA Center for Computing Sciences), Julia Yang (U.S. Department of Defense), Mike Green (U.S. Department of Defense)",
        "abstract": "We report on our approach to the NIST TREC 2024 retrieval-augmented generation (RAG) track. The goal of this track was to build and evaluate systems that can answer complex questions by 1) retrieving excerpts of webpages from a large text collection (hundreds of millions of excerpts taken from tens of millions of webpages); 2) summarizing relevant information within retrieved excerpts into an answer containing up to 400 words; 3) attributing each sentence in the generated summary to one or more retrieved excerpts. We participated in the retrieval (R) task and retrieval augmented generation (RAG) task.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/ncsu-las.rag.pdf",
        "key": "ncsu-las-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{ncsu-las-trec2024-papers-proc-1,\n   author = {Yue Wang (UNC at Chapel Hill), John M. Conroy (IDA Center for Computing Sciences), Neil Molino (IDA Center for Computing Sciences), Julia Yang (U.S. Department of Defense), Mike Green (U.S. Department of Defense)},\n   title = {Laboratory for Analytic Sciences in TREC 2024 Retrieval Augmented Generation Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {ncsu-las},\n   trec_runs = {LAS_ENN_T5_RERANKED_MXBAI, LAS-splade-mxbai-rrf, LAS-splade-mxbai, LAS-splade-mxbai-rrf-mmr8, LAS-splade-mxbai-mmr8-RAG, LAS-T5-mxbai-mmr8-RAG, LAS_enn_t5, LAS_ann_t5_qdrant, LAS-splade-mxbai-rrf-mmr8-doc, LAS_splad_mxbai-rrf-occams_50_RAG},\n   trec_tracks = {rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/ncsu-las.rag.pdf}\n}"
      },
      "uis-iai-trec2024-papers-proc-1": {
        "file": "uis-iai.rag.pdf",
        "pid": "uis-iai",
        "title": "The University of Stavanger (IAI) at the TREC 2024 Retrieval-Augmented Generation Track",
        "author": "Weronika Lajewska (University of Stavanger), Krisztian Balog (University of Stavanger)",
        "abstract": "This paper describes the participation of the IAI group at the University of Stavanger in the TREC 2024 Retrieval-Augmented Generation track. We employ a modular pipeline for Grounded Information Nugget-based GEneration of Conversational Information-Seeking Responses (GINGER) to ensure factual correctness and source attribution. The multistage process includes detecting, clustering, and ranking information nuggets, summarizing top clusters, and generating follow-up questions based on uncovered subspaces of relevant information. In our runs, we experiment with different length of the responses and different number of input passages. Preliminary results indicate that ours was one of the top performing systems in the augmented generation task.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/uis-iai.rag.pdf",
        "key": "uis-iai-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{uis-iai-trec2024-papers-proc-1,\n   author = {Weronika Lajewska (University of Stavanger), Krisztian Balog (University of Stavanger)},\n   title = {The University of Stavanger (IAI) at the TREC 2024 Retrieval-Augmented Generation Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {uis-iai},\n   trec_runs = {ginger_top_5, baseline_top_5, ginger-fluency_top_5, ginger-fluency_top_10, ginger-fluency_top_20},\n   trec_tracks = {rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/uis-iai.rag.pdf}\n}"
      },
      "webis-trec2024-papers-proc-1": {
        "file": "webis.biogen.rag.tot.pdf",
        "pid": "webis",
        "title": "Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks",
        "author": "Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)",
        "abstract": "In this paper, we describe the Webis Group's participation in the 2024~edition of TREC. We participated in the Biomedical Generative Retrieval track, the Retrieval-Augmented Generation track, and the Tip-of-the-Tongue track. For the biomedical track, we applied different paradigms of retrieval-augmented generation with open- and closed-source LLMs. For the Retrieval-Augmented Generation track, we aimed to contrast manual response submissions with fully-automated responses. For the Tip-of-the-Tongue track, we employed query relaxation as in our last year's submission (i.e., leaving out terms that likely reduce the retrieval effectiveness) that we combine with a new cross-encoder that we trained on an enriched version of the TOMT-KIS dataset.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf",
        "key": "webis-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{webis-trec2024-papers-proc-1,\n   author = {Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)},\n   title = {Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {webis},\n   trec_runs = {webis-01, webis-02, webis-03, webis-04, webis-05, webis-ag-run0-taskrag, webis-ag-run1-taskrag, webis-manual, webis-rag-run0-taskrag, webis-rag-run1-taskrag, webis-rag-run3-taskrag, webis-ag-run3-reuserag, webis-rag-run4-reuserag, webis-rag-run5-reuserag, webis-ag-run2-reuserag, webis-1, webis-2, webis-3, webis-gpt-1, webis-gpt-4, webis-gpt-6, webis-5, webis-base, webis-tot-01, webis-tot-02, webis-tot-04, webis-tot-03},\n   trec_tracks = {biogen.rag.tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf}\n}"
      }
    },
    "biogen": {
      "coordinators-trec2024-papers-proc-1": {
        "file": "Overview_biogen.pdf",
        "pid": "coordinators",
        "title": "Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track",
        "author": "Deepak Gupta, Dina Demner-Fushman, William Hersh, Steven Bedrick, Kirk Roberts",
        "abstract": "With the advancement of large language models (LLMs), the biomedical domain has seen significant progress and improvement in multiple tasks such as biomedical question answering, lay language summarization of the biomedical literature, clinical note summarization, etc. However, hallucinations or confabulations remain one of the key challenges when using LLMs in the biomedical and other domains. Inaccuracies may be particularly harmful in high-risk situations, such as making clinical decisions or appraising biomedical research. Studies on the evaluation of the LLMs' abilities to ground generated statements in verifiable sources have shown that models perform significantly worse on lay-user generated questions, and often fail to reference relevant sources. This can be problematic when those seeking information want evidence from studies to back up the claims from LLMs[3]. Unsupported statements are a major barrier to using LLMs in any applications that may affect health. Methods for grounding generated statements in reliable sources along with practical evaluation approaches are needed to overcome this barrier. Towards this, in our pilot task organized at TREC 2024, we introduced the task of reference attribution as a means to mitigate the generation of false statements by LLMs answering biomedical questions.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_biogen.pdf",
        "key": "coordinators-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-1,\n   author = {Deepak Gupta, Dina Demner-Fushman, William Hersh, Steven Bedrick, Kirk Roberts},\n   title = {Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {biogen}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_biogen.pdf}\n}"
      },
      "ur-iw-trec2024-papers-proc-1": {
        "file": "ur-iw.biogen.pdf",
        "pid": "ur-iw",
        "title": "Exploring the Few-Shot Performance of Low-Cost Proprietary Models in the 2024 TREC BioGen Track",
        "author": "Samy Ateia (University of Regensburg), Udo Kruschwitz (University of Regensburg)",
        "abstract": "For the 2024 TREC Biomedical Generative Retrieval (BioGen) Track, we evaluated proprietary low-cost large language models (LLMs) in few-shot and zero-shot settings for biomedical question answering. Building upon our prior competitive approach from the CLEF 2024 BioASQ challenge, we adapted our methods to the BioGen task. We reused few-shot examples from BioASQ and generated additional ones from the test set for the BioGen specific answer format, by using an LLM judge to select examples. Our approach involved query expansion, BM25-based retrieval using Elasticsearch, snippet extraction, reranking, and answer generation both with and without 10-shot learning and additional relevant context from Wikipedia. The results are in line with our findings at BioASQ, indicating that additional Wikipedia context did not improve the results, while 10-shot learning did. An interactive reference implementation that showcases Google's Gemini-1.5-flash performance with 3-shot learning is available online and the source code of this demo is available on GitHub.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/ur-iw.biogen.pdf",
        "key": "ur-iw-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{ur-iw-trec2024-papers-proc-1,\n   author = {Samy Ateia (University of Regensburg), Udo Kruschwitz (University of Regensburg)},\n   title = {Exploring the Few-Shot Performance of Low-Cost Proprietary Models in the 2024 TREC BioGen Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {ur-iw},\n   trec_runs = {zero-shot-gpt4o-mini, zero-shot-gemini-flash, ten-shot-gpt4o-mini, ten-shot-gemini-flash, ten-shot-gpt4o-mini-wiki, ten-shot-gemini-flash-wiki},\n   trec_tracks = {biogen}\n   url = {https://trec.nist.gov/pubs/trec33/papers/ur-iw.biogen.pdf}\n}"
      },
      "webis-trec2024-papers-proc-1": {
        "file": "webis.biogen.rag.tot.pdf",
        "pid": "webis",
        "title": "Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks",
        "author": "Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)",
        "abstract": "In this paper, we describe the Webis Group's participation in the 2024~edition of TREC. We participated in the Biomedical Generative Retrieval track, the Retrieval-Augmented Generation track, and the Tip-of-the-Tongue track. For the biomedical track, we applied different paradigms of retrieval-augmented generation with open- and closed-source LLMs. For the Retrieval-Augmented Generation track, we aimed to contrast manual response submissions with fully-automated responses. For the Tip-of-the-Tongue track, we employed query relaxation as in our last year's submission (i.e., leaving out terms that likely reduce the retrieval effectiveness) that we combine with a new cross-encoder that we trained on an enriched version of the TOMT-KIS dataset.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf",
        "key": "webis-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{webis-trec2024-papers-proc-1,\n   author = {Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)},\n   title = {Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {webis},\n   trec_runs = {webis-01, webis-02, webis-03, webis-04, webis-05, webis-ag-run0-taskrag, webis-ag-run1-taskrag, webis-manual, webis-rag-run0-taskrag, webis-rag-run1-taskrag, webis-rag-run3-taskrag, webis-ag-run3-reuserag, webis-rag-run4-reuserag, webis-rag-run5-reuserag, webis-ag-run2-reuserag, webis-1, webis-2, webis-3, webis-gpt-1, webis-gpt-4, webis-gpt-6, webis-5, webis-base, webis-tot-01, webis-tot-02, webis-tot-04, webis-tot-03},\n   trec_tracks = {biogen.rag.tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf}\n}"
      }
    },
    "avs": {
      "WHU-NERCMS-trec2024-papers-proc-1": {
        "file": "WHU-NERCMS.avs.pdf",
        "pid": "WHU-NERCMS",
        "title": "WHU-NERCMS AT TRECVID2024: AD-HOC VIDEO SEARCH TASK",
        "author": "Heng Liu (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU), Jiangshan He (NERCMS), Zeyuan Zhang (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU), Yuanyuan Xu (NERCMS), Chao Liang (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU)",
        "abstract": "The WHU-NERCMS team participated in the ad-hoc video search (AVS) task of TRECVID 2024. In this year\u2019s AVS task, we continued to use multiple visual semantic embedding methods, combined with interactive feedback-guided ranking aggregation techniques to integrate different models and their outputs to generate the final ranked video shot list. We submitted 4 runs each for automatic and interactive tasks, along with one attempt for a manual assistance task. Table 1 shows our results forthis year.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/WHU-NERCMS.avs.pdf",
        "key": "WHU-NERCMS-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{WHU-NERCMS-trec2024-papers-proc-1,\n   author = {Heng Liu (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU), Jiangshan He (NERCMS), Zeyuan Zhang (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU), Yuanyuan Xu (NERCMS), Chao Liang (Hubei Key Laboratory of Multimedia and Network Communication Engineering, NERCMS, WHU)},\n   title = {WHU-NERCMS AT TRECVID2024: AD-HOC VIDEO SEARCH TASK},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {WHU-NERCMS},\n   trec_runs = {run4, run3, run2, Manual_run1, relevance_feedback_run4, relevance_feedback_run1, auto_run1, rf_run2, RF_run3},\n   trec_tracks = {avs}\n   url = {https://trec.nist.gov/pubs/trec33/papers/WHU-NERCMS.avs.pdf}\n}"
      },
      "CERTH-ITI-trec2024-papers-proc-1": {
        "file": "CERTH-ITI.avs.actev.pdf",
        "pid": "CERTH-ITI",
        "title": "ITI-CERTH participation in ActEV and AVS Tracks of TRECVID 2024",
        "author": "Konstantinos Gkountakos, Damianos Galanopoulos, Antonios Leventakis, Georgios Tsionkis, Klearchos Stavrothanasopoulos, Konstantinos Ioannidis, Stefanos Vrochidis, Vasileios Mezaris, Ioannis Kompatsiaris",
        "abstract": "This report presents the overview of the runs related to Ad-hoc Video Search (AVS) and Activities in Extended Video (ActEV) tasks on behalf of the ITI-CERTH team. Our participation in the AVS task involves a collection of five cross-modal deep network architectures and numerous pre-trained models, which are used to calculate the similarities between video shots and queries. These calculated similarities serve as input to a trainable neural network that effectively combines them. During the retrieval stage, we also introduce a normalization step that utilizes both the current and previous AVS queries for revising the combined video shot-query similarities. For the ActEV task, we adapt our framework to support a rule-based classification to overcome the challenges of detecting and recognizing activities in a multi-label manner while experimenting with two separate activity classifiers.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/CERTH-ITI.avs.actev.pdf",
        "key": "CERTH-ITI-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{CERTH-ITI-trec2024-papers-proc-1,\n   author = {Konstantinos Gkountakos, Damianos Galanopoulos, Antonios Leventakis, Georgios Tsionkis, Klearchos Stavrothanasopoulos, Konstantinos Ioannidis, Stefanos Vrochidis, Vasileios Mezaris, Ioannis Kompatsiaris},\n   title = {ITI-CERTH participation in ActEV and AVS Tracks of TRECVID 2024},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {CERTH-ITI},\n   trec_runs = {certh.iti.avs.24.main.run.1, certh.iti.avs.24.main.run.2, certh.iti.avs.24.main.run.3, certh.iti.avs.24.progress.run.1, certh.iti.avs.24.progress.run.2, certh.iti.avs.24.progress.run.3},\n   trec_tracks = {avs.actev}\n   url = {https://trec.nist.gov/pubs/trec33/papers/CERTH-ITI.avs.actev.pdf}\n}"
      },
      "ruc_aim3-trec2024-papers-proc-1": {
        "file": "ruc_aim3.avs.pdf",
        "pid": "ruc_aim3",
        "title": "RUC_AIM3 at TRECVID 2024: Ad-hoc Video Search",
        "author": "Xueyan Wang (Renmin University of China), Yang Du (Renmin University of China), Yuqi Liu (Renmin University of China), Qin Jin (Renmin University of China)",
        "abstract": "This report presents our solution for the Ad-hoc Video Search (AVS) task of TRECVID 2024. Based on our baseline AVS model in TRECVID 2023, we further improve the searching performance by integrating multiple visual-embedding models, performing video captioning to be used for topic-to-caption searches, and applying a re-ranking strategy for top candidate search selection. Our submissions from our improved AVS model rank the 3rd in TRECVID AVS 2024 on mean average precision (mAP) in the main task, achieving the best run of 36.8.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/ruc_aim3.avs.pdf",
        "key": "ruc_aim3-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{ruc_aim3-trec2024-papers-proc-1,\n   author = {Xueyan Wang (Renmin University of China), Yang Du (Renmin University of China), Yuqi Liu (Renmin University of China), Qin Jin (Renmin University of China)},\n   title = {RUC_AIM3 at TRECVID 2024: Ad-hoc Video Search},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {ruc_aim3},\n   trec_runs = {add_captioning, baseline, add_QArerank, add_captioning_QArerank},\n   trec_tracks = {avs}\n   url = {https://trec.nist.gov/pubs/trec33/papers/ruc_aim3.avs.pdf}\n}"
      },
      "softbank-meisei-trec2024-papers-proc-2": {
        "file": "softbank-meisei.avs.vtt.pdf",
        "pid": "softbank-meisei",
        "title": "Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks",
        "author": "Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)",
        "abstract": "The Softbank-Meisei team participated in the ad-hoc video search (AVS) and video-to-text (VTT) tasks at TREC 2024. In this year's AVS task, we submitted four fully automatic systems for both the main and progress tasks. Our systems utilized pre-trained vision and language models, including CLIP, BLIP, and BLIP-2, along with several other advanced models. We also expanded the original query texts using text generation and image generation techniques to enhance data diversity. The integration ratios of these models were optimized based on results from previous benchmark test datasets. In this year's VTT, as last year, we submitted four main task methods using multiple model captioning, reranking, and generative AI for summarization. For the subtasks, we submitted three methods using the output of each model. Last year's test data for the main task showed improvements of about 0.04 points in CIDEr-D and about 0.03 points in SPICE, based on the indices we had on hand.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf",
        "key": "softbank-meisei-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{softbank-meisei-trec2024-papers-proc-2,\n   author = {Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)},\n   title = {Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {softbank-meisei},\n   trec_runs = {SoftbankMeisei - Progress Run 1, SoftbankMeisei - Progress Run 2, SoftbankMeisei - Progress Run 3, SoftbankMeisei - Progress Run 4, SoftbankMeisei - Main Run 1, SoftbankMeisei - Main Run 2, SoftbankMeisei - Main Run 3, SoftbankMeisei - Main Run 4, SoftbankMeisei_vtt_main_run1, SoftbankMeisei_vtt_main_run2, SoftbankMeisei_vtt_main_run3, SoftbankMeisei_vtt_main_run4, SoftbankMeisei_vtt_sub_run2, SoftbankMeisei_vtt_sub_run3, SoftbankMeisei_vtt_sub_run1},\n   trec_tracks = {avs.vtt}\n   url = {https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf}\n}"
      },
      "softbank-meisei-trec2024-papers-proc-3": {
        "file": "softbank-meisei.avs.vtt.pdf",
        "pid": "softbank-meisei",
        "title": "Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks",
        "author": "Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)",
        "abstract": "The Softbank-Meisei team participated in the ad-hoc video search (AVS) and video-to-text (VTT) tasks at TREC 2024. In this year's AVS task, we submitted four fully automatic systems for both the main and progress tasks. Our systems utilized pre-trained vision and language models, including CLIP, BLIP, and BLIP-2, along with several other advanced models. We also expanded the original query texts using text generation and image generation techniques to enhance data diversity. The integration ratios of these models were optimized based on results from previous benchmark test datasets. In this year's VTT, as last year, we submitted four main task methods using multiple model captioning, reranking, and generative AI for summarization. For the subtasks, we submitted three methods using the output of each model. Last year's test data for the main task showed improvements of about 0.04 points in CIDEr-D and about 0.03 points in SPICE, based on the indices we had on hand.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf",
        "key": "softbank-meisei-trec2024-papers-proc-3",
        "bibtex": "\n@inproceedings{softbank-meisei-trec2024-papers-proc-3,\n   author = {Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)},\n   title = {Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {softbank-meisei},\n   trec_runs = {SoftbankMeisei - Progress Run 1, SoftbankMeisei - Progress Run 2, SoftbankMeisei - Progress Run 3, SoftbankMeisei - Progress Run 4, SoftbankMeisei - Main Run 1, SoftbankMeisei - Main Run 2, SoftbankMeisei - Main Run 3, SoftbankMeisei - Main Run 4, SoftbankMeisei_vtt_main_run1, SoftbankMeisei_vtt_main_run2, SoftbankMeisei_vtt_main_run3, SoftbankMeisei_vtt_main_run4, SoftbankMeisei_vtt_sub_run2, SoftbankMeisei_vtt_sub_run3, SoftbankMeisei_vtt_sub_run1},\n   trec_tracks = {avs.vtt}\n   url = {https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-6": {
        "file": "Overview_avs.vtt.actev.pdf",
        "pid": "coordinators",
        "title": "TRECVID 2024 - Evaluating video search, captioning, and activity recognition",
        "author": "George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)",
        "abstract": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology.\r\nOver the last two decades, this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals world-wide contribute significant time and effort. This year TRECVID has been merged back to TREC (Text Retrieval Conference1) and planned the following four tracks:\r\n1. Ad-hoc Video Search (AVS)\r\n2. Video to Text (VTT)\r\n3. Activities in Extended Video (ActEV)\r\n4. Medical Video Question Answering (Med-VidQA)",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf",
        "key": "coordinators-trec2024-papers-proc-6",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-6,\n   author = {George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)},\n   title = {TRECVID 2024 - Evaluating video search, captioning, and activity recognition},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {avs.vtt.actev}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf}\n}"
      }
    },
    "lateral": {
      "WaterlooClarke-trec2024-papers-proc-1": {
        "file": "WaterlooClarke.lateral.rag.pdf",
        "pid": "WaterlooClarke",
        "title": "Monster Ranking",
        "author": "Charles L. A. Clarke (University of Waterloo), Siqing Huo (University of Waterloo), Negar Arabzadeh (University of Waterloo)",
        "abstract": "Participating as the UWClarke group, we focused on the RAG track; we also submitted runs for the Lateral Reading Track. For the retrieval task (R) of the RAG Track, we attempted what we have come to call \u201cmonster ranking\u201d. Largely ignoring cost and computational resources, monster ranking attempts to determine the best possible ranked list for a query by whatever means possible, including explicit LLM-based relevance judgments, both pointwise and pairwise. While a monster ranker could never be deployed in a production environment, its output may be valuable for evaluating cheaper and faster rankers. For the full retrieval augmented generation (RAG) task we explored two general approaches, depending on if generation happens first or second: 1) Generate an Answer and support with Retrieved Evidence (GARE). 2) Retrieve And Generate with Evidence (RAGE).",
        "url": "https://trec.nist.gov/pubs/trec33/papers/WaterlooClarke.lateral.rag.pdf",
        "key": "WaterlooClarke-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{WaterlooClarke-trec2024-papers-proc-1,\n   author = {Charles L. A. Clarke (University of Waterloo), Siqing Huo (University of Waterloo), Negar Arabzadeh (University of Waterloo)},\n   title = {Monster Ranking},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {WaterlooClarke},\n   trec_runs = {uwclarke_auto, uwclarke_auto_summarized, UWCrag, UWCrag_stepbystep, UWCgarag, monster, uwc1, uwc2, uwc0, uwcCQAR, uwcCQA, uwcCQR, uwcCQ, uwcBA, uwcBQ, UWClarke_rerank},\n   trec_tracks = {lateral.rag}\n   url = {https://trec.nist.gov/pubs/trec33/papers/WaterlooClarke.lateral.rag.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-5": {
        "file": "Overview_lateral.pdf",
        "pid": "coordinators",
        "title": "Overview of the TREC 2024 Lateral Reading Track",
        "author": "Dake Zhang (University of Waterloo), Mark D. Smucker (University of Waterloo), Charles L. A. Clarke (University of Waterloo)",
        "abstract": "The current web landscape, characterized by abundant information and widespread misinformation, highlights the pressing need for people to evaluate the trustworthiness of online content effectively. However, this remains a daunting challenge for many internet users. The TREC 2024 Lateral Reading Track seeks to address this issue by supporting the use of lateral reading, a proven strategy used by professional fact-checkers, to help users evaluate news articles more effectively and efficiently. In its first year, the track had two tasks: (1) generating questions that readers should consider when assessing the trustworthiness of the given news articles, and (2) retrieving documents to help answer these questions. This paper presents an overview of the track, including its objectives, methodologies, resources, and evaluation results. Our evaluation of the submitted runs shows the significant challenges these tasks pose to existing approaches, including state-of-the-art large language models. Further details on this track can be found on its website: https://trec-dragun.github.io/.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_lateral.pdf",
        "key": "coordinators-trec2024-papers-proc-5",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-5,\n   author = {Dake Zhang (University of Waterloo), Mark D. Smucker (University of Waterloo), Charles L. A. Clarke (University of Waterloo)},\n   title = {Overview of the TREC 2024 Lateral Reading Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {lateral}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_lateral.pdf}\n}"
      }
    },
    "actev": {
      "CERTH-ITI-trec2024-papers-proc-1": {
        "file": "CERTH-ITI.avs.actev.pdf",
        "pid": "CERTH-ITI",
        "title": "ITI-CERTH participation in ActEV and AVS Tracks of TRECVID 2024",
        "author": "Konstantinos Gkountakos, Damianos Galanopoulos, Antonios Leventakis, Georgios Tsionkis, Klearchos Stavrothanasopoulos, Konstantinos Ioannidis, Stefanos Vrochidis, Vasileios Mezaris, Ioannis Kompatsiaris",
        "abstract": "This report presents the overview of the runs related to Ad-hoc Video Search (AVS) and Activities in Extended Video (ActEV) tasks on behalf of the ITI-CERTH team. Our participation in the AVS task involves a collection of five cross-modal deep network architectures and numerous pre-trained models, which are used to calculate the similarities between video shots and queries. These calculated similarities serve as input to a trainable neural network that effectively combines them. During the retrieval stage, we also introduce a normalization step that utilizes both the current and previous AVS queries for revising the combined video shot-query similarities. For the ActEV task, we adapt our framework to support a rule-based classification to overcome the challenges of detecting and recognizing activities in a multi-label manner while experimenting with two separate activity classifiers.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/CERTH-ITI.avs.actev.pdf",
        "key": "CERTH-ITI-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{CERTH-ITI-trec2024-papers-proc-1,\n   author = {Konstantinos Gkountakos, Damianos Galanopoulos, Antonios Leventakis, Georgios Tsionkis, Klearchos Stavrothanasopoulos, Konstantinos Ioannidis, Stefanos Vrochidis, Vasileios Mezaris, Ioannis Kompatsiaris},\n   title = {ITI-CERTH participation in ActEV and AVS Tracks of TRECVID 2024},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {CERTH-ITI},\n   trec_runs = {certh.iti.avs.24.main.run.1, certh.iti.avs.24.main.run.2, certh.iti.avs.24.main.run.3, certh.iti.avs.24.progress.run.1, certh.iti.avs.24.progress.run.2, certh.iti.avs.24.progress.run.3},\n   trec_tracks = {avs.actev}\n   url = {https://trec.nist.gov/pubs/trec33/papers/CERTH-ITI.avs.actev.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-6": {
        "file": "Overview_avs.vtt.actev.pdf",
        "pid": "coordinators",
        "title": "TRECVID 2024 - Evaluating video search, captioning, and activity recognition",
        "author": "George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)",
        "abstract": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology.\r\nOver the last two decades, this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals world-wide contribute significant time and effort. This year TRECVID has been merged back to TREC (Text Retrieval Conference1) and planned the following four tracks:\r\n1. Ad-hoc Video Search (AVS)\r\n2. Video to Text (VTT)\r\n3. Activities in Extended Video (ActEV)\r\n4. Medical Video Question Answering (Med-VidQA)",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf",
        "key": "coordinators-trec2024-papers-proc-6",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-6,\n   author = {George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)},\n   title = {TRECVID 2024 - Evaluating video search, captioning, and activity recognition},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {avs.vtt.actev}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf}\n}"
      }
    },
    "product": {
      "jbnu-trec2024-papers-proc-1": {
        "file": "jbnu.product.pdf",
        "pid": "jbnu",
        "title": "JBNU at TREC 2024 Product Search Track",
        "author": "Gi-taek An (Jeonbuk National University), Seong-Hyuk Yim (Jeonbuk National University), Jun-Yong Park (Jeonbuk National University), Woo-Seok Choi (Jeonbuk National University), Kyung-Soon Lee (Jeonbuk National University)",
        "abstract": "This paper describes the participation of the jbnu team in the TREC 2024 Product Search Track. This study addresses two key challenges in product search related to sparse and dense retrieval models. For sparse retrieval models, we propose modifying the activation function to GELU to filter out products that, despite being retrieved due to token expansion, are irrelevant for recommendation based on the scoring mechanism. For dense retrieval models, product search document indexing data was generated using the generative model T5 to address input token limitations. Experimental results demonstrate that both proposed methods yield performance improvements over baseline models.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/jbnu.product.pdf",
        "key": "jbnu-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{jbnu-trec2024-papers-proc-1,\n   author = {Gi-taek An (Jeonbuk National University), Seong-Hyuk Yim (Jeonbuk National University), Jun-Yong Park (Jeonbuk National University), Woo-Seok Choi (Jeonbuk National University), Kyung-Soon Lee (Jeonbuk National University)},\n   title = {JBNU at TREC 2024 Product Search Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {jbnu},\n   trec_runs = {jbnu08, jbnu04, jbnu09, jbnu01, jbnu07, jbnu10, jbnu03, jbnu02, jbnu11, jbnu12, jbnu05, jbnu06},\n   trec_tracks = {product}\n   url = {https://trec.nist.gov/pubs/trec33/papers/jbnu.product.pdf}\n}"
      }
    },
    "tot": {
      "IISER-K-trec2024-papers-proc-1": {
        "file": "IISER-K.tot.pdf",
        "pid": "IISER-K",
        "title": "IISERK@ToT_2024:  Query Reformulation and Layered Retrieval for  Tip-of-Tongue Items",
        "author": "Subinay Adhikary (IISER-K),, Shuvam Banerji Seal (IISER-K),, Soumyadeep Sar (IISER-K),, Dwaipayan Roy (IISER-K)",
        "abstract": "In this study, we explore various approaches for known-item retrieval, referred to as ``Tip-of-the-Tongue\" (ToT). The TREC 2024 ToT track involves retrieving previously encountered items, such as movie names or landmarks when the searcher struggles to recall their exact identifiers. In this paper, we (ThinkIR) focus on four different approaches to retrieve the correct item for each query, including BM25 with optimized parameters and leveraging Large Language Models (LLMs) to reformulate the queries. Subsequently, we utilize these reformulated queries during retrieval using the BM25 model for each method. The four-step query reformulation technique, combined with two-layer retrieval, has enhanced retrieval performance in terms of NDCG and Recall. Eventually,  two-layer retrieval achieves the best performance among all the runs, with a Recall@1000 of 0.8067.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/IISER-K.tot.pdf",
        "key": "IISER-K-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{IISER-K-trec2024-papers-proc-1,\n   author = {Subinay Adhikary (IISER-K),, Shuvam Banerji Seal (IISER-K),, Soumyadeep Sar (IISER-K),, Dwaipayan Roy (IISER-K)},\n   title = {IISERK@ToT_2024:  Query Reformulation and Layered Retrieval for  Tip-of-Tongue Items},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {IISER-K},\n   trec_runs = {ThinkIR_BM25, ThinIR_BM25_layer_2, ThinkIR_semantic, ThinkIR_4_layer_2_w_small},\n   trec_tracks = {tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/IISER-K.tot.pdf}\n}"
      },
      "yalenlp-trec2024-papers-proc-1": {
        "file": "yalenlp.tot.pdf",
        "pid": "yalenlp",
        "title": "Yale NLP at TREC 2024: Tip-of-the-Tongue Track",
        "author": "Rohan Phanse (Yale University), Gabrielle Kaili-May Liu (Yale University), Arman Cohan (Yale University)",
        "abstract": "This paper describes our submissions to the TREC 2024 Tip-of-the-Tongue (ToT) track. We use a two-stage pipeline consisting of DPR-based retrieval followed by reranking with GPT-4o mini to answer ToT queries across three domains: movies, celebrities, and landmarks. Two of our runs performed retrieval using a \"general\" DPR model trained to handle queries from all domains. For our third run, we developed an approach to route queries to multiple \"expert\" DPR models each trained on a single domain. To build training sets for our DPR models, we collected existing ToT queries and generated over 100k synthetic queries using few-shot prompting with LLMs. After retrieval, results were reranked either listwise or using a combined pointwise and listwise approach. Our results demonstrate the efficacy of our three submitted approaches, which achieved NDCG@1000 scores ranging from 0.51 to 0.60.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/yalenlp.tot.pdf",
        "key": "yalenlp-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{yalenlp-trec2024-papers-proc-1,\n   author = {Rohan Phanse (Yale University), Gabrielle Kaili-May Liu (Yale University), Arman Cohan (Yale University)},\n   title = {Yale NLP at TREC 2024: Tip-of-the-Tongue Track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {yalenlp},\n   trec_runs = {dpr-lst-rerank, dpr-pnt-lst-rerank, dpr-router-lst-rerank},\n   trec_tracks = {tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/yalenlp.tot.pdf}\n}"
      },
      "webis-trec2024-papers-proc-1": {
        "file": "webis.biogen.rag.tot.pdf",
        "pid": "webis",
        "title": "Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks",
        "author": "Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)",
        "abstract": "In this paper, we describe the Webis Group's participation in the 2024~edition of TREC. We participated in the Biomedical Generative Retrieval track, the Retrieval-Augmented Generation track, and the Tip-of-the-Tongue track. For the biomedical track, we applied different paradigms of retrieval-augmented generation with open- and closed-source LLMs. For the Retrieval-Augmented Generation track, we aimed to contrast manual response submissions with fully-automated responses. For the Tip-of-the-Tongue track, we employed query relaxation as in our last year's submission (i.e., leaving out terms that likely reduce the retrieval effectiveness) that we combine with a new cross-encoder that we trained on an enriched version of the TOMT-KIS dataset.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf",
        "key": "webis-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{webis-trec2024-papers-proc-1,\n   author = {Maik Fr\u00f6be (Friedrich-Schiller-Universit\u00e4t), Lukas Gienapp (Leipzig University & ScaDS.AI), Harrisen Scells (Universit\u00e4t Kassel), Eric Oliver Schmidt (Martin-Luther-Universit\u00e4t Halle), Matti Wiegmann (Bauhaus-Universit\u00e4t Weimar), Martin Potthast, Universit\u00e4t Kassel (Universit\u00e4t Kassel & hessian.AI & ScaDS.AI), Matthias Hagen (Friedrich-Schiller-Universit\u00e4t Jena)},\n   title = {Webis at TREC 2024: Biomedical Generative Retrieval, Retrieval-Augmented Generation, and Tip-of-the-Tongue Tracks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {webis},\n   trec_runs = {webis-01, webis-02, webis-03, webis-04, webis-05, webis-ag-run0-taskrag, webis-ag-run1-taskrag, webis-manual, webis-rag-run0-taskrag, webis-rag-run1-taskrag, webis-rag-run3-taskrag, webis-ag-run3-reuserag, webis-rag-run4-reuserag, webis-rag-run5-reuserag, webis-ag-run2-reuserag, webis-1, webis-2, webis-3, webis-gpt-1, webis-gpt-4, webis-gpt-6, webis-5, webis-base, webis-tot-01, webis-tot-02, webis-tot-04, webis-tot-03},\n   trec_tracks = {biogen.rag.tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/webis.biogen.rag.tot.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-3": {
        "file": "Overview_tot.pdf",
        "pid": "coordinators",
        "title": "Overview of the TREC 2024 Tip-of-the-Tongue track",
        "author": "Jaime Arguello (University of North Carolina, USA), Samarth Bhargav (University of Amsterdam, Netherlands), Fernando Diaz (Carnegie Mellon University, USA), To Eun Kim (Carnegie Mellon University, USA), Yifan He (Carnegie Mellon University, USA), Evangelos Kanoulas (University of Amsterdam, Netherlands), Bhaskar Mitra (Microsoft Research, Canada)",
        "abstract": "Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems.  The TREC 2024 ToT track focused on a single ad-hoc retrieval task.  Participants were provided with training and development data in the movie domain.  Conversely, systems were tested on data that combined three domains: movies, celebrities, and landmarks.  This year, 6 groups (including the track coordinators) submitted 18 runs.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_tot.pdf",
        "key": "coordinators-trec2024-papers-proc-3",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-3,\n   author = {Jaime Arguello (University of North Carolina, USA), Samarth Bhargav (University of Amsterdam, Netherlands), Fernando Diaz (Carnegie Mellon University, USA), To Eun Kim (Carnegie Mellon University, USA), Yifan He (Carnegie Mellon University, USA), Evangelos Kanoulas (University of Amsterdam, Netherlands), Bhaskar Mitra (Microsoft Research, Canada)},\n   title = {Overview of the TREC 2024 Tip-of-the-Tongue track},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {tot}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_tot.pdf}\n}"
      }
    },
    "plaba": {
      "ntu_nlp-trec2024-papers-proc-1": {
        "file": "ntu_nlp.plaba.pdf",
        "pid": "ntu_nlp",
        "title": "Enhancing Accessibility of Medical Texts through Large Language Model-Driven Plain Language Adaptation",
        "author": "Ting-Wei Chang (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan) , Hen-Hsen Huang (Institute of Information Science, Academia Sinica, Taiwan) , Hsin-Hsi Chen (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan, AI Research Center (AINTU), National Taiwan University, Taiwan)",
        "abstract": "This paper addresses the challenge of making complex healthcare information more accessible through automated Plain Language Adaptation (PLA). PLA aims to simplify technical medical language, bridging a critical gap between the complexity of healthcare texts and patients\u2019 reading comprehension. Recent advances in Large Language Models (LLMs), such as GPT and BART, have opened new possibilities for PLA, especially in zero-shot and few-shot learning contexts where task-specific data is limited. In this work, we leverage the capabilities of LLMs such as GPT-4o-mini, Gemini-1.5-pro, and LLaMA for text simplification. Additionally, we incorporate Mixture-of-Agents (MoA) techniques to enhance adaptability and robustness in PLA tasks. Key contributions include a comparative analysis of prompting strategies, finetuning with QLoRA on different LLMs, and the integration of MoA technique. Our findings demonstrate the effectiveness of LLM-driven PLA, showcasing its potential in making healthcare information more comprehensible while preserving essential content.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/ntu_nlp.plaba.pdf",
        "key": "ntu_nlp-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{ntu_nlp-trec2024-papers-proc-1,\n   author = {Ting-Wei Chang (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan) , Hen-Hsen Huang (Institute of Information Science, Academia Sinica, Taiwan) , Hsin-Hsi Chen (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan, AI Research Center (AINTU), National Taiwan University, Taiwan)},\n   title = {Enhancing Accessibility of Medical Texts through Large Language Model-Driven Plain Language Adaptation},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {ntu_nlp},\n   trec_runs = {task2_moa_tier3_post, task2_moa_tier1_post, task2_moa_tier2_post, gemini-1.5-pro_demon5_replace-demon5, gemini-1.5-flash_demon5_replace-demon5, gpt-4o-mini _demon5_replace-demon5},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/ntu_nlp.plaba.pdf}\n}"
      },
      "UM-trec2024-papers-proc-1": {
        "file": "UM.plaba.pdf",
        "pid": "UM",
        "title": "{MaLei} at the PLABA Track of TAC-2024: RoBERTa for Task 1 -- LLaMA3.1 and GPT-4o for Task 2",
        "author": "Zhidong Ling, Zhihao Li, Pablo Romero, Lifeng Han, Goran Nenadic",
        "abstract": "This report is the system description of the \\textsc{MaLei} team (\\textbf{Manchester} and \\textbf{Leiden}) for the shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024 (we had an earlier name BeeManc following the last year). \r\nThis report contains two sections corresponding to the two sub-tasks in PLABA-2024. \r\nIn task one, we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon, and acronyms in the biomedical abstracts and reported the F1 score. \r\nDue to time constraints, we didn't finish the replacement task. \r\nIn task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA.\r\nFrom the official Evaluation from PLABA-2024 on Task 1A and 1B, our \\textbf{much smaller fine-tuned RoBERTa-Base} model ranked 3rd and 2nd respectively on the two sub-tasks, and the \\textbf{1st on averaged F1 scores across the two tasks} from 9 evaluated systems. Our LLaMA-3.1-70B-instructed model achieved the \\textbf{highest Completeness} score for Task-2.\r\nWe share our source codes, fine-tuned models, and related resources at \\url{https://github.com/HECTA-UoM/PLABA-2024}",
        "url": "https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf",
        "key": "UM-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{UM-trec2024-papers-proc-1,\n   author = {Zhidong Ling, Zhihao Li, Pablo Romero, Lifeng Han, Goran Nenadic},\n   title = {{MaLei} at the PLABA Track of TAC-2024: RoBERTa for Task 1 -- LLaMA3.1 and GPT-4o for Task 2},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {UM},\n   trec_runs = {GPT, LLaMa 3.1 70B instruction (2nd run), Roberta-base},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf}\n}"
      },
      "UM-trec2024-papers-proc-2": {
        "file": "UM.plaba.pdf",
        "pid": "UM",
        "title": "{MaLei} at the PLABA Track of TAC-2024: RoBERTa for Task 1 -- LLaMA3.1 and GPT-4o for Task 2",
        "author": "Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic",
        "abstract": "This report is the system description of the \\textsc{MaLei} team (\\textbf{Manchester} and \\textbf{Leiden}) for shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024 (we had an earlier name BeeManc following last year). \r\nThis report contains two sections corresponding to the two sub-tasks in PLABA-2024. \r\nIn task one, we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon and acronyms in the biomedical abstracts and reported the F1 score. \r\nDue to time constraints, we didn't finish the replacement task. \r\nIn task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA.\r\nFrom the official Evaluation from PLABA-2024 on Task 1A and 1B, our \\textbf{much smaller fine-tuned RoBERTa-Base} model ranked 3rd and 2nd respectively on the two sub-tasks, and the \\textbf{1st on averaged F1 scores across the two tasks} from 9 evaluated systems. Our LLaMA-3.1-70B-instructed model achieved the \\textbf{highest Completeness} score for Task-2.\r\nWe share our source codes, fine-tuned models, and related resources at \\url{https://github.com/HECTA-UoM/PLABA2024}",
        "url": "https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf",
        "key": "UM-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{UM-trec2024-papers-proc-2,\n   author = {Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic},\n   title = {{MaLei} at the PLABA Track of TAC-2024: RoBERTa for Task 1 -- LLaMA3.1 and GPT-4o for Task 2},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {UM},\n   trec_runs = {GPT, LLaMa 3.1 70B instruction (2nd run), Roberta-base},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf}\n}"
      },
      "um_fhs-trec2024-papers-proc-1": {
        "file": "um_fhs.plaba.pdf",
        "pid": "um_fhs",
        "title": "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text",
        "author": "Primoz Kocbek (University of Maribor), Leon Kopitar (University of Maribor), Zhihong Zhang (Columbia University), Emirhan Ayd\u0131n (Manisa Celal Bayar University), Maxim Topaz (Columbia University), Gregor Stiglic (University of Maribor)",
        "abstract": "This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13\u201314 years old students). We tested three approaches using OpenAI\u2019s gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/um_fhs.plaba.pdf",
        "key": "um_fhs-trec2024-papers-proc-1",
        "bibtex": "\n@inproceedings{um_fhs-trec2024-papers-proc-1,\n   author = {Primoz Kocbek (University of Maribor), Leon Kopitar (University of Maribor), Zhihong Zhang (Columbia University), Emirhan Ayd\u0131n (Manisa Celal Bayar University), Maxim Topaz (Columbia University), Gregor Stiglic (University of Maribor)},\n   title = {UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {um_fhs},\n   trec_runs = {plaba_um_fhs_sub1, plaba_um_fhs_sub2, plaba_um_fhs_sub3},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/um_fhs.plaba.pdf}\n}"
      },
      "UM-trec2024-papers-proc-3": {
        "file": "UM.plaba.pdf",
        "pid": "UM",
        "title": "{MaLei} at the PLABA Track of TREC 2024: RoBERTa for Term Replacement -- LLaMA3.1 and GPT-4o for Complete Abstract Adaptation",
        "author": "Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic",
        "abstract": "This report is the system description of the \\textsc{MaLei} team (\\textbf{Manchester} and \\textbf{Leiden}) for shared task Plain Language Adaptation of Biomedical Abstracts (PLABA) 2024 (we had an earlier name BeeManc following last year). \r\nThis report contains two sections corresponding to the two sub-tasks in PLABA-2024. \r\nIn task one, we applied fine-tuned ReBERTa-Base models to identify and classify the difficult terms, jargon and acronyms in the biomedical abstracts and reported the F1 score. \r\nDue to time constraints, we didn't finish the replacement task. \r\nIn task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot prompts to complete the abstract adaptation and reported the scores in BLEU, SARI, BERTScore, LENS, and SALSA.\r\nFrom the official Evaluation from PLABA-2024 on Task 1A and 1B, our \\textbf{much smaller fine-tuned RoBERTa-Base} model ranked 3rd and 2nd respectively on the two sub-tasks, and the \\textbf{1st on averaged F1 scores across the two tasks} from 9 evaluated systems. Our LLaMA-3.1-70B-instructed model achieved the \\textbf{highest Completeness} score for Task-2.\r\nWe share our source codes, fine-tuned models, and related resources at \\url{https://github.com/HECTA-UoM/PLABA2024}",
        "url": "https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf",
        "key": "UM-trec2024-papers-proc-3",
        "bibtex": "\n@inproceedings{UM-trec2024-papers-proc-3,\n   author = {Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic},\n   title = {{MaLei} at the PLABA Track of TREC 2024: RoBERTa for Term Replacement -- LLaMA3.1 and GPT-4o for Complete Abstract Adaptation},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {UM},\n   trec_runs = {GPT, LLaMa 3.1 70B instruction (2nd run), Roberta-base},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/UM.plaba.pdf}\n}"
      },
      "UAmsterdam-trec2024-papers-proc-2": {
        "file": "UAmsterdam.plaba.pdf",
        "pid": "UAmsterdam",
        "title": "Biomedical Text Simplification Models Trained on Aligned Abstracts and Lay Summaries",
        "author": "Jan Bakker (University of Amsterdam), Taiki Papandreou-Lazos (University of Amsterdam), Jaap Kamps (University of Amsterdam)",
        "abstract": "This paper documents the University of Amsterdam\u2019s participation in the TREC 2024 Plain Language Adaptation of Biomedical Abstracts (PLABA) Track. We investigated the effectiveness of text simplification models trained on aligned pairs of sentences in biomedical abstracts and plain language summaries. We participated in Task 2 on Complete Abstract Adaptation and conducted post-submission experiments in Task 1 on Term Replacement. Our main findings are the following. First, we used text simplification models trained on aligned real-world scientific abstracts and plain language summaries. We observed better performance for the context-aware model relative to the sentence-level model. Second, our experiments show the value of training on external corpora and demonstrate very reasonable out-of-domain performance on the PLABA data. Third, more generally, our models are conservative and cautious in gratuitous edits or information insertions. This approach ensures the fidelity of the generated output and limits the risk of overgeneration or hallucination.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/UAmsterdam.plaba.pdf",
        "key": "UAmsterdam-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{UAmsterdam-trec2024-papers-proc-2,\n   author = {Jan Bakker (University of Amsterdam), Taiki Papandreou-Lazos (University of Amsterdam), Jaap Kamps (University of Amsterdam)},\n   title = {Biomedical Text Simplification Models Trained on Aligned Abstracts and Lay Summaries},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {UAmsterdam},\n   trec_runs = {UAms-ConBART-Cochrane, UAms-BART-Cochrane},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/UAmsterdam.plaba.pdf}\n}"
      },
      "UAmsterdam-trec2024-papers-proc-3": {
        "file": "UAmsterdam.plaba.pdf",
        "pid": "UAmsterdam",
        "title": "Biomedical Text Simplification Models Trained on Aligned Abstracts and Lay Summaries",
        "author": "Jan Bakker (University of Amsterdam), Taiki Papandreou-Lazos (University of Amsterdam), Jaap Kamps (University of Amsterdam)",
        "abstract": "This paper documents the University of Amsterdam\u2019s participation in the TREC 2024 Plain Language Adaptation of Biomedical Abstracts (PLABA) Track. We investigated the effectiveness of text simplification models trained on aligned pairs of sentences in biomedical abstracts and plain language summaries. We participated in Task 2 on Complete Abstract Adaptation and conducted post-submission experiments in Task 1 on Term Replacement. Our main findings are the following. First, we used text simplification models trained on aligned real-world scientific abstracts and plain language summaries. We observed better performance for the context-aware model relative to the sentence-level model. Second, our experiments show the value of training on external corpora and demonstrate very reasonable out-of-domain performance on the PLABA data. Third, more generally, our models are conservative and cautious in gratuitous edits or information insertions. This approach ensures the fidelity of the generated output and limits the risk of overgeneration or hallucination.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/UAmsterdam.plaba.pdf",
        "key": "UAmsterdam-trec2024-papers-proc-3",
        "bibtex": "\n@inproceedings{UAmsterdam-trec2024-papers-proc-3,\n   author = {Jan Bakker (University of Amsterdam), Taiki Papandreou-Lazos (University of Amsterdam), Jaap Kamps (University of Amsterdam)},\n   title = {Biomedical Text Simplification Models Trained on Aligned Abstracts and Lay Summaries},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {UAmsterdam},\n   trec_runs = {UAms-ConBART-Cochrane, UAms-BART-Cochrane},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/UAmsterdam.plaba.pdf}\n}"
      },
      "SIB-trec2024-papers-proc-7": {
        "file": "SIB.plaba.pdf",
        "pid": "SIB",
        "title": "SIB Text-Mining at TREC PLABA 2024",
        "author": "Luc Mottin (SIB, Swiss Institute of Bioinformatics, Geneva, Switzerland)Ana\u00efs Mottaz (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Julien Knafou (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Alexandre Flament (HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Julien Gobeill (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Patrick Ruch (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)",
        "abstract": "The comprehension of health information by patients has a real influence on the efficacy of their treatment. However,while more health resources are increasingly available to the public, the use of medical jargon and complex syntaxmakes them difficult to understand. Recent advances in machine translation and text simplification may help to makethese resources more accessible by adapting biomedical text into plain language. In this context, the TREC 2024 PlainLanguage Adaptation of Biomedical Abstracts track sought to develop specialized algorithms able to adapt biomedicalabstracts into plain language for the general public. The SIB Text Mining group participated in the \u201cComplete AbstractAdaptation\u201d subtask. Our first approach examines how a specific prompting using a state-of-the-art Large LanguageModel performs in the global adaptation of biomedical documents, with the intention of proposing a baseline with notechnical improvements to compare more advanced strategies. The second approach investigates how the fine tuningof the transformer handles the task, and the third approach integrates a Retrieval Augmented Generation function tohelp generate a new document based on information from relevant sources.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/SIB.plaba.pdf",
        "key": "SIB-trec2024-papers-proc-7",
        "bibtex": "\n@inproceedings{SIB-trec2024-papers-proc-7,\n   author = {Luc Mottin (SIB, Swiss Institute of Bioinformatics, Geneva, Switzerland)Ana\u00efs Mottaz (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Julien Knafou (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Alexandre Flament (HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Julien Gobeill (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)Patrick Ruch (SIB, Swiss Institute of Bioinformatics; HES-SO, University of Applied Sciences and Arts of Western Switzerland, Geneva, Switzerland)},\n   title = {SIB Text-Mining at TREC PLABA 2024},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {SIB},\n   trec_runs = {TREC2024_SIB_run1, TREC2024_SIB_run3, TREC2024_SIB_run4},\n   trec_tracks = {plaba}\n   url = {https://trec.nist.gov/pubs/trec33/papers/SIB.plaba.pdf}\n}"
      }
    },
    "vtt": {
      "softbank-meisei-trec2024-papers-proc-2": {
        "file": "softbank-meisei.avs.vtt.pdf",
        "pid": "softbank-meisei",
        "title": "Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks",
        "author": "Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)",
        "abstract": "The Softbank-Meisei team participated in the ad-hoc video search (AVS) and video-to-text (VTT) tasks at TREC 2024. In this year's AVS task, we submitted four fully automatic systems for both the main and progress tasks. Our systems utilized pre-trained vision and language models, including CLIP, BLIP, and BLIP-2, along with several other advanced models. We also expanded the original query texts using text generation and image generation techniques to enhance data diversity. The integration ratios of these models were optimized based on results from previous benchmark test datasets. In this year's VTT, as last year, we submitted four main task methods using multiple model captioning, reranking, and generative AI for summarization. For the subtasks, we submitted three methods using the output of each model. Last year's test data for the main task showed improvements of about 0.04 points in CIDEr-D and about 0.03 points in SPICE, based on the indices we had on hand.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf",
        "key": "softbank-meisei-trec2024-papers-proc-2",
        "bibtex": "\n@inproceedings{softbank-meisei-trec2024-papers-proc-2,\n   author = {Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)},\n   title = {Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {softbank-meisei},\n   trec_runs = {SoftbankMeisei - Progress Run 1, SoftbankMeisei - Progress Run 2, SoftbankMeisei - Progress Run 3, SoftbankMeisei - Progress Run 4, SoftbankMeisei - Main Run 1, SoftbankMeisei - Main Run 2, SoftbankMeisei - Main Run 3, SoftbankMeisei - Main Run 4, SoftbankMeisei_vtt_main_run1, SoftbankMeisei_vtt_main_run2, SoftbankMeisei_vtt_main_run3, SoftbankMeisei_vtt_main_run4, SoftbankMeisei_vtt_sub_run2, SoftbankMeisei_vtt_sub_run3, SoftbankMeisei_vtt_sub_run1},\n   trec_tracks = {avs.vtt}\n   url = {https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf}\n}"
      },
      "softbank-meisei-trec2024-papers-proc-3": {
        "file": "softbank-meisei.avs.vtt.pdf",
        "pid": "softbank-meisei",
        "title": "Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks",
        "author": "Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)",
        "abstract": "The Softbank-Meisei team participated in the ad-hoc video search (AVS) and video-to-text (VTT) tasks at TREC 2024. In this year's AVS task, we submitted four fully automatic systems for both the main and progress tasks. Our systems utilized pre-trained vision and language models, including CLIP, BLIP, and BLIP-2, along with several other advanced models. We also expanded the original query texts using text generation and image generation techniques to enhance data diversity. The integration ratios of these models were optimized based on results from previous benchmark test datasets. In this year's VTT, as last year, we submitted four main task methods using multiple model captioning, reranking, and generative AI for summarization. For the subtasks, we submitted three methods using the output of each model. Last year's test data for the main task showed improvements of about 0.04 points in CIDEr-D and about 0.03 points in SPICE, based on the indices we had on hand.",
        "url": "https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf",
        "key": "softbank-meisei-trec2024-papers-proc-3",
        "bibtex": "\n@inproceedings{softbank-meisei-trec2024-papers-proc-3,\n   author = {Kazuya Ueki (Meisei University), Yuma Suzuki (SoftBank Corp.), Hiroki Takushima (SoftBank Corp.), Haruki Sato (Agoop Corp.), Takumi Takada (SB Intuitions Corp.), Aiswariya Manoj Kumar (SoftBank Corp.), Hayato Tanoue (SoftBank Corp.), Hiroki Nishihara (SoftBank Corp.), Yuki Shibata (SoftBank Corp.), Takayuki Hori (SoftBank Corp.)},\n   title = {Softbank-Meisei at TREC 2024 Ad-hoc Video Search and Video to Text Tasks},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {softbank-meisei},\n   trec_runs = {SoftbankMeisei - Progress Run 1, SoftbankMeisei - Progress Run 2, SoftbankMeisei - Progress Run 3, SoftbankMeisei - Progress Run 4, SoftbankMeisei - Main Run 1, SoftbankMeisei - Main Run 2, SoftbankMeisei - Main Run 3, SoftbankMeisei - Main Run 4, SoftbankMeisei_vtt_main_run1, SoftbankMeisei_vtt_main_run2, SoftbankMeisei_vtt_main_run3, SoftbankMeisei_vtt_main_run4, SoftbankMeisei_vtt_sub_run2, SoftbankMeisei_vtt_sub_run3, SoftbankMeisei_vtt_sub_run1},\n   trec_tracks = {avs.vtt}\n   url = {https://trec.nist.gov/pubs/trec33/papers/softbank-meisei.avs.vtt.pdf}\n}"
      },
      "coordinators-trec2024-papers-proc-6": {
        "file": "Overview_avs.vtt.actev.pdf",
        "pid": "coordinators",
        "title": "TRECVID 2024 - Evaluating video search, captioning, and activity recognition",
        "author": "George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)",
        "abstract": "The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis and retrieval evaluation with the goal of promoting progress in research and development of content-based exploitation and retrieval of information from digital video via open, tasks-based evaluation supported by metrology.\r\nOver the last two decades, this effort has yielded a better understanding of how systems can effectively accomplish such processing and how one can reliably benchmark their performance. TRECVID has been funded by NIST (National Institute of Standards and Technology) and other US government agencies. In addition, many organizations and individuals world-wide contribute significant time and effort. This year TRECVID has been merged back to TREC (Text Retrieval Conference1) and planned the following four tracks:\r\n1. Ad-hoc Video Search (AVS)\r\n2. Video to Text (VTT)\r\n3. Activities in Extended Video (ActEV)\r\n4. Medical Video Question Answering (Med-VidQA)",
        "url": "https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf",
        "key": "coordinators-trec2024-papers-proc-6",
        "bibtex": "\n@inproceedings{coordinators-trec2024-papers-proc-6,\n   author = {George Awad (NIST), Jonathan Fiscus (NIST), Afzal Godil (NIST), Lukas Diduch (NIST), Yvette Graham (Trinity College Dublin), Georges Qu\u00e9not (LIG)},\n   title = {TRECVID 2024 - Evaluating video search, captioning, and activity recognition},\n   booktitle = {The Thirty-Third Text REtrieval Conference Proceedings (TREC 2024), Gaithersburg, MD, USA, November 15-18, 2024},\n   series = {NIST Special Publication},\n   volume = {1329},\n   publisher = {National Institute of Standards and Technology (NIST)},\n   year = {2024},\n   trec_org = {coordinators},\n   trec_runs = {},\n   trec_tracks = {avs.vtt.actev}\n   url = {https://trec.nist.gov/pubs/trec33/papers/Overview_avs.vtt.actev.pdf}\n}"
      }
    }
  }
}
